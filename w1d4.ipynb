{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starship006/ARENA-work/blob/main/w1d4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fancy_einsum einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlRgy2kjSzdd",
        "outputId": "a811e651-aa95-471d-81b0-7af4a1b75c99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fancy_einsum\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.5.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: fancy-einsum, einops\n",
            "Successfully installed einops-0.5.0 fancy-einsum-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aOsyx41fOvuN"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import fancy_einsum as einsum\n",
        "import einops\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oei89mWsOvuP"
      },
      "source": [
        "# Training Shakespeare Himself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HguafRmOvuQ"
      },
      "source": [
        "## Copy transformer code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9pLknZ-OvuQ"
      },
      "source": [
        "Well, not copy entirely - I'm gonna put down optimizations so it can use the GPU.\n",
        "\n",
        "And I did just that. The speed improvements are MASSIVE, wow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SMSyuB3OvuQ",
        "outputId": "36c9d02f-be9e-40b6-a6be-9101d27b6f33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
        "t.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "id": "dARzlkbDOvuQ"
      },
      "outputs": [],
      "source": [
        "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
        "    '''\n",
        "    Implements multihead masked attention on the matrices Q, K and V.\n",
        "\n",
        "    Q: shape (batch, seq_len, nheads*headsize)\n",
        "    K: shape (batch, seq_len, nheads*headsize)\n",
        "    V: shape (batch, seq_len, nheads*headsize)\n",
        "    '''\n",
        "    \n",
        "    Q = einops.rearrange(Q, 'b s (n h) -> b n s h', n = num_heads)\n",
        "    K = einops.rearrange(K, 'b s (n h) -> b n s h', n = num_heads)\n",
        "    V = einops.rearrange(V, 'b s (n h) -> b n s h', n = num_heads)\n",
        "\n",
        "\n",
        "    scores = einsum.einsum('b n k h, b n s h -> b n s k', K, Q)\n",
        "    assert scores.shape == t.Size([Q.shape[0], num_heads,Q.shape[2], K.shape[2]])\n",
        "\n",
        "    scores = scores / np.sqrt(Q.shape[-1])\n",
        "    attention = scores + t.triu(t.ones_like(scores,device = device) * float(\"-inf\"), diagonal=1) # THIS IS STOLEN FROM JAY - testing it out\n",
        "    softed = t.softmax(attention,dim=-1)\n",
        "    result =  einsum.einsum('batch numheads seqQ seqK, batch numheads seqK headsize -> batch numheads seqQ headsize',softed, V)\n",
        "    return einops.rearrange(result, 'batch numheads seqQ headsize -> batch seqQ (numheads headsize)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "id": "pxFIcpudOvuR"
      },
      "outputs": [],
      "source": [
        "class MultiheadMaskedAttention(nn.Module):\n",
        "    W_QKV: nn.Linear\n",
        "    W_O: nn.Linear\n",
        "\n",
        "    def __init__(self, hidden_size: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = hidden_size // num_heads\n",
        "\n",
        "        self.WQKV = t.nn.Linear(self.hidden_size, 3 * hidden_size) # TODO: why do we use a linear layer here? aren't they matricies?\n",
        "        self.W0 = t.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        '''\n",
        "        x: shape (batch, seq, hidden_size)\n",
        "\n",
        "        Return: shape (batch, seq, hidden_size)\n",
        "        '''\n",
        "        #print(\"YO?\")\n",
        "        x = x.float() # seems like it needs to be a float!\n",
        "        QKV = self.WQKV(x)\n",
        "        Q = QKV[:,:,:self.hidden_size]\n",
        "        K = QKV[:,:,self.hidden_size:self.hidden_size * 2]\n",
        "        V = QKV[:,:,self.hidden_size * 2:]\n",
        "        assert Q.shape == K.shape == V.shape == x.shape\n",
        "        return self.W0(multihead_masked_attention(Q,K,V,self.num_heads))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "id": "nG9-RZDwOvuR"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TransformerConfig:\n",
        "    '''Constants used throughout your decoder-only transformer model.'''\n",
        "\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    vocab_size: int\n",
        "    hidden_size: int\n",
        "    max_seq_len: int\n",
        "    dropout: float = 0.1\n",
        "    layer_norm_epsilon: float = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "id": "dxznpdXeOvuS"
      },
      "outputs": [],
      "source": [
        "# from yesterday\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, max_seq_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dim = embedding_dim\n",
        "        self.length = max_seq_len\n",
        "\n",
        "        # mostly copied. i understand this, just need to work on \n",
        "        # making more tensors and getting more exposure to methods of making tensors\n",
        "        def P (delta):\n",
        "            n = 10000 # hardcoded\n",
        "            d = embedding_dim\n",
        "            l = max_seq_len\n",
        "            sin_array = np.sin(delta / n ** (2 * np.arange(d//2) / d))\n",
        "            cos_array = np.cos(delta / n ** (2 * np.arange(d//2) / d))\n",
        "\n",
        "            array = np.zeros(d)\n",
        "            array[::2] = sin_array\n",
        "            array[1::2] = cos_array\n",
        "\n",
        "            return array\n",
        "\n",
        "        tokenArray = []\n",
        "        for i in range(max_seq_len):\n",
        "            tokenArray.append(P(i)) # changed from previous design\n",
        "        \n",
        "        self.multMax = t.tensor(np.array(tokenArray), dtype=t.float, device = device)\n",
        "        \n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        '''\n",
        "        x: shape (batch, seq_len, embedding_dim)\n",
        "        '''\n",
        "        return x + self.multMax[:x.shape[1]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "id": "4evwbrSDOvuT"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "    def forward(self, x: t.Tensor):\n",
        "        x = x.float() # seems like it needs to be a float!\n",
        "        return self.layers(x).float() # ima do the same thing again!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "id": "O_i-QmphOvuT"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.attentionBlock = nn.Sequential(\n",
        "            MultiheadMaskedAttention(config.hidden_size,  config.num_heads),\n",
        "            nn.LayerNorm(config.hidden_size)\n",
        "        )\n",
        "        self.MLP = nn.Sequential(\n",
        "            MLP(config),\n",
        "            nn.LayerNorm(config.hidden_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        partOne = x + self.attentionBlock(x)\n",
        "        return (partOne + self.MLP(partOne)).float() # seems like it needs to be a float!\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "id": "YoM0le7qOvuU"
      },
      "outputs": [],
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.tokenize = nn.Embedding(config.vocab_size, config.hidden_size).to(device)\n",
        "        self.positionize = PositionalEncoding(config.hidden_size,config.max_seq_len)\n",
        "        self.restModel = nn.Sequential(\n",
        "            nn.Dropout(config.dropout),\n",
        "            *[DecoderBlock(config) for i in range(config.num_layers)],\n",
        "            nn.LayerNorm(config.hidden_size),\n",
        "        )\n",
        "        self.unembed = self.tokenize.weight.T.to(device)\n",
        "        \n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        x = self.tokenize(x)\n",
        "        x = self.positionize(x)\n",
        "        toUnembed = self.restModel(x).to(device)\n",
        "        return toUnembed@self.unembed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E108n5oQOvuU"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvqBu-CxOvuU"
      },
      "source": [
        "Make the dataset to parse through all of the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "x17J93OHOvuU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, words, seq_len, fractionOfWords):\n",
        "        self.fractionOfWords = fractionOfWords\n",
        "        self.words = words\n",
        "        self.setOfWords = set(words)\n",
        "        self.seq_len = seq_len\n",
        "        self.max_len = len(self.words) - (self.seq_len + 1)\n",
        "        self.vocab_size = len(self.setOfWords)\n",
        "        self.word_to_token = {word: idx for (idx, word) in enumerate(sorted(self.setOfWords))}\n",
        "        self.token_to_word = {idx: word for (idx, word) in enumerate(sorted(self.setOfWords))}\n",
        "        self.allTokens = t.tensor([self.word_to_token[word] for word in self.words],device = device)\n",
        "        \n",
        "        if (self.fractionOfWords > 0.9):\n",
        "            print(\"Probably don't do this. Errors may about\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.max_len * self.fractionOfWords)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.allTokens[idx:idx + self.seq_len + 1]\n",
        "        input = tokens[:-1]\n",
        "        target = tokens[1:]\n",
        "        return input, target \n",
        "\n",
        "    def getDataSize(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def convertToTokens(self, phrase: list) -> t.tensor:\n",
        "        return t.tensor([self.word_to_token[word] for word in phrase],device = device)\n",
        "\n",
        "    def convertStringToTokenList(self, phrase: str) -> list:\n",
        "        words = re.split(r\"\\b\", phrase)\n",
        "        return [self.word_to_token[word] for word in words]\n",
        "\n",
        "    def convertToText(self, tokens: t.tensor):\n",
        "        temp = []\n",
        "        for i, value in enumerate(tokens):\n",
        "            #print(value.item())\n",
        "            temp.append(self.token_to_word[value.item()])\n",
        "        return temp\n",
        "\n",
        "    def decodeList(self, words: list):\n",
        "        temp = []\n",
        "        for value in words:\n",
        "            temp.append(self.token_to_word[value])\n",
        "        return temp\n",
        "        \n",
        "    def listToString(self, words: list) -> str:\n",
        "        temp = \"\"\n",
        "        for word in words:\n",
        "            temp = temp + word\n",
        "        return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "nv945w7VOvuU"
      },
      "outputs": [],
      "source": [
        "file = open(\"shakespeare.txt\")\n",
        "text = file.read()\n",
        "words = re.split(r\"\\b\", text)\n",
        "\n",
        "fractionOfWords = 0.08 # what percent of the corpus to train on \n",
        "\n",
        "\n",
        "lengthOfSeq = 40\n",
        "\n",
        "shak = CustomTextDataset(words, lengthOfSeq, fractionOfWords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeWDlv7FOvuU"
      },
      "source": [
        "## Running this data through a transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VdqNIkuIOvuV"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(shak, batch_size=32,shuffle=True)\n",
        "\n",
        "# this specific one trained for 9 minutes and 18 seconds\n",
        "\n",
        "thisConfig = TransformerConfig(\n",
        "    num_layers = 4, # 6 layers in the Attention paper\n",
        "    num_heads = 4, # 8 heads in Attention paper\n",
        "    vocab_size = trainloader.dataset.getDataSize(), # 37000 tokens in Attention paper (?)\n",
        "    hidden_size = 512, # recall that this = num_heads * headsize | 512 is the embedding dim used in Attention paper\n",
        "    max_seq_len = lengthOfSeq, \n",
        "    dropout = 0.1, # same as Attention paper\n",
        "    layer_norm_epsilon=0.00001\n",
        ")\n",
        "\n",
        "myTransformer = DecoderOnlyTransformer(thisConfig).to(device)\n",
        "optimizer = t.optim.Adam(myTransformer.parameters(), lr = 1e-3)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Tl8YEwDhOvuV"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 1\n",
        "\n",
        "losses = []\n",
        "myTransformer.train()\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    for inputs, targets in trainloader:\n",
        "        outputs = myTransformer(inputs).to(device)\n",
        "        targets = t.nn.functional.one_hot(targets, num_classes=trainloader.dataset.getDataSize()).float().to(device)\n",
        "        \n",
        "        outputs = einops.rearrange(outputs, 'batch seq vocab -> (batch seq) vocab')\n",
        "        targets = einops.rearrange(targets, 'batch seq vocab -> (batch seq) vocab')\n",
        "\n",
        "        outputs = outputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        loss = criterion(outputs,targets).to(device)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "NUjqRiN4OvuV",
        "outputId": "b9b17c32-2384-499b-e706-6986c1758792"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f356b0fac10>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df = pd.DataFrame(losses)\n",
        "df.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vyx4rgkOvuV",
        "outputId": "148c211f-15a6-457c-9c3a-f10d87ab8263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  111, 20202,   111, 31376,   111, 31376,   111, 31102,   111,  4933,\n",
            "          405,  2037,   111], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " 'gone',\n",
              " ' ',\n",
              " 'to',\n",
              " ' ',\n",
              " 'to',\n",
              " ' ',\n",
              " 'the',\n",
              " ' ',\n",
              " 'Florentine',\n",
              " ',\\n',\n",
              " 'And',\n",
              " ' ']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# quick test - use sample method to actually use the transformer: \n",
        "\n",
        "myTransformer.eval()\n",
        "\n",
        "testPhrase = [\"Be\", \" \", \"not\", \" \", \"afraid\", \" \", \"to\", \" \", \"the\", \" \", \"Florentine\", \"\\n\",\n",
        "              \"And\"]\n",
        "input = shak.convertToTokens(testPhrase)\n",
        "input = input[None, :]\n",
        "tokens = myTransformer(input).argmax(dim=-1)[0]\n",
        "print(tokens)\n",
        "shak.convertToText(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling"
      ],
      "metadata": {
        "id": "CSwuSOXhVi1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_sampling_methods(input_ids: t.Tensor, logits: t.Tensor, temperature=1.0, freq_penalty=0.0, top_k=0, top_p=0.0) -> int:\n",
        "  assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token IDS\"\n",
        "  assert temperature >= 0\n",
        "\n",
        "  return sample_basic(logits)\n",
        "\n",
        "\n",
        "def sample_token(\n",
        "    model,\n",
        "    encodeMethod,\n",
        "    decodeMethod,\n",
        "    initial_text: str,\n",
        "    max_tokens_generated = 40,\n",
        "    **kwargs) -> str:\n",
        "    # samples tokens until model outputs eos_token_id or token limit reached\n",
        "\n",
        "    \n",
        "\n",
        "    model.eval()\n",
        "    input_ids: list = encodeMethod(initial_text)\n",
        "    generated_ids = []\n",
        "    device = next(model.parameters()).device #what is next doing here?\n",
        "\n",
        "    tokens_to_generate = max_tokens_generated - len(input_ids)\n",
        "    for _ in range(tokens_to_generate):\n",
        "        #print(input_ids + generated_ids)\n",
        "        new_input_ids = t.tensor(input_ids + generated_ids, dtype=t.int64, device=device)\n",
        "        #print(new_input_ids.unsqueeze(0).shape)\n",
        "        logits = model(new_input_ids.unsqueeze(0))[0, -1]\n",
        "        #print(logits.shape)\n",
        "        new_token = apply_sampling_methods(new_input_ids, logits, **kwargs)\n",
        "        generated_ids.append(new_token)\n",
        "\n",
        "      \n",
        "    return decodeMethod(input_ids + generated_ids)\n",
        "\n",
        "\n",
        "# quick test:\n",
        "\n",
        "myTransformer.eval()\n",
        "\n",
        "testPhrase = [\"Be\", \" \", \"not\", \" \", \"afraid\", \" \", \"to\", \" \", \"the\", \" \", \"Florentine\", \"\\n\",\n",
        "              \"And\"]\n",
        "input = shak.convertToTokens(testPhrase)\n",
        "type(input)\n",
        "\n",
        "\n",
        "print(shak.listToString(sample_token(myTransformer,shak.convertStringToTokenList,shak.decodeList,\n",
        "                                     \"Who am I\", 40)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkE5o5i-VlVK",
        "outputId": "7d43b141-7893-4907-b9be-0eac8fd86f79"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who am I of my heart,\n",
            "And that which doth my heart to the heart to the heart,\n",
            "And \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_basic(logits):\n",
        "    return logits.argmax(dim=-1).item()"
      ],
      "metadata": {
        "id": "McngECeVWCl-"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CBK0fyujcfIr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}