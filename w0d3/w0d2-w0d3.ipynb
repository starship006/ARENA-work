{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making your own modules (remnant from day 2)\n",
    "Not a hard section! Just an hour and 21 minutes spent on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Callable\n",
    "import torch as t\n",
    "IntOrPair = Union[int, tuple[int, int]]\n",
    "Pair = tuple[int, int]\n",
    "\n",
    "def force_pair(v: IntOrPair) -> Pair:\n",
    "    '''Convert v to a pair of int, if it isn't already.'''\n",
    "    if isinstance(v, tuple):\n",
    "        if len(v) != 2:\n",
    "            raise ValueError(v)\n",
    "        return (int(v[0]), int(v[1]))\n",
    "    elif isinstance(v, int):\n",
    "        return (v, v)\n",
    "    raise ValueError(v)\n",
    "\n",
    "def pad2d(x: t.Tensor, left: int, right: int, top: int, bottom: int, pad_value: float) -> t.Tensor:\n",
    "    '''Return a new tensor with padding applied to the edges.\n",
    "\n",
    "    x: shape (batch, in_channels, height, width), dtype float32\n",
    "\n",
    "    Return: shape (batch, in_channels, top + height + bottom, left + width + right)\n",
    "    '''\n",
    "    B, I, H, W = x.shape\n",
    "\n",
    "    tens = x.new_full((B, I, top + H + bottom, left + W + right), pad_value)\n",
    "    tens[...,top:top+H, left:left + W] = x\n",
    "    return tens\n",
    "\n",
    "\n",
    "\n",
    "# Examples of how this function can be used:\n",
    "#       force_pair((1, 2))     ->  (1, 2)\n",
    "#       force_pair(2)          ->  (2, 2)\n",
    "#       force_pair((1, 2, 3))  ->  ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually verify that this is an informative repr: MaxPool2d(kernel_size = (3, 3) stride = (2, 2) padding = (1, 1))\n"
     ]
    }
   ],
   "source": [
    "import utilsd2\n",
    "import torch.nn as nn\n",
    "\n",
    "class MaxPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size: IntOrPair, stride: Optional[IntOrPair] = None, padding: IntOrPair = 1):\n",
    "        super().__init__()\n",
    "        if stride == None:\n",
    "            stride = kernel_size\n",
    "        kernel_size = force_pair(kernel_size)\n",
    "        stride = force_pair(stride)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = force_pair(padding)\n",
    "\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        new_x = pad2d(x, self.padding[1], self.padding[1], self.padding[0], self.padding[0], -99999999999)\n",
    "        batch, in_channels, height, width = x.shape\n",
    "        kernel_height = self.kernel_size[0]\n",
    "        kernel_width = self.kernel_size[1]\n",
    "\n",
    "        xB, xIB, xH, xW = new_x.stride()\n",
    "\n",
    "        output_width = 1 + (width + 2 * self.padding[1] - kernel_width) // self.stride[1]\n",
    "        output_height = 1 + (height + 2 * self.padding[0] - kernel_height) // self.stride[0]\n",
    "        size = (batch, in_channels, output_height, output_width, kernel_height, kernel_width)\n",
    "        strides = (xB, xIB, xH * self.stride[0], xW * self.stride[1],  xH, xW)\n",
    "        new_x = t.as_strided(new_x, size, strides)\n",
    "\n",
    "        new_x = t.amax(new_x, 5)\n",
    "        new_x = t.amax(new_x, 4)\n",
    "        return new_x\n",
    "\n",
    "\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        '''Add additional information to the string representation of this class.'''\n",
    "        output = \"\"\n",
    "        output += \"kernel_size = \" + str(self.kernel_size)\n",
    "        output += \" stride = \" + str(self.stride)\n",
    "        output += \" padding = \" + str(self.padding)\n",
    "        return output\n",
    "        \n",
    "\n",
    "utilsd2.test_maxpool2d_module(MaxPool2d)\n",
    "m = MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "print(f\"Manually verify that this is an informative repr: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return t.maximum(x, t.tensor(0.0))\n",
    "\n",
    "utilsd2.test_relu(ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten(start_dim = 2 end_dim = 3)\n"
     ]
    }
   ],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self, start_dim: int = 1, end_dim: int = -1) -> None:\n",
    "        super().__init__()\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        '''Flatten out dimensions from start_dim to end_dim, inclusive of both.\n",
    "        '''\n",
    "        # get start and end dimensions\n",
    "        start, end = self.start_dim, self.end_dim\n",
    "        if (start < 0):\n",
    "            start = len(input.shape) + start\n",
    "        if (end < 0):\n",
    "            end = len(input.shape) + end\n",
    "\n",
    "        # get shape of output\n",
    "        shape = []\n",
    "        for index in range(len(input.shape)):\n",
    "            shape.append(input.shape[index])\n",
    "        \n",
    "        # on start_dim element, multiply each value up until end_dim, removing them each time\n",
    "        to_remove = end - start\n",
    "        for i in range(to_remove):\n",
    "            shape[start] *= shape.pop(start + 1)\n",
    "        shape = tuple(shape)\n",
    "\n",
    "        reshaped = t.reshape(input,shape)\n",
    "        return reshaped\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        output = \"\"\n",
    "        output += \"start_dim = \" + str(self.start_dim) + \" \"\n",
    "        output += \"end_dim = \" + str(self.end_dim)\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "utilsd2.test_flatten(Flatten)\n",
    "f = Flatten(2,3)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(weight.shape = torch.Size([4, 3]) bias.shape = torch.Size([4]))\n"
     ]
    }
   ],
   "source": [
    "from audioop import bias\n",
    "import math\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias=True):\n",
    "        '''A simple linear (technically, affine) transformation.\n",
    "\n",
    "        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n",
    "        If `bias` is False, set `self.bias` to None.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        bound = -1 / math.sqrt(in_features)\n",
    "        self.weight = nn.Parameter(t.FloatTensor(out_features,in_features).uniform_(bound, -bound))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(t.FloatTensor(out_features).uniform_(bound, -bound))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (*, in_features)\n",
    "        Return: shape (*, out_features)\n",
    "        '''\n",
    "        result = x@self.weight.T\n",
    "        if self.bias != None:\n",
    "            result = result + self.bias\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        output = \"\"\n",
    "        output += \"weight.shape = \" + str(self.weight.shape) + \" \"\n",
    "        if self.bias != None:\n",
    "            output += \"bias.shape = \" + str(self.bias.shape)\n",
    "        else:\n",
    "            output += \"bias = None\"\n",
    "        return output\n",
    "\n",
    "\n",
    "utilsd2.test_linear_forward(Linear)\n",
    "utilsd2.test_linear_parameters(Linear)\n",
    "utilsd2.test_linear_no_bias(Linear)\n",
    "\n",
    "print(Linear(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(weights shape = torch.Size([3, 2, 4, 4]) stride = (1, 1) padding = (0, 0) )\n"
     ]
    }
   ],
   "source": [
    "from fancy_einsum import einsum\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, kernel_size: IntOrPair, stride: IntOrPair = 1, padding: IntOrPair = 0\n",
    "    ):\n",
    "        '''\n",
    "        Same as torch.nn.Conv2d with bias=False.\n",
    "\n",
    "        Name your weight field `self.weight` for compatibility with the PyTorch version.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        kernel_size = force_pair(kernel_size)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        bound = -1 / math.sqrt(kernel_size[0] * kernel_size[1] * in_channels)\n",
    "        self.weight = nn.Parameter(t.FloatTensor(out_channels, in_channels, kernel_size[0], kernel_size[1]).uniform_(bound, -bound))\n",
    "        self.stride = force_pair(stride)\n",
    "        self.padding = force_pair(padding)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        # setup a strided tensor to represent input\n",
    "        new_x = pad2d(x, self.padding[1], self.padding[1], self.padding[0], self.padding[0], 0)\n",
    "        batch, in_channels, height, width = x.shape\n",
    "        out_channels, in_channels, kernel_height, kernel_width = self.weight.shape\n",
    "        xB, xIB, xH, xW = new_x.stride()\n",
    "        output_width = 1 + (width + 2 * self.padding[1] - kernel_width) // self.stride[1]\n",
    "        output_height = 1 + (height + 2 * self.padding[0] - kernel_height) // self.stride[0]\n",
    "        size = (batch, in_channels, output_height, output_width, kernel_height, kernel_width)\n",
    "        strides = (xB, xIB, xH * self.stride[0], xW * self.stride[1],  xH, xW)\n",
    "        new_x = t.as_strided(new_x, size, strides)\n",
    "\n",
    "        return einsum('batch in_channels output_height output_width kernel_height kernel_width, out_channels in_channels kernel_height kernel_width -> batch out_channels output_height output_width ', new_x, self.weight)\n",
    "\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        output = \"\"\n",
    "        output += \"weights shape = \" + str(self.weight.shape) + \" \"\n",
    "        output += \"stride = \" + str(self.stride) + \" \"\n",
    "        output += \"padding = \" + str(self.padding) + \" \"\n",
    "        return output\n",
    "\n",
    "utilsd2.test_conv2d_module(Conv2d)\n",
    "print(Conv2d(2,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training a CNN\n",
    "\n",
    "A lot of debugging. Fun, though. 2 hours 16 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t \n",
    "import PIL \n",
    "from PIL import Image \n",
    "import json \n",
    "from pathlib import Path \n",
    "from typing import Union, Tuple, Callable, Optional \n",
    "import plotly.graph_objects as go \n",
    "import plotly.express as px \n",
    "from plotly.subplots import make_subplots \n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(weights shape = torch.Size([32, 1, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
      "  (ReLU1): ReLU()\n",
      "  (max1): MaxPool2d(kernel_size = (2, 2) stride = (2, 2) padding = (0, 0))\n",
      "  (conv2): Conv2d(weights shape = torch.Size([64, 32, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
      "  (ReLU2): ReLU()\n",
      "  (max2): MaxPool2d(kernel_size = (2, 2) stride = (2, 2) padding = (0, 0))\n",
      "  (flatten): Flatten(start_dim = 1 end_dim = -1)\n",
      "  (lin1): Linear(weight.shape = torch.Size([128, 3136]) bias.shape = torch.Size([128]))\n",
      "  (lin2): Linear(weight.shape = torch.Size([10, 128]) bias.shape = torch.Size([10]))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(1, 32, 3, 1, 1)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.max1 = MaxPool2d(2,2,0)\n",
    "        self.conv2 = Conv2d(32, 64, 3, 1, 1)\n",
    "        self.ReLU2 = ReLU()\n",
    "        self.max2 = MaxPool2d(2,2,0)\n",
    "        self.flatten = Flatten()\n",
    "        self.lin1 = Linear(3136, 128)\n",
    "        self.lin2 = Linear(128, 10)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        funcsToApply = [self.conv1, self.ReLU1, self.max1, self.conv2, self.ReLU2, self.max2, self.flatten, self.lin1, self.lin2]\n",
    "        result = x\n",
    "        for f in funcsToApply:\n",
    "            print(f)\n",
    "            result = f(x)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x = self.max1(self.ReLU1(self.conv1(x)))\n",
    "        x = self.max2(self.ReLU2(self.conv2(x)))\n",
    "        x = self.lin2(self.lin1(self.flatten(x)))\n",
    "        return x\n",
    "\n",
    "model = ConvNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "from torchvision import datasets, transforms \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief interlude on tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232aaf4e72704003928a4cf880de29bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7456820efa430bbbeeb6d636a3e1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005083e5cb9a423da1c0dbcb28983834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3cdad3f991475cab186a6b42321e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331f707efbaa488c9780dbc2a6bf2a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1982a450ac05466cac72c59983c5c1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [204], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m tqdm_notebook(\u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)):\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm_notebook(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m), leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> 9\u001b[0m         time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm_notebook(\u001b[39menumerate\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m))):\n\u001b[1;32m     11\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "import time\n",
    "\n",
    "for i in tqdm_notebook(range(100)):\n",
    "    time.sleep(0.01)\n",
    "\n",
    "for j in tqdm_notebook(range(5)):\n",
    "    for i in tqdm_notebook(range(100), leave=False):\n",
    "        time.sleep(0.01)\n",
    "for i in tqdm_notebook(enumerate(range(100))):\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a320e9b403204f49a734c5c979e28a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2853b04c39da4501ba94ae80ea2fb006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [205], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     t\u001b[39m.\u001b[39msave(model, MODEL_FILENAME)\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_list\n\u001b[0;32m---> 38\u001b[0m loss_list \u001b[39m=\u001b[39m train_convnet(trainloader, epochs, loss_fn)\n\u001b[1;32m     40\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mline(y\u001b[39m=\u001b[39mloss_list, template\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msimple_white\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCross entropy loss on MNIST\u001b[39m\u001b[39m\"\u001b[39m, yaxis_range\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m(loss_list)])\n",
      "Cell \u001b[0;32mIn [205], line 28\u001b[0m, in \u001b[0;36mtrain_convnet\u001b[0;34m(trainloader, epochs, loss_fn)\u001b[0m\n\u001b[1;32m     26\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_hat, y)\n\u001b[1;32m     27\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     30\u001b[0m     loss_list\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, train loss is \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/optim/adam.py:305\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    307\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "\n",
    "MODEL_FILENAME = \"./w1d2_convnet_mnist.pt\"\n",
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train_convnet(trainloader: DataLoader, epochs: int, loss_fn: Callable) -> list:\n",
    "    '''\n",
    "    Defines a ConvNet using our previous code, and trains it on the data in trainloader.\n",
    "    '''\n",
    "\n",
    "    model = ConvNet().to(device).train()\n",
    "    optimizer = t.optim.Adam(model.parameters())\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "\n",
    "        for (x, y) in tqdm_notebook(trainloader, leave=False):\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs}, train loss is {loss:.6f}\")\n",
    "\n",
    "    print(f\"Saving model to: {MODEL_FILENAME}\")\n",
    "    t.save(model, MODEL_FILENAME)\n",
    "    return loss_list\n",
    "\n",
    "loss_list = train_convnet(trainloader, epochs, loss_fn)\n",
    "\n",
    "fig = px.line(y=loss_list, template=\"simple_white\")\n",
    "fig.update_layout(title=\"Cross entropy loss on MNIST\", yaxis_range=[0, max(loss_list)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "editeded train_convenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training data\n",
    "\n",
    "testset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03bcd089c3347beac976d09d6d39e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a72a1b7b7d40dea02fe740886cbbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalCorrect =  tensor(16.), count = 128\n",
      "Epoch 0/1, accuracy is 0.125000\n",
      "totalCorrect =  tensor(46.), count = 128\n",
      "Epoch 0/1, accuracy is 0.359375\n",
      "totalCorrect =  tensor(51.), count = 128\n",
      "Epoch 0/1, accuracy is 0.398438\n",
      "totalCorrect =  tensor(34.), count = 128\n",
      "Epoch 0/1, accuracy is 0.265625\n",
      "totalCorrect =  tensor(35.), count = 128\n",
      "Epoch 0/1, accuracy is 0.273438\n",
      "totalCorrect =  tensor(51.), count = 128\n",
      "Epoch 0/1, accuracy is 0.398438\n",
      "totalCorrect =  tensor(77.), count = 128\n",
      "Epoch 0/1, accuracy is 0.601562\n",
      "totalCorrect =  tensor(87.), count = 128\n",
      "Epoch 0/1, accuracy is 0.679688\n",
      "totalCorrect =  tensor(92.), count = 128\n",
      "Epoch 0/1, accuracy is 0.718750\n",
      "totalCorrect =  tensor(91.), count = 128\n",
      "Epoch 0/1, accuracy is 0.710938\n",
      "totalCorrect =  tensor(92.), count = 128\n",
      "Epoch 0/1, accuracy is 0.718750\n",
      "totalCorrect =  tensor(91.), count = 128\n",
      "Epoch 0/1, accuracy is 0.710938\n",
      "totalCorrect =  tensor(100.), count = 128\n",
      "Epoch 0/1, accuracy is 0.781250\n",
      "totalCorrect =  tensor(102.), count = 128\n",
      "Epoch 0/1, accuracy is 0.796875\n",
      "totalCorrect =  tensor(103.), count = 128\n",
      "Epoch 0/1, accuracy is 0.804688\n",
      "totalCorrect =  tensor(108.), count = 128\n",
      "Epoch 0/1, accuracy is 0.843750\n",
      "totalCorrect =  tensor(97.), count = 128\n",
      "Epoch 0/1, accuracy is 0.757812\n",
      "totalCorrect =  tensor(101.), count = 128\n",
      "Epoch 0/1, accuracy is 0.789062\n",
      "totalCorrect =  tensor(96.), count = 128\n",
      "Epoch 0/1, accuracy is 0.750000\n",
      "totalCorrect =  tensor(105.), count = 128\n",
      "Epoch 0/1, accuracy is 0.820312\n",
      "totalCorrect =  tensor(104.), count = 128\n",
      "Epoch 0/1, accuracy is 0.812500\n",
      "totalCorrect =  tensor(103.), count = 128\n",
      "Epoch 0/1, accuracy is 0.804688\n",
      "totalCorrect =  tensor(107.), count = 128\n",
      "Epoch 0/1, accuracy is 0.835938\n",
      "totalCorrect =  tensor(114.), count = 128\n",
      "Epoch 0/1, accuracy is 0.890625\n",
      "totalCorrect =  tensor(113.), count = 128\n",
      "Epoch 0/1, accuracy is 0.882812\n",
      "totalCorrect =  tensor(118.), count = 128\n",
      "Epoch 0/1, accuracy is 0.921875\n",
      "totalCorrect =  tensor(116.), count = 128\n",
      "Epoch 0/1, accuracy is 0.906250\n",
      "totalCorrect =  tensor(108.), count = 128\n",
      "Epoch 0/1, accuracy is 0.843750\n",
      "totalCorrect =  tensor(111.), count = 128\n",
      "Epoch 0/1, accuracy is 0.867188\n",
      "totalCorrect =  tensor(115.), count = 128\n",
      "Epoch 0/1, accuracy is 0.898438\n",
      "totalCorrect =  tensor(114.), count = 128\n",
      "Epoch 0/1, accuracy is 0.890625\n",
      "totalCorrect =  tensor(112.), count = 128\n",
      "Epoch 0/1, accuracy is 0.875000\n",
      "totalCorrect =  tensor(114.), count = 128\n",
      "Epoch 0/1, accuracy is 0.890625\n",
      "totalCorrect =  tensor(117.), count = 128\n",
      "Epoch 0/1, accuracy is 0.914062\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(115.), count = 128\n",
      "Epoch 0/1, accuracy is 0.898438\n",
      "totalCorrect =  tensor(114.), count = 128\n",
      "Epoch 0/1, accuracy is 0.890625\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(115.), count = 128\n",
      "Epoch 0/1, accuracy is 0.898438\n",
      "totalCorrect =  tensor(114.), count = 128\n",
      "Epoch 0/1, accuracy is 0.890625\n",
      "totalCorrect =  tensor(116.), count = 128\n",
      "Epoch 0/1, accuracy is 0.906250\n",
      "totalCorrect =  tensor(113.), count = 128\n",
      "Epoch 0/1, accuracy is 0.882812\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(112.), count = 128\n",
      "Epoch 0/1, accuracy is 0.875000\n",
      "totalCorrect =  tensor(115.), count = 128\n",
      "Epoch 0/1, accuracy is 0.898438\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(118.), count = 128\n",
      "Epoch 0/1, accuracy is 0.921875\n",
      "totalCorrect =  tensor(118.), count = 128\n",
      "Epoch 0/1, accuracy is 0.921875\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(118.), count = 128\n",
      "Epoch 0/1, accuracy is 0.921875\n",
      "totalCorrect =  tensor(118.), count = 128\n",
      "Epoch 0/1, accuracy is 0.921875\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(118.), count = 128\n",
      "Epoch 0/1, accuracy is 0.921875\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(118.), count = 128\n",
      "Epoch 0/1, accuracy is 0.921875\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(118.), count = 128\n",
      "Epoch 0/1, accuracy is 0.921875\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(116.), count = 128\n",
      "Epoch 0/1, accuracy is 0.906250\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(117.), count = 128\n",
      "Epoch 0/1, accuracy is 0.914062\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(118.), count = 128\n",
      "Epoch 0/1, accuracy is 0.921875\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(115.), count = 128\n",
      "Epoch 0/1, accuracy is 0.898438\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(115.), count = 128\n",
      "Epoch 0/1, accuracy is 0.898438\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(116.), count = 128\n",
      "Epoch 0/1, accuracy is 0.906250\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(117.), count = 128\n",
      "Epoch 0/1, accuracy is 0.914062\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(120.), count = 128\n",
      "Epoch 0/1, accuracy is 0.937500\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(119.), count = 128\n",
      "Epoch 0/1, accuracy is 0.929688\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(121.), count = 128\n",
      "Epoch 0/1, accuracy is 0.945312\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(122.), count = 128\n",
      "Epoch 0/1, accuracy is 0.953125\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(123.), count = 128\n",
      "Epoch 0/1, accuracy is 0.960938\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(124.), count = 128\n",
      "Epoch 0/1, accuracy is 0.968750\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(128.), count = 128\n",
      "Epoch 0/1, accuracy is 1.000000\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(125.), count = 128\n",
      "Epoch 0/1, accuracy is 0.976562\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(126.), count = 128\n",
      "Epoch 0/1, accuracy is 0.984375\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "totalCorrect =  tensor(127.), count = 128\n",
      "Epoch 0/1, accuracy is 0.992188\n",
      "Epoch 0/1, train loss is 0.176662\n",
      "Saving model to: ./w1d2_convnet_mnist.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#1F77B4",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937
         ],
         "xaxis": "x",
         "y": [
          2.331237554550171,
          2.5845046043395996,
          2.2018136978149414,
          2.1014628410339355,
          1.9794058799743652,
          1.9245765209197998,
          1.8603531122207642,
          1.7199254035949707,
          1.436248779296875,
          1.261235237121582,
          1.3151167631149292,
          1.0026297569274902,
          0.9952118992805481,
          0.7808892726898193,
          0.8642293214797974,
          0.7781838774681091,
          0.5753024816513062,
          0.669155478477478,
          0.8757992386817932,
          0.5567652583122253,
          0.41176238656044006,
          0.6836579442024231,
          0.5282265543937683,
          0.9314320683479309,
          0.6118572950363159,
          0.46891212463378906,
          0.4937220811843872,
          0.5172354578971863,
          0.38836178183555603,
          0.6449170708656311,
          0.35505959391593933,
          0.644468367099762,
          0.36339646577835083,
          0.27516692876815796,
          0.566165030002594,
          0.4106357991695404,
          0.2511554956436157,
          0.3454902172088623,
          0.272968053817749,
          0.5337721109390259,
          0.2231745719909668,
          0.32453352212905884,
          0.2776234745979309,
          0.22888381779193878,
          0.2914147675037384,
          0.5039643049240112,
          0.24505160748958588,
          0.3601222336292267,
          0.2860569357872009,
          0.3133428990840912,
          0.32978442311286926,
          0.3946802020072937,
          0.24087077379226685,
          0.21735765039920807,
          0.21597407758235931,
          0.4963671863079071,
          0.14095383882522583,
          0.16269025206565857,
          0.28000494837760925,
          0.12413530796766281,
          0.39485082030296326,
          0.2458440661430359,
          0.27083417773246765,
          0.32845935225486755,
          0.19741888344287872,
          0.2770017981529236,
          0.22925572097301483,
          0.19012075662612915,
          0.3044131100177765,
          0.26705658435821533,
          0.3575960397720337,
          0.17239056527614594,
          0.3327254354953766,
          0.27730295062065125,
          0.25729164481163025,
          0.2182614803314209,
          0.23526977002620697,
          0.3015882670879364,
          0.15848691761493683,
          0.14135092496871948,
          0.17616280913352966,
          0.17061777412891388,
          0.2642977833747864,
          0.20398053526878357,
          0.1725682020187378,
          0.23067958652973175,
          0.28164276480674744,
          0.2820056676864624,
          0.1807880848646164,
          0.15739385783672333,
          0.14275090396404266,
          0.11970864981412888,
          0.2268323451280594,
          0.2109413743019104,
          0.1254720389842987,
          0.16769464313983917,
          0.1162676215171814,
          0.32075944542884827,
          0.07100322097539902,
          0.05954546853899956,
          0.07646159827709198,
          0.20716537535190582,
          0.2109852433204651,
          0.13042744994163513,
          0.09902089834213257,
          0.2652398347854614,
          0.10935817658901215,
          0.21645542979240417,
          0.1393635869026184,
          0.040584392845630646,
          0.21863405406475067,
          0.1604006141424179,
          0.1960093230009079,
          0.10218381881713867,
          0.0742819532752037,
          0.09330180287361145,
          0.22241438925266266,
          0.065365731716156,
          0.34312549233436584,
          0.2238159030675888,
          0.148932084441185,
          0.10701414942741394,
          0.14836950600147247,
          0.4412355124950409,
          0.07814881205558777,
          0.24170322716236115,
          0.15595084428787231,
          0.150714710354805,
          0.11986440420150757,
          0.11623943597078323,
          0.15263961255550385,
          0.0733521431684494,
          0.06520651280879974,
          0.17296496033668518,
          0.24390795826911926,
          0.17585355043411255,
          0.18678025901317596,
          0.07144414633512497,
          0.09173859655857086,
          0.13096484541893005,
          0.37124791741371155,
          0.1728316694498062,
          0.055010534822940826,
          0.08491582423448563,
          0.20361049473285675,
          0.2834659814834595,
          0.09603170305490494,
          0.15828126668930054,
          0.1744382232427597,
          0.12726271152496338,
          0.08309711515903473,
          0.14354489743709564,
          0.16374294459819794,
          0.12662237882614136,
          0.15577301383018494,
          0.11235690861940384,
          0.16532936692237854,
          0.13126151263713837,
          0.2469944804906845,
          0.05459572374820709,
          0.13740216195583344,
          0.07367783039808273,
          0.11426743865013123,
          0.12033811956644058,
          0.1476089358329773,
          0.08575210720300674,
          0.06755857914686203,
          0.07749724388122559,
          0.14565224945545197,
          0.12570615112781525,
          0.13410881161689758,
          0.2622210383415222,
          0.11863920837640762,
          0.13803406059741974,
          0.10697649419307709,
          0.1440528780221939,
          0.18827247619628906,
          0.05406224727630615,
          0.05666511878371239,
          0.09844417870044708,
          0.3165404498577118,
          0.0982203409075737,
          0.06795245409011841,
          0.13449208438396454,
          0.10234089940786362,
          0.13735385239124298,
          0.19454756379127502,
          0.10191889107227325,
          0.07473862916231155,
          0.05117741972208023,
          0.043851256370544434,
          0.07425299286842346,
          0.1771479696035385,
          0.18843995034694672,
          0.09940963983535767,
          0.09123626351356506,
          0.15274733304977417,
          0.05389877408742905,
          0.1558799296617508,
          0.0588410422205925,
          0.2950197458267212,
          0.18486016988754272,
          0.0707898885011673,
          0.12694145739078522,
          0.04546215757727623,
          0.15687140822410583,
          0.11601664870977402,
          0.12498507648706436,
          0.04047508165240288,
          0.09349698573350906,
          0.22603045403957367,
          0.06581740081310272,
          0.12811137735843658,
          0.08209162950515747,
          0.1944200098514557,
          0.15918689966201782,
          0.07554911822080612,
          0.1337892860174179,
          0.12545868754386902,
          0.038169339299201965,
          0.1385231912136078,
          0.08249668776988983,
          0.15481051802635193,
          0.04511353373527527,
          0.15621256828308105,
          0.10407675057649612,
          0.14743849635124207,
          0.1439347267150879,
          0.13205912709236145,
          0.0743672177195549,
          0.12686371803283691,
          0.039061639457941055,
          0.16752250492572784,
          0.04324730858206749,
          0.03630588948726654,
          0.1061754897236824,
          0.039582010358572006,
          0.059633318334817886,
          0.09998436272144318,
          0.09402236342430115,
          0.07147624343633652,
          0.1668338179588318,
          0.12415080517530441,
          0.12807485461235046,
          0.1565973162651062,
          0.1102064773440361,
          0.09560535103082657,
          0.23753835260868073,
          0.06401009857654572,
          0.09876875579357147,
          0.07747947424650192,
          0.03243454173207283,
          0.13030096888542175,
          0.10704423487186432,
          0.05360526591539383,
          0.04150238633155823,
          0.13120925426483154,
          0.32463374733924866,
          0.05269504338502884,
          0.13603849709033966,
          0.13196751475334167,
          0.03787241876125336,
          0.02555042691528797,
          0.08964625000953674,
          0.36918774247169495,
          0.12191659212112427,
          0.18150323629379272,
          0.0968034639954567,
          0.09891147911548615,
          0.032549090683460236,
          0.21051561832427979,
          0.14370441436767578,
          0.07172979414463043,
          0.05590708553791046,
          0.05074958875775337,
          0.09746062755584717,
          0.069449782371521,
          0.028148556128144264,
          0.04763277620077133,
          0.06499578803777695,
          0.05478407070040703,
          0.16126276552677155,
          0.06702499091625214,
          0.023863226175308228,
          0.08004098385572433,
          0.006818434223532677,
          0.11858616769313812,
          0.11896559596061707,
          0.06535419821739197,
          0.13191941380500793,
          0.03912774473428726,
          0.12100379168987274,
          0.06768541038036346,
          0.032835789024829865,
          0.13940288126468658,
          0.09010854363441467,
          0.125422865152359,
          0.27678388357162476,
          0.18162594735622406,
          0.032696835696697235,
          0.13517077267169952,
          0.0979388952255249,
          0.043031759560108185,
          0.14085626602172852,
          0.18699002265930176,
          0.15420971810817719,
          0.09175878763198853,
          0.05469061806797981,
          0.021020885556936264,
          0.02328241616487503,
          0.03031674213707447,
          0.06591667234897614,
          0.16711270809173584,
          0.14231520891189575,
          0.01877315156161785,
          0.062144238501787186,
          0.04215523228049278,
          0.2565217614173889,
          0.07622987031936646,
          0.07339015603065491,
          0.039326757192611694,
          0.06045305356383324,
          0.08708155900239944,
          0.0721934586763382,
          0.11217623949050903,
          0.06891533732414246,
          0.06347972899675369,
          0.13173215091228485,
          0.14980949461460114,
          0.047604773193597794,
          0.13673588633537292,
          0.032357677817344666,
          0.09158878028392792,
          0.02630101516842842,
          0.20196415483951569,
          0.19099192321300507,
          0.15021951496601105,
          0.0861702635884285,
          0.13642562925815582,
          0.14811517298221588,
          0.08120335638523102,
          0.2484349012374878,
          0.07042357325553894,
          0.05389343947172165,
          0.06918280571699142,
          0.14136333763599396,
          0.1621934324502945,
          0.11118446290493011,
          0.05099327117204666,
          0.013545392081141472,
          0.06164772808551788,
          0.2527846097946167,
          0.11115012317895889,
          0.22837591171264648,
          0.045348215848207474,
          0.2565711736679077,
          0.06516847014427185,
          0.09139350056648254,
          0.10840141773223877,
          0.03631338104605675,
          0.022116493433713913,
          0.07768262922763824,
          0.13977204263210297,
          0.059923142194747925,
          0.18030038475990295,
          0.15590690076351166,
          0.02183743380010128,
          0.09177614748477936,
          0.14013704657554626,
          0.058148790150880814,
          0.12329152971506119,
          0.09211359918117523,
          0.031081179156899452,
          0.03755341097712517,
          0.06976716220378876,
          0.06938672810792923,
          0.11624594032764435,
          0.003875737078487873,
          0.016680754721164703,
          0.03578180447220802,
          0.10714047402143478,
          0.031086333096027374,
          0.018963444977998734,
          0.037245575338602066,
          0.02040587179362774,
          0.02732422947883606,
          0.0825718566775322,
          0.03411601483821869,
          0.21002383530139923,
          0.20414304733276367,
          0.03609439730644226,
          0.04002239182591438,
          0.08519665896892548,
          0.08446865528821945,
          0.06402698159217834,
          0.07064077258110046,
          0.11829039454460144,
          0.07133869081735611,
          0.1078905314207077,
          0.05391523987054825,
          0.14970044791698456,
          0.022567786276340485,
          0.1044771671295166,
          0.19370318949222565,
          0.023287275806069374,
          0.1300734579563141,
          0.13467320799827576,
          0.03830840066075325,
          0.05529572069644928,
          0.03657108172774315,
          0.126560240983963,
          0.011604824103415012,
          0.0531381331384182,
          0.2551487982273102,
          0.0226815827190876,
          0.2667544186115265,
          0.010410137474536896,
          0.03573554381728172,
          0.08132339268922806,
          0.05392631143331528,
          0.1077403873205185,
          0.17067056894302368,
          0.02142445556819439,
          0.18110516667366028,
          0.12917475402355194,
          0.12969861924648285,
          0.1599651724100113,
          0.05297074466943741,
          0.07059013843536377,
          0.09722088277339935,
          0.09862443059682846,
          0.07090422511100769,
          0.06933877617120743,
          0.043973375111818314,
          0.02710116282105446,
          0.11240304261445999,
          0.018623048439621925,
          0.12756969034671783,
          0.05986132100224495,
          0.07904878258705139,
          0.06571254879236221,
          0.2226005643606186,
          0.17113330960273743,
          0.08400214463472366,
          0.07821135222911835,
          0.10852092504501343,
          0.18163993954658508,
          0.06713001430034637,
          0.19281655550003052,
          0.02381584793329239,
          0.10708031803369522,
          0.06764204055070877,
          0.07300226390361786,
          0.13123029470443726,
          0.09512386471033096,
          0.043000057339668274,
          0.16707782447338104,
          0.03775925561785698,
          0.022762654349207878,
          0.159568652510643,
          0.00790361501276493,
          0.05436427891254425,
          0.2009512484073639,
          0.02866009622812271,
          0.012085523456335068,
          0.027375634759664536,
          0.14818306267261505,
          0.2417328804731369,
          0.04755871370434761,
          0.08826134353876114,
          0.009402818977832794,
          0.0551588274538517,
          0.03960604965686798,
          0.1178559958934784,
          0.05535683408379555,
          0.00769450468942523,
          0.03266020119190216,
          0.17865191400051117,
          0.060407303273677826,
          0.037504129111766815,
          0.05734899640083313,
          0.017675234004855156,
          0.01945217326283455,
          0.0824701339006424,
          0.08183044195175171,
          0.09602337330579758,
          0.11008979380130768,
          0.009253992699086666,
          0.10938314348459244,
          0.027643078938126564,
          0.022327013313770294,
          0.06929749250411987,
          0.06663773953914642,
          0.07451441884040833,
          0.037648361176252365,
          0.10960234701633453,
          0.18333613872528076,
          0.1132798120379448,
          0.06927642971277237,
          0.07014281302690506,
          0.05894933640956879,
          0.03236265107989311,
          0.03966107964515686,
          0.03299375995993614,
          0.09090066701173782,
          0.016057465225458145,
          0.21175560355186462,
          0.04226519167423248,
          0.011494758538901806,
          0.09809659421443939,
          0.05520237982273102,
          0.008688651025295258,
          0.012761645019054413,
          0.00689086876809597,
          0.02100689522922039,
          0.02541923336684704,
          0.031770896166563034,
          0.06091434136033058,
          0.05757772922515869,
          0.15535105764865875,
          0.02590141072869301,
          0.08383020758628845,
          0.08063820004463196,
          0.011979544535279274,
          0.1084488183259964,
          0.09480953216552734,
          0.0532040037214756,
          0.03946731984615326,
          0.050018422305583954,
          0.025477027520537376,
          0.03519541770219803,
          0.16361762583255768,
          0.0832187831401825,
          0.008958583697676659,
          0.0435275137424469,
          0.02073429897427559,
          0.10364540666341782,
          0.03033740445971489,
          0.020826706662774086,
          0.11533639580011368,
          0.025929423049092293,
          0.06947226822376251,
          0.03683900460600853,
          0.05639968439936638,
          0.011727492325007915,
          0.06720773130655289,
          0.19851258397102356,
          0.12375106662511826,
          0.015660326927900314,
          0.16464132070541382,
          0.06764737516641617,
          0.15767690539360046,
          0.07877252250909805,
          0.17574340105056763,
          0.13409928977489471,
          0.011680133640766144,
          0.021855568513274193,
          0.007742850575596094,
          0.0979437455534935,
          0.025827525183558464,
          0.12475645542144775,
          0.012445805594325066,
          0.011535191908478737,
          0.029722411185503006,
          0.10032036900520325,
          0.18481524288654327,
          0.047279637306928635,
          0.008049078285694122,
          0.049876630306243896,
          0.016303181648254395,
          0.20016880333423615,
          0.04157808795571327,
          0.12749415636062622,
          0.1002865582704544,
          0.0668569803237915,
          0.0202542282640934,
          0.06176857650279999,
          0.01740947924554348,
          0.10049232840538025,
          0.03300164267420769,
          0.07470838725566864,
          0.11543247103691101,
          0.01354127749800682,
          0.0801621749997139,
          0.015794852748513222,
          0.24928517639636993,
          0.19558697938919067,
          0.05750538781285286,
          0.14815424382686615,
          0.05323963239789009,
          0.12966051697731018,
          0.026743344962596893,
          0.05924556031823158,
          0.016620095819234848,
          0.04543603956699371,
          0.014284937642514706,
          0.17107325792312622,
          0.03244685381650925,
          0.2728511691093445,
          0.017180610448122025,
          0.15008597075939178,
          0.04679955914616585,
          0.02752002514898777,
          0.07607056200504303,
          0.07116662710905075,
          0.12081148475408554,
          0.04495232552289963,
          0.03553233668208122,
          0.04976800084114075,
          0.06344668567180634,
          0.02417564205825329,
          0.2172485888004303,
          0.135087788105011,
          0.18346154689788818,
          0.01434139721095562,
          0.041314832866191864,
          0.036148570477962494,
          0.17074023187160492,
          0.20074598491191864,
          0.0465184710919857,
          0.04656139761209488,
          0.09074269235134125,
          0.13134953379631042,
          0.12046390771865845,
          0.03596886619925499,
          0.06638218462467194,
          0.017095113173127174,
          0.1577509194612503,
          0.01376262865960598,
          0.09144233167171478,
          0.04630964249372482,
          0.07044745981693268,
          0.10854149609804153,
          0.020754963159561157,
          0.24090562760829926,
          0.02962852641940117,
          0.09682801365852356,
          0.07157934457063675,
          0.08072753250598907,
          0.04557653144001961,
          0.04650453105568886,
          0.03481931984424591,
          0.10561466962099075,
          0.05777760222554207,
          0.054579101502895355,
          0.07589831948280334,
          0.0227473434060812,
          0.019710829481482506,
          0.050981555134058,
          0.01940426230430603,
          0.0557764507830143,
          0.02935667335987091,
          0.03503074124455452,
          0.11966779828071594,
          0.17447307705879211,
          0.09775643050670624,
          0.0529664047062397,
          0.009816097095608711,
          0.040922198444604874,
          0.11849801987409592,
          0.0825897827744484,
          0.04723094403743744,
          0.07778844982385635,
          0.07772127538919449,
          0.014280019327998161,
          0.06756508350372314,
          0.06210945174098015,
          0.01560366339981556,
          0.07466326653957367,
          0.07102707028388977,
          0.043520260602235794,
          0.04922665283083916,
          0.09022198617458344,
          0.052058011293411255,
          0.11370164901018143,
          0.08655629307031631,
          0.022975722327828407,
          0.08084956556558609,
          0.11864224821329117,
          0.09764689207077026,
          0.0544651634991169,
          0.14562469720840454,
          0.020412951707839966,
          0.04423479735851288,
          0.01815093494951725,
          0.016827313229441643,
          0.0655818060040474,
          0.11338304728269577,
          0.1620291918516159,
          0.12194357812404633,
          0.13269153237342834,
          0.05843109264969826,
          0.022942056879401207,
          0.1375088393688202,
          0.012032845988869667,
          0.0318741649389267,
          0.1359531730413437,
          0.012985083274543285,
          0.08031459152698517,
          0.18915726244449615,
          0.05170956999063492,
          0.024695971980690956,
          0.1923864632844925,
          0.01270428579300642,
          0.028362996876239777,
          0.13385215401649475,
          0.05279628932476044,
          0.03365145996212959,
          0.008933031931519508,
          0.035476524382829666,
          0.07625341415405273,
          0.0376192070543766,
          0.14258038997650146,
          0.15739400684833527,
          0.02695326879620552,
          0.04533413425087929,
          0.06601759046316147,
          0.0633625015616417,
          0.05478585883975029,
          0.05899692326784134,
          0.01939455419778824,
          0.24339412152767181,
          0.011680309660732746,
          0.1765671670436859,
          0.039567530155181885,
          0.05782528966665268,
          0.07880096137523651,
          0.11671436578035355,
          0.0887044221162796,
          0.02685440331697464,
          0.21110591292381287,
          0.058429744094610214,
          0.1271311491727829,
          0.02135646529495716,
          0.0675918385386467,
          0.12716302275657654,
          0.02639557234942913,
          0.03987441584467888,
          0.06453710794448853,
          0.08835391700267792,
          0.03484601154923439,
          0.06171925365924835,
          0.01462487131357193,
          0.04693052917718887,
          0.18482035398483276,
          0.04036220908164978,
          0.00915798731148243,
          0.11242897063493729,
          0.012395787984132767,
          0.026894276961684227,
          0.01647486723959446,
          0.057143304497003555,
          0.045527130365371704,
          0.009583665058016777,
          0.05948025360703468,
          0.0405912883579731,
          0.020036863163113594,
          0.02839156612753868,
          0.08524361252784729,
          0.04691242054104805,
          0.2226642668247223,
          0.13334597647190094,
          0.030241120606660843,
          0.17245787382125854,
          0.009860207326710224,
          0.03695956617593765,
          0.06673994660377502,
          0.02142646722495556,
          0.07052136212587357,
          0.009784466587007046,
          0.02902841754257679,
          0.02299356274306774,
          0.16961245238780975,
          0.0580909326672554,
          0.02966969460248947,
          0.08991440385580063,
          0.03214108943939209,
          0.08114350587129593,
          0.0543171763420105,
          0.024650346487760544,
          0.016711141914129257,
          0.11070804297924042,
          0.03824852779507637,
          0.0636652484536171,
          0.004190366715192795,
          0.15144331753253937,
          0.018258819356560707,
          0.02365322969853878,
          0.0157737135887146,
          0.06810855120420456,
          0.020657449960708618,
          0.1278119832277298,
          0.06161996349692345,
          0.03232252597808838,
          0.0368351973593235,
          0.008308803662657738,
          0.026223523542284966,
          0.017343513667583466,
          0.06836672872304916,
          0.018468378111720085,
          0.05279332771897316,
          0.05383490398526192,
          0.03014557436108589,
          0.01097307913005352,
          0.08068902790546417,
          0.17751272022724152,
          0.008550389669835567,
          0.010060643777251244,
          0.026706283912062645,
          0.01462855376303196,
          0.04245421662926674,
          0.013735370710492134,
          0.18443132936954498,
          0.13156971335411072,
          0.012384435161948204,
          0.12122111022472382,
          0.08662338554859161,
          0.013191213831305504,
          0.260727196931839,
          0.19027838110923767,
          0.029868297278881073,
          0.05960763618350029,
          0.03716201335191727,
          0.1550433486700058,
          0.11745566874742508,
          0.012857770547270775,
          0.09518302232027054,
          0.015613926574587822,
          0.04053834080696106,
          0.00696048466488719,
          0.009288182482123375,
          0.09524525701999664,
          0.06936225295066833,
          0.061048008501529694,
          0.008126853965222836,
          0.06438719481229782,
          0.10590451210737228,
          0.0561826191842556,
          0.01921793445944786,
          0.13514229655265808,
          0.06763844192028046,
          0.06275954097509384,
          0.042742565274238586,
          0.030835706740617752,
          0.015967264771461487,
          0.0399172380566597,
          0.04697860777378082,
          0.049240194261074066,
          0.215492844581604,
          0.01452433131635189,
          0.031354449689388275,
          0.01924813725054264,
          0.03712226450443268,
          0.048396144062280655,
          0.05959115922451019,
          0.18877562880516052,
          0.1390761286020279,
          0.014468387700617313,
          0.11081364750862122,
          0.04692620411515236,
          0.045398931950330734,
          0.05354354530572891,
          0.09655370563268661,
          0.06209934130311012,
          0.018136924132704735,
          0.07155442237854004,
          0.03002355992794037,
          0.02033824287354946,
          0.14076104760169983,
          0.03960954397916794,
          0.06926875561475754,
          0.006143581122159958,
          0.008253401145339012,
          0.05556657537817955,
          0.006934124045073986,
          0.013524515554308891,
          0.07523559033870697,
          0.08153628557920456,
          0.06240678206086159,
          0.11062991619110107,
          0.026935217902064323,
          0.03411963954567909,
          0.12377077341079712,
          0.072057344019413,
          0.10419969260692596,
          0.056552741676568985,
          0.024178914725780487,
          0.15001967549324036,
          0.08149032294750214,
          0.10332557559013367,
          0.09013059735298157,
          0.05744776129722595,
          0.05336739122867584,
          0.06602834910154343,
          0.010987887158989906,
          0.02496912330389023,
          0.10900046676397324,
          0.027501408010721207,
          0.005975260399281979,
          0.05607832595705986,
          0.01840880885720253,
          0.037542007863521576,
          0.013571193441748619,
          0.03573305904865265,
          0.016529418528079987,
          0.042945414781570435,
          0.1834374964237213,
          0.08235371857881546,
          0.09752996265888214,
          0.11269225180149078,
          0.037053242325782776,
          0.032510120421648026,
          0.03766245022416115,
          0.036923233419656754,
          0.03226635605096817,
          0.07683006674051285,
          0.04704893007874489,
          0.10850674659013748,
          0.04112601280212402,
          0.04535406455397606,
          0.006781138479709625,
          0.1291298121213913,
          0.03628505393862724,
          0.041619617491960526,
          0.060054291039705276,
          0.08379005640745163,
          0.027744514867663383,
          0.02262788638472557,
          0.0315641425549984,
          0.02864151820540428,
          0.02272946573793888,
          0.037160541862249374,
          0.04674962908029556,
          0.09197048097848892,
          0.0028038909658789635,
          0.1299569457769394,
          0.003891443135216832,
          0.1766621619462967
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "text": "Cross entropy loss on MNIST"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          2.5845046043395996
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Loss",
         "type": "scatter",
         "xaxis": "x",
         "y": [
          2.331237554550171,
          2.5845046043395996,
          2.2018136978149414,
          2.1014628410339355,
          1.9794058799743652,
          1.9245765209197998,
          1.8603531122207642,
          1.7199254035949707,
          1.436248779296875,
          1.261235237121582,
          1.3151167631149292,
          1.0026297569274902,
          0.9952118992805481,
          0.7808892726898193,
          0.8642293214797974,
          0.7781838774681091,
          0.5753024816513062,
          0.669155478477478,
          0.8757992386817932,
          0.5567652583122253,
          0.41176238656044006,
          0.6836579442024231,
          0.5282265543937683,
          0.9314320683479309,
          0.6118572950363159,
          0.46891212463378906,
          0.4937220811843872,
          0.5172354578971863,
          0.38836178183555603,
          0.6449170708656311,
          0.35505959391593933,
          0.644468367099762,
          0.36339646577835083,
          0.27516692876815796,
          0.566165030002594,
          0.4106357991695404,
          0.2511554956436157,
          0.3454902172088623,
          0.272968053817749,
          0.5337721109390259,
          0.2231745719909668,
          0.32453352212905884,
          0.2776234745979309,
          0.22888381779193878,
          0.2914147675037384,
          0.5039643049240112,
          0.24505160748958588,
          0.3601222336292267,
          0.2860569357872009,
          0.3133428990840912,
          0.32978442311286926,
          0.3946802020072937,
          0.24087077379226685,
          0.21735765039920807,
          0.21597407758235931,
          0.4963671863079071,
          0.14095383882522583,
          0.16269025206565857,
          0.28000494837760925,
          0.12413530796766281,
          0.39485082030296326,
          0.2458440661430359,
          0.27083417773246765,
          0.32845935225486755,
          0.19741888344287872,
          0.2770017981529236,
          0.22925572097301483,
          0.19012075662612915,
          0.3044131100177765,
          0.26705658435821533,
          0.3575960397720337,
          0.17239056527614594,
          0.3327254354953766,
          0.27730295062065125,
          0.25729164481163025,
          0.2182614803314209,
          0.23526977002620697,
          0.3015882670879364,
          0.15848691761493683,
          0.14135092496871948,
          0.17616280913352966,
          0.17061777412891388,
          0.2642977833747864,
          0.20398053526878357,
          0.1725682020187378,
          0.23067958652973175,
          0.28164276480674744,
          0.2820056676864624,
          0.1807880848646164,
          0.15739385783672333,
          0.14275090396404266,
          0.11970864981412888,
          0.2268323451280594,
          0.2109413743019104,
          0.1254720389842987,
          0.16769464313983917,
          0.1162676215171814,
          0.32075944542884827,
          0.07100322097539902,
          0.05954546853899956,
          0.07646159827709198,
          0.20716537535190582,
          0.2109852433204651,
          0.13042744994163513,
          0.09902089834213257,
          0.2652398347854614,
          0.10935817658901215,
          0.21645542979240417,
          0.1393635869026184,
          0.040584392845630646,
          0.21863405406475067,
          0.1604006141424179,
          0.1960093230009079,
          0.10218381881713867,
          0.0742819532752037,
          0.09330180287361145,
          0.22241438925266266,
          0.065365731716156,
          0.34312549233436584,
          0.2238159030675888,
          0.148932084441185,
          0.10701414942741394,
          0.14836950600147247,
          0.4412355124950409,
          0.07814881205558777,
          0.24170322716236115,
          0.15595084428787231,
          0.150714710354805,
          0.11986440420150757,
          0.11623943597078323,
          0.15263961255550385,
          0.0733521431684494,
          0.06520651280879974,
          0.17296496033668518,
          0.24390795826911926,
          0.17585355043411255,
          0.18678025901317596,
          0.07144414633512497,
          0.09173859655857086,
          0.13096484541893005,
          0.37124791741371155,
          0.1728316694498062,
          0.055010534822940826,
          0.08491582423448563,
          0.20361049473285675,
          0.2834659814834595,
          0.09603170305490494,
          0.15828126668930054,
          0.1744382232427597,
          0.12726271152496338,
          0.08309711515903473,
          0.14354489743709564,
          0.16374294459819794,
          0.12662237882614136,
          0.15577301383018494,
          0.11235690861940384,
          0.16532936692237854,
          0.13126151263713837,
          0.2469944804906845,
          0.05459572374820709,
          0.13740216195583344,
          0.07367783039808273,
          0.11426743865013123,
          0.12033811956644058,
          0.1476089358329773,
          0.08575210720300674,
          0.06755857914686203,
          0.07749724388122559,
          0.14565224945545197,
          0.12570615112781525,
          0.13410881161689758,
          0.2622210383415222,
          0.11863920837640762,
          0.13803406059741974,
          0.10697649419307709,
          0.1440528780221939,
          0.18827247619628906,
          0.05406224727630615,
          0.05666511878371239,
          0.09844417870044708,
          0.3165404498577118,
          0.0982203409075737,
          0.06795245409011841,
          0.13449208438396454,
          0.10234089940786362,
          0.13735385239124298,
          0.19454756379127502,
          0.10191889107227325,
          0.07473862916231155,
          0.05117741972208023,
          0.043851256370544434,
          0.07425299286842346,
          0.1771479696035385,
          0.18843995034694672,
          0.09940963983535767,
          0.09123626351356506,
          0.15274733304977417,
          0.05389877408742905,
          0.1558799296617508,
          0.0588410422205925,
          0.2950197458267212,
          0.18486016988754272,
          0.0707898885011673,
          0.12694145739078522,
          0.04546215757727623,
          0.15687140822410583,
          0.11601664870977402,
          0.12498507648706436,
          0.04047508165240288,
          0.09349698573350906,
          0.22603045403957367,
          0.06581740081310272,
          0.12811137735843658,
          0.08209162950515747,
          0.1944200098514557,
          0.15918689966201782,
          0.07554911822080612,
          0.1337892860174179,
          0.12545868754386902,
          0.038169339299201965,
          0.1385231912136078,
          0.08249668776988983,
          0.15481051802635193,
          0.04511353373527527,
          0.15621256828308105,
          0.10407675057649612,
          0.14743849635124207,
          0.1439347267150879,
          0.13205912709236145,
          0.0743672177195549,
          0.12686371803283691,
          0.039061639457941055,
          0.16752250492572784,
          0.04324730858206749,
          0.03630588948726654,
          0.1061754897236824,
          0.039582010358572006,
          0.059633318334817886,
          0.09998436272144318,
          0.09402236342430115,
          0.07147624343633652,
          0.1668338179588318,
          0.12415080517530441,
          0.12807485461235046,
          0.1565973162651062,
          0.1102064773440361,
          0.09560535103082657,
          0.23753835260868073,
          0.06401009857654572,
          0.09876875579357147,
          0.07747947424650192,
          0.03243454173207283,
          0.13030096888542175,
          0.10704423487186432,
          0.05360526591539383,
          0.04150238633155823,
          0.13120925426483154,
          0.32463374733924866,
          0.05269504338502884,
          0.13603849709033966,
          0.13196751475334167,
          0.03787241876125336,
          0.02555042691528797,
          0.08964625000953674,
          0.36918774247169495,
          0.12191659212112427,
          0.18150323629379272,
          0.0968034639954567,
          0.09891147911548615,
          0.032549090683460236,
          0.21051561832427979,
          0.14370441436767578,
          0.07172979414463043,
          0.05590708553791046,
          0.05074958875775337,
          0.09746062755584717,
          0.069449782371521,
          0.028148556128144264,
          0.04763277620077133,
          0.06499578803777695,
          0.05478407070040703,
          0.16126276552677155,
          0.06702499091625214,
          0.023863226175308228,
          0.08004098385572433,
          0.006818434223532677,
          0.11858616769313812,
          0.11896559596061707,
          0.06535419821739197,
          0.13191941380500793,
          0.03912774473428726,
          0.12100379168987274,
          0.06768541038036346,
          0.032835789024829865,
          0.13940288126468658,
          0.09010854363441467,
          0.125422865152359,
          0.27678388357162476,
          0.18162594735622406,
          0.032696835696697235,
          0.13517077267169952,
          0.0979388952255249,
          0.043031759560108185,
          0.14085626602172852,
          0.18699002265930176,
          0.15420971810817719,
          0.09175878763198853,
          0.05469061806797981,
          0.021020885556936264,
          0.02328241616487503,
          0.03031674213707447,
          0.06591667234897614,
          0.16711270809173584,
          0.14231520891189575,
          0.01877315156161785,
          0.062144238501787186,
          0.04215523228049278,
          0.2565217614173889,
          0.07622987031936646,
          0.07339015603065491,
          0.039326757192611694,
          0.06045305356383324,
          0.08708155900239944,
          0.0721934586763382,
          0.11217623949050903,
          0.06891533732414246,
          0.06347972899675369,
          0.13173215091228485,
          0.14980949461460114,
          0.047604773193597794,
          0.13673588633537292,
          0.032357677817344666,
          0.09158878028392792,
          0.02630101516842842,
          0.20196415483951569,
          0.19099192321300507,
          0.15021951496601105,
          0.0861702635884285,
          0.13642562925815582,
          0.14811517298221588,
          0.08120335638523102,
          0.2484349012374878,
          0.07042357325553894,
          0.05389343947172165,
          0.06918280571699142,
          0.14136333763599396,
          0.1621934324502945,
          0.11118446290493011,
          0.05099327117204666,
          0.013545392081141472,
          0.06164772808551788,
          0.2527846097946167,
          0.11115012317895889,
          0.22837591171264648,
          0.045348215848207474,
          0.2565711736679077,
          0.06516847014427185,
          0.09139350056648254,
          0.10840141773223877,
          0.03631338104605675,
          0.022116493433713913,
          0.07768262922763824,
          0.13977204263210297,
          0.059923142194747925,
          0.18030038475990295,
          0.15590690076351166,
          0.02183743380010128,
          0.09177614748477936,
          0.14013704657554626,
          0.058148790150880814,
          0.12329152971506119,
          0.09211359918117523,
          0.031081179156899452,
          0.03755341097712517,
          0.06976716220378876,
          0.06938672810792923,
          0.11624594032764435,
          0.003875737078487873,
          0.016680754721164703,
          0.03578180447220802,
          0.10714047402143478,
          0.031086333096027374,
          0.018963444977998734,
          0.037245575338602066,
          0.02040587179362774,
          0.02732422947883606,
          0.0825718566775322,
          0.03411601483821869,
          0.21002383530139923,
          0.20414304733276367,
          0.03609439730644226,
          0.04002239182591438,
          0.08519665896892548,
          0.08446865528821945,
          0.06402698159217834,
          0.07064077258110046,
          0.11829039454460144,
          0.07133869081735611,
          0.1078905314207077,
          0.05391523987054825,
          0.14970044791698456,
          0.022567786276340485,
          0.1044771671295166,
          0.19370318949222565,
          0.023287275806069374,
          0.1300734579563141,
          0.13467320799827576,
          0.03830840066075325,
          0.05529572069644928,
          0.03657108172774315,
          0.126560240983963,
          0.011604824103415012,
          0.0531381331384182,
          0.2551487982273102,
          0.0226815827190876,
          0.2667544186115265,
          0.010410137474536896,
          0.03573554381728172,
          0.08132339268922806,
          0.05392631143331528,
          0.1077403873205185,
          0.17067056894302368,
          0.02142445556819439,
          0.18110516667366028,
          0.12917475402355194,
          0.12969861924648285,
          0.1599651724100113,
          0.05297074466943741,
          0.07059013843536377,
          0.09722088277339935,
          0.09862443059682846,
          0.07090422511100769,
          0.06933877617120743,
          0.043973375111818314,
          0.02710116282105446,
          0.11240304261445999,
          0.018623048439621925,
          0.12756969034671783,
          0.05986132100224495,
          0.07904878258705139,
          0.06571254879236221,
          0.2226005643606186,
          0.17113330960273743,
          0.08400214463472366,
          0.07821135222911835,
          0.10852092504501343,
          0.18163993954658508,
          0.06713001430034637,
          0.19281655550003052,
          0.02381584793329239,
          0.10708031803369522,
          0.06764204055070877,
          0.07300226390361786,
          0.13123029470443726,
          0.09512386471033096,
          0.043000057339668274,
          0.16707782447338104,
          0.03775925561785698,
          0.022762654349207878,
          0.159568652510643,
          0.00790361501276493,
          0.05436427891254425,
          0.2009512484073639,
          0.02866009622812271,
          0.012085523456335068,
          0.027375634759664536,
          0.14818306267261505,
          0.2417328804731369,
          0.04755871370434761,
          0.08826134353876114,
          0.009402818977832794,
          0.0551588274538517,
          0.03960604965686798,
          0.1178559958934784,
          0.05535683408379555,
          0.00769450468942523,
          0.03266020119190216,
          0.17865191400051117,
          0.060407303273677826,
          0.037504129111766815,
          0.05734899640083313,
          0.017675234004855156,
          0.01945217326283455,
          0.0824701339006424,
          0.08183044195175171,
          0.09602337330579758,
          0.11008979380130768,
          0.009253992699086666,
          0.10938314348459244,
          0.027643078938126564,
          0.022327013313770294,
          0.06929749250411987,
          0.06663773953914642,
          0.07451441884040833,
          0.037648361176252365,
          0.10960234701633453,
          0.18333613872528076,
          0.1132798120379448,
          0.06927642971277237,
          0.07014281302690506,
          0.05894933640956879,
          0.03236265107989311,
          0.03966107964515686,
          0.03299375995993614,
          0.09090066701173782,
          0.016057465225458145,
          0.21175560355186462,
          0.04226519167423248,
          0.011494758538901806,
          0.09809659421443939,
          0.05520237982273102,
          0.008688651025295258,
          0.012761645019054413,
          0.00689086876809597,
          0.02100689522922039,
          0.02541923336684704,
          0.031770896166563034,
          0.06091434136033058,
          0.05757772922515869,
          0.15535105764865875,
          0.02590141072869301,
          0.08383020758628845,
          0.08063820004463196,
          0.011979544535279274,
          0.1084488183259964,
          0.09480953216552734,
          0.0532040037214756,
          0.03946731984615326,
          0.050018422305583954,
          0.025477027520537376,
          0.03519541770219803,
          0.16361762583255768,
          0.0832187831401825,
          0.008958583697676659,
          0.0435275137424469,
          0.02073429897427559,
          0.10364540666341782,
          0.03033740445971489,
          0.020826706662774086,
          0.11533639580011368,
          0.025929423049092293,
          0.06947226822376251,
          0.03683900460600853,
          0.05639968439936638,
          0.011727492325007915,
          0.06720773130655289,
          0.19851258397102356,
          0.12375106662511826,
          0.015660326927900314,
          0.16464132070541382,
          0.06764737516641617,
          0.15767690539360046,
          0.07877252250909805,
          0.17574340105056763,
          0.13409928977489471,
          0.011680133640766144,
          0.021855568513274193,
          0.007742850575596094,
          0.0979437455534935,
          0.025827525183558464,
          0.12475645542144775,
          0.012445805594325066,
          0.011535191908478737,
          0.029722411185503006,
          0.10032036900520325,
          0.18481524288654327,
          0.047279637306928635,
          0.008049078285694122,
          0.049876630306243896,
          0.016303181648254395,
          0.20016880333423615,
          0.04157808795571327,
          0.12749415636062622,
          0.1002865582704544,
          0.0668569803237915,
          0.0202542282640934,
          0.06176857650279999,
          0.01740947924554348,
          0.10049232840538025,
          0.03300164267420769,
          0.07470838725566864,
          0.11543247103691101,
          0.01354127749800682,
          0.0801621749997139,
          0.015794852748513222,
          0.24928517639636993,
          0.19558697938919067,
          0.05750538781285286,
          0.14815424382686615,
          0.05323963239789009,
          0.12966051697731018,
          0.026743344962596893,
          0.05924556031823158,
          0.016620095819234848,
          0.04543603956699371,
          0.014284937642514706,
          0.17107325792312622,
          0.03244685381650925,
          0.2728511691093445,
          0.017180610448122025,
          0.15008597075939178,
          0.04679955914616585,
          0.02752002514898777,
          0.07607056200504303,
          0.07116662710905075,
          0.12081148475408554,
          0.04495232552289963,
          0.03553233668208122,
          0.04976800084114075,
          0.06344668567180634,
          0.02417564205825329,
          0.2172485888004303,
          0.135087788105011,
          0.18346154689788818,
          0.01434139721095562,
          0.041314832866191864,
          0.036148570477962494,
          0.17074023187160492,
          0.20074598491191864,
          0.0465184710919857,
          0.04656139761209488,
          0.09074269235134125,
          0.13134953379631042,
          0.12046390771865845,
          0.03596886619925499,
          0.06638218462467194,
          0.017095113173127174,
          0.1577509194612503,
          0.01376262865960598,
          0.09144233167171478,
          0.04630964249372482,
          0.07044745981693268,
          0.10854149609804153,
          0.020754963159561157,
          0.24090562760829926,
          0.02962852641940117,
          0.09682801365852356,
          0.07157934457063675,
          0.08072753250598907,
          0.04557653144001961,
          0.04650453105568886,
          0.03481931984424591,
          0.10561466962099075,
          0.05777760222554207,
          0.054579101502895355,
          0.07589831948280334,
          0.0227473434060812,
          0.019710829481482506,
          0.050981555134058,
          0.01940426230430603,
          0.0557764507830143,
          0.02935667335987091,
          0.03503074124455452,
          0.11966779828071594,
          0.17447307705879211,
          0.09775643050670624,
          0.0529664047062397,
          0.009816097095608711,
          0.040922198444604874,
          0.11849801987409592,
          0.0825897827744484,
          0.04723094403743744,
          0.07778844982385635,
          0.07772127538919449,
          0.014280019327998161,
          0.06756508350372314,
          0.06210945174098015,
          0.01560366339981556,
          0.07466326653957367,
          0.07102707028388977,
          0.043520260602235794,
          0.04922665283083916,
          0.09022198617458344,
          0.052058011293411255,
          0.11370164901018143,
          0.08655629307031631,
          0.022975722327828407,
          0.08084956556558609,
          0.11864224821329117,
          0.09764689207077026,
          0.0544651634991169,
          0.14562469720840454,
          0.020412951707839966,
          0.04423479735851288,
          0.01815093494951725,
          0.016827313229441643,
          0.0655818060040474,
          0.11338304728269577,
          0.1620291918516159,
          0.12194357812404633,
          0.13269153237342834,
          0.05843109264969826,
          0.022942056879401207,
          0.1375088393688202,
          0.012032845988869667,
          0.0318741649389267,
          0.1359531730413437,
          0.012985083274543285,
          0.08031459152698517,
          0.18915726244449615,
          0.05170956999063492,
          0.024695971980690956,
          0.1923864632844925,
          0.01270428579300642,
          0.028362996876239777,
          0.13385215401649475,
          0.05279628932476044,
          0.03365145996212959,
          0.008933031931519508,
          0.035476524382829666,
          0.07625341415405273,
          0.0376192070543766,
          0.14258038997650146,
          0.15739400684833527,
          0.02695326879620552,
          0.04533413425087929,
          0.06601759046316147,
          0.0633625015616417,
          0.05478585883975029,
          0.05899692326784134,
          0.01939455419778824,
          0.24339412152767181,
          0.011680309660732746,
          0.1765671670436859,
          0.039567530155181885,
          0.05782528966665268,
          0.07880096137523651,
          0.11671436578035355,
          0.0887044221162796,
          0.02685440331697464,
          0.21110591292381287,
          0.058429744094610214,
          0.1271311491727829,
          0.02135646529495716,
          0.0675918385386467,
          0.12716302275657654,
          0.02639557234942913,
          0.03987441584467888,
          0.06453710794448853,
          0.08835391700267792,
          0.03484601154923439,
          0.06171925365924835,
          0.01462487131357193,
          0.04693052917718887,
          0.18482035398483276,
          0.04036220908164978,
          0.00915798731148243,
          0.11242897063493729,
          0.012395787984132767,
          0.026894276961684227,
          0.01647486723959446,
          0.057143304497003555,
          0.045527130365371704,
          0.009583665058016777,
          0.05948025360703468,
          0.0405912883579731,
          0.020036863163113594,
          0.02839156612753868,
          0.08524361252784729,
          0.04691242054104805,
          0.2226642668247223,
          0.13334597647190094,
          0.030241120606660843,
          0.17245787382125854,
          0.009860207326710224,
          0.03695956617593765,
          0.06673994660377502,
          0.02142646722495556,
          0.07052136212587357,
          0.009784466587007046,
          0.02902841754257679,
          0.02299356274306774,
          0.16961245238780975,
          0.0580909326672554,
          0.02966969460248947,
          0.08991440385580063,
          0.03214108943939209,
          0.08114350587129593,
          0.0543171763420105,
          0.024650346487760544,
          0.016711141914129257,
          0.11070804297924042,
          0.03824852779507637,
          0.0636652484536171,
          0.004190366715192795,
          0.15144331753253937,
          0.018258819356560707,
          0.02365322969853878,
          0.0157737135887146,
          0.06810855120420456,
          0.020657449960708618,
          0.1278119832277298,
          0.06161996349692345,
          0.03232252597808838,
          0.0368351973593235,
          0.008308803662657738,
          0.026223523542284966,
          0.017343513667583466,
          0.06836672872304916,
          0.018468378111720085,
          0.05279332771897316,
          0.05383490398526192,
          0.03014557436108589,
          0.01097307913005352,
          0.08068902790546417,
          0.17751272022724152,
          0.008550389669835567,
          0.010060643777251244,
          0.026706283912062645,
          0.01462855376303196,
          0.04245421662926674,
          0.013735370710492134,
          0.18443132936954498,
          0.13156971335411072,
          0.012384435161948204,
          0.12122111022472382,
          0.08662338554859161,
          0.013191213831305504,
          0.260727196931839,
          0.19027838110923767,
          0.029868297278881073,
          0.05960763618350029,
          0.03716201335191727,
          0.1550433486700058,
          0.11745566874742508,
          0.012857770547270775,
          0.09518302232027054,
          0.015613926574587822,
          0.04053834080696106,
          0.00696048466488719,
          0.009288182482123375,
          0.09524525701999664,
          0.06936225295066833,
          0.061048008501529694,
          0.008126853965222836,
          0.06438719481229782,
          0.10590451210737228,
          0.0561826191842556,
          0.01921793445944786,
          0.13514229655265808,
          0.06763844192028046,
          0.06275954097509384,
          0.042742565274238586,
          0.030835706740617752,
          0.015967264771461487,
          0.0399172380566597,
          0.04697860777378082,
          0.049240194261074066,
          0.215492844581604,
          0.01452433131635189,
          0.031354449689388275,
          0.01924813725054264,
          0.03712226450443268,
          0.048396144062280655,
          0.05959115922451019,
          0.18877562880516052,
          0.1390761286020279,
          0.014468387700617313,
          0.11081364750862122,
          0.04692620411515236,
          0.045398931950330734,
          0.05354354530572891,
          0.09655370563268661,
          0.06209934130311012,
          0.018136924132704735,
          0.07155442237854004,
          0.03002355992794037,
          0.02033824287354946,
          0.14076104760169983,
          0.03960954397916794,
          0.06926875561475754,
          0.006143581122159958,
          0.008253401145339012,
          0.05556657537817955,
          0.006934124045073986,
          0.013524515554308891,
          0.07523559033870697,
          0.08153628557920456,
          0.06240678206086159,
          0.11062991619110107,
          0.026935217902064323,
          0.03411963954567909,
          0.12377077341079712,
          0.072057344019413,
          0.10419969260692596,
          0.056552741676568985,
          0.024178914725780487,
          0.15001967549324036,
          0.08149032294750214,
          0.10332557559013367,
          0.09013059735298157,
          0.05744776129722595,
          0.05336739122867584,
          0.06602834910154343,
          0.010987887158989906,
          0.02496912330389023,
          0.10900046676397324,
          0.027501408010721207,
          0.005975260399281979,
          0.05607832595705986,
          0.01840880885720253,
          0.037542007863521576,
          0.013571193441748619,
          0.03573305904865265,
          0.016529418528079987,
          0.042945414781570435,
          0.1834374964237213,
          0.08235371857881546,
          0.09752996265888214,
          0.11269225180149078,
          0.037053242325782776,
          0.032510120421648026,
          0.03766245022416115,
          0.036923233419656754,
          0.03226635605096817,
          0.07683006674051285,
          0.04704893007874489,
          0.10850674659013748,
          0.04112601280212402,
          0.04535406455397606,
          0.006781138479709625,
          0.1291298121213913,
          0.03628505393862724,
          0.041619617491960526,
          0.060054291039705276,
          0.08379005640745163,
          0.027744514867663383,
          0.02262788638472557,
          0.0315641425549984,
          0.02864151820540428,
          0.02272946573793888,
          0.037160541862249374,
          0.04674962908029556,
          0.09197048097848892,
          0.0028038909658789635,
          0.1299569457769394,
          0.003891443135216832,
          0.1766621619462967
         ],
         "yaxis": "y"
        },
        {
         "name": "Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937
         ],
         "xaxis": "x",
         "y": [
          0.125,
          0.359375,
          0.3984375,
          0.265625,
          0.2734375,
          0.3984375,
          0.6015625,
          0.6796875,
          0.71875,
          0.7109375,
          0.71875,
          0.7109375,
          0.78125,
          0.796875,
          0.8046875,
          0.84375,
          0.7578125,
          0.7890625,
          0.75,
          0.8203125,
          0.8125,
          0.8046875,
          0.8359375,
          0.890625,
          0.8828125,
          0.921875,
          0.90625,
          0.84375,
          0.8671875,
          0.8984375,
          0.890625,
          0.875,
          0.890625,
          0.9140625,
          0.9375,
          0.8984375,
          0.890625,
          0.9375,
          0.9296875,
          0.8984375,
          0.890625,
          0.90625,
          0.8828125,
          0.953125,
          0.9609375,
          0.953125,
          0.9375,
          0.9296875,
          0.953125,
          0.875,
          0.8984375,
          0.953125,
          0.921875,
          0.921875,
          0.9609375,
          0.9453125,
          0.921875,
          0.921875,
          0.9296875,
          0.9375,
          0.9375,
          0.921875,
          0.96875,
          0.921875,
          0.9609375,
          0.953125,
          0.953125,
          0.96875,
          0.9375,
          0.9609375,
          0.96875,
          0.9453125,
          0.9453125,
          0.9453125,
          0.9609375,
          0.9453125,
          0.9296875,
          0.9609375,
          0.921875,
          0.9375,
          0.9453125,
          0.9453125,
          0.9375,
          0.9609375,
          0.9453125,
          0.9609375,
          0.953125,
          0.9375,
          0.9765625,
          0.9609375,
          0.984375,
          0.90625,
          0.9453125,
          0.953125,
          0.9296875,
          0.9609375,
          0.9375,
          0.9140625,
          0.9453125,
          0.9375,
          0.9375,
          0.953125,
          0.96875,
          0.9375,
          0.9453125,
          0.9609375,
          0.96875,
          0.921875,
          0.9375,
          0.9375,
          0.96875,
          0.9765625,
          0.953125,
          0.9609375,
          0.9765625,
          0.9609375,
          0.9765625,
          0.984375,
          0.9765625,
          0.96875,
          0.96875,
          0.8984375,
          0.9453125,
          0.9296875,
          1,
          0.9609375,
          0.8984375,
          0.9453125,
          0.9296875,
          0.9609375,
          0.96875,
          0.9375,
          0.953125,
          0.9609375,
          0.96875,
          0.9609375,
          0.9609375,
          0.9921875,
          0.9765625,
          0.9609375,
          0.984375,
          0.9609375,
          0.9921875,
          0.96875,
          0.984375,
          0.9765625,
          0.96875,
          0.9609375,
          0.9609375,
          0.96875,
          0.953125,
          0.96875,
          0.9765625,
          0.953125,
          0.9453125,
          0.9765625,
          0.96875,
          0.953125,
          0.9765625,
          0.9609375,
          0.96875,
          0.9453125,
          0.984375,
          0.953125,
          0.9609375,
          0.953125,
          0.96875,
          0.9609375,
          0.9765625,
          0.9765625,
          0.953125,
          0.953125,
          0.9453125,
          0.984375,
          1,
          0.953125,
          0.9609375,
          0.96875,
          0.953125,
          0.96875,
          0.9765625,
          0.9609375,
          0.984375,
          0.953125,
          0.984375,
          0.9453125,
          0.9375,
          0.9765625,
          0.953125,
          0.9921875,
          0.9921875,
          0.9765625,
          0.953125,
          0.90625,
          0.9765625,
          0.9765625,
          0.9765625,
          0.9375,
          0.9765625,
          0.984375,
          0.9609375,
          0.9765625,
          0.96875,
          0.9296875,
          0.984375,
          0.9296875,
          0.9609375,
          0.9375,
          0.96875,
          0.9140625,
          0.9609375,
          0.96875,
          0.96875,
          0.96875,
          0.9921875,
          0.9453125,
          0.9609375,
          0.984375,
          0.96875,
          0.9765625,
          0.96875,
          0.9921875,
          0.984375,
          0.9921875,
          0.9765625,
          0.9765625,
          0.96875,
          0.96875,
          0.9765625,
          0.9765625,
          0.96875,
          0.984375,
          0.96875,
          0.984375,
          0.96875,
          0.984375,
          0.984375,
          0.9765625,
          0.9765625,
          0.984375,
          0.96875,
          0.9765625,
          0.984375,
          0.9453125,
          0.984375,
          0.9765625,
          0.96875,
          0.96875,
          0.96875,
          0.9765625,
          0.953125,
          0.96875,
          0.9765625,
          0.984375,
          0.9609375,
          0.984375,
          0.9765625,
          0.984375,
          0.9765625,
          0.984375,
          0.9765625,
          0.96875,
          0.96875,
          0.9609375,
          0.9375,
          0.9453125,
          0.9609375,
          0.9921875,
          0.984375,
          0.953125,
          0.984375,
          0.953125,
          0.953125,
          0.984375,
          0.9609375,
          0.9609375,
          0.9765625,
          0.984375,
          0.96875,
          0.96875,
          0.9765625,
          1,
          0.9921875,
          0.984375,
          0.984375,
          0.96875,
          0.9375,
          0.9921875,
          0.96875,
          0.96875,
          0.984375,
          0.984375,
          0.9765625,
          0.96875,
          0.984375,
          0.984375,
          0.9765625,
          0.9765625,
          1,
          0.9765625,
          0.9765625,
          0.9609375,
          0.9609375,
          0.9921875,
          0.96875,
          0.984375,
          0.953125,
          0.9765625,
          0.9609375,
          0.984375,
          0.9609375,
          0.96875,
          0.984375,
          0.9453125,
          0.9765625,
          0.9765625,
          0.953125,
          0.96875,
          0.953125,
          0.9921875,
          0.9375,
          0.9453125,
          0.9609375,
          0.9609375,
          0.9765625,
          0.984375,
          0.9609375,
          0.96875,
          0.96875,
          0.953125,
          0.953125,
          0.9765625,
          0.9765625,
          0.984375,
          0.9921875,
          0.96875,
          0.9765625,
          0.96875,
          0.96875,
          0.9765625,
          0.984375,
          0.9921875,
          0.96875,
          0.9765625,
          0.9765625,
          0.984375,
          0.953125,
          0.984375,
          0.9609375,
          0.9765625,
          0.953125,
          0.9765625,
          0.984375,
          0.9921875,
          0.984375,
          0.96875,
          0.9765625,
          0.96875,
          1,
          0.984375,
          0.9609375,
          0.984375,
          0.9921875,
          0.9921875,
          0.96875,
          0.9921875,
          1,
          0.9765625,
          0.984375,
          0.9609375,
          0.96875,
          0.953125,
          0.96875,
          0.984375,
          0.9609375,
          0.9765625,
          0.9765625,
          0.96875,
          0.984375,
          0.9609375,
          0.96875,
          0.96875,
          0.984375,
          0.9609375,
          0.984375,
          0.9453125,
          0.984375,
          0.984375,
          0.96875,
          0.96875,
          0.9921875,
          0.984375,
          0.9921875,
          0.984375,
          0.96875,
          0.984375,
          0.984375,
          0.96875,
          0.9765625,
          0.9765625,
          0.9765625,
          0.953125,
          0.984375,
          0.953125,
          0.96875,
          0.9765625,
          0.9921875,
          0.9921875,
          1,
          0.9609375,
          0.953125,
          0.9921875,
          0.96875,
          0.984375,
          0.984375,
          0.9921875,
          0.984375,
          0.96875,
          1,
          0.984375,
          0.984375,
          0.9765625,
          0.9921875,
          0.9765625,
          0.9609375,
          1,
          0.96875,
          0.96875,
          0.984375,
          0.984375,
          0.96875,
          0.9765625,
          0.9765625,
          0.9609375,
          0.9765625,
          0.984375,
          0.9609375,
          0.984375,
          0.9921875,
          1,
          0.9765625,
          0.9921875,
          0.9765625,
          0.9609375,
          0.953125,
          0.96875,
          0.9921875,
          0.96875,
          0.96875,
          0.9921875,
          0.9765625,
          0.96875,
          0.9765625,
          0.984375,
          0.953125,
          0.9453125,
          0.984375,
          1,
          0.9921875,
          0.9921875,
          0.984375,
          0.953125,
          0.96875,
          0.96875,
          0.96875,
          0.9765625,
          0.96875,
          0.9921875,
          0.9921875,
          0.9921875,
          0.96875,
          0.96875,
          0.984375,
          1,
          0.984375,
          0.9921875,
          0.984375,
          0.9765625,
          0.9765625,
          0.9296875,
          0.984375,
          0.96875,
          0.9921875,
          0.953125,
          0.96875,
          0.984375,
          0.984375,
          0.953125,
          0.96875,
          0.9921875,
          0.984375,
          0.984375,
          0.984375,
          0.96875,
          0.984375,
          0.984375,
          1,
          0.984375,
          0.984375,
          0.96875,
          0.9921875,
          0.9609375,
          0.9921875,
          0.9765625,
          0.984375,
          0.984375,
          0.9921875,
          0.9921875,
          0.9765625,
          0.9921875,
          0.9921875,
          0.9765625,
          0.9921875,
          0.984375,
          0.9765625,
          0.984375,
          0.96875,
          0.9765625,
          0.984375,
          0.9765625,
          0.96875,
          1,
          1,
          0.96875,
          1,
          0.9765625,
          0.96875,
          0.9921875,
          0.9453125,
          0.9921875,
          0.9765625,
          0.96875,
          0.984375,
          0.984375,
          0.984375,
          0.96875,
          0.96875,
          0.96875,
          0.9921875,
          0.9609375,
          0.984375,
          0.9609375,
          0.96875,
          1,
          0.9609375,
          0.984375,
          0.9765625,
          0.96875,
          0.96875,
          0.9609375,
          0.984375,
          0.96875,
          0.9921875,
          0.9921875,
          0.984375,
          0.984375,
          0.984375,
          0.9765625,
          0.9765625,
          0.9765625,
          0.9921875,
          0.9921875,
          0.984375,
          0.96875,
          0.984375,
          0.9765625,
          0.9921875,
          0.9921875,
          0.9921875,
          0.984375,
          0.9921875,
          0.96875,
          0.9921875,
          0.9609375,
          0.9765625,
          0.9765625,
          0.9921875,
          0.984375,
          1,
          0.9765625,
          0.9765625,
          0.9921875,
          0.9921875,
          0.9765625,
          0.984375,
          1,
          0.984375,
          0.984375,
          0.96875,
          0.9921875,
          0.9765625,
          0.9765625,
          0.9609375,
          0.984375,
          0.984375,
          1,
          0.9609375,
          0.984375,
          0.96875,
          0.984375,
          0.984375,
          0.984375,
          0.984375,
          0.9453125,
          0.9921875,
          0.9453125,
          0.9921875,
          0.9609375,
          0.984375,
          0.9765625,
          0.96875,
          0.9765625,
          0.9921875,
          0.9765625,
          0.9765625,
          0.96875,
          0.984375,
          0.9765625,
          0.9765625,
          0.984375,
          0.984375,
          0.96875,
          0.9921875,
          0.9765625,
          0.9921875,
          0.9453125,
          0.9765625,
          0.984375,
          0.9921875,
          0.984375,
          0.984375,
          0.984375,
          0.9921875,
          0.984375,
          0.9609375,
          0.984375,
          0.9921875,
          0.9921875,
          0.9921875,
          0.96875,
          0.953125,
          0.96875,
          0.9921875,
          1,
          0.9609375,
          0.984375,
          0.984375,
          0.9609375,
          0.9609375,
          0.9921875,
          0.9609375,
          0.9921875,
          0.9921875,
          1,
          0.96875,
          0.984375,
          0.984375,
          0.9765625,
          0.9921875,
          0.9765625,
          0.96875,
          0.984375,
          0.96875,
          0.9609375,
          0.9765625,
          0.953125,
          0.9921875,
          0.96875,
          0.9921875,
          0.984375,
          0.984375,
          0.9921875,
          0.984375,
          0.984375,
          0.984375,
          0.9921875,
          0.9921875,
          1,
          0.9609375,
          0.9921875,
          0.96875,
          0.9765625,
          0.96875,
          0.984375,
          0.984375,
          0.96875,
          0.9921875,
          0.9921875,
          0.9609375,
          0.9921875,
          0.9921875,
          0.9609375,
          1,
          0.984375,
          0.9921875,
          0.984375,
          0.984375,
          0.96875,
          0.9921875,
          0.9765625,
          0.9921875,
          0.9609375,
          0.9765625,
          1,
          0.984375,
          0.9765625,
          0.984375,
          0.9921875,
          1,
          0.96875,
          0.9765625,
          0.984375,
          1,
          0.9921875,
          0.9765625,
          0.9765625,
          0.9765625,
          0.9765625,
          0.984375,
          0.9609375,
          0.9765625,
          0.9609375,
          0.9921875,
          1,
          0.984375,
          0.9921875,
          0.9765625,
          0.96875,
          1,
          0.9921875,
          1,
          0.96875,
          0.984375,
          0.984375,
          1,
          0.953125,
          0.96875,
          0.953125,
          0.9765625,
          0.9765625,
          0.9921875,
          0.9765625,
          0.9921875,
          0.984375,
          1,
          0.984375,
          0.9921875,
          0.96875,
          0.9921875,
          0.9609375,
          0.9765625,
          0.9453125,
          0.9921875,
          0.9765625,
          1,
          0.984375,
          1,
          0.9921875,
          0.984375,
          0.9921875,
          0.9765625,
          0.984375,
          0.9609375,
          0.9765625,
          0.9765625,
          0.9921875,
          0.9765625,
          0.9765625,
          0.9765625,
          1,
          0.9765625,
          1,
          0.9921875,
          0.9921875,
          0.984375,
          0.9921875,
          0.984375,
          0.96875,
          0.9765625,
          0.9921875,
          0.9921875,
          0.9609375,
          0.9609375,
          0.9765625,
          0.9765625,
          0.9765625,
          0.9921875,
          0.984375,
          0.9765625,
          0.984375,
          0.96875,
          1,
          0.984375,
          0.984375,
          0.9765625,
          0.984375,
          0.984375,
          1,
          0.984375,
          1,
          0.984375,
          0.984375,
          0.984375,
          1,
          0.984375,
          0.9921875,
          0.9921875,
          0.9609375,
          1,
          0.96875,
          0.9921875,
          0.9921875,
          0.9921875,
          0.984375,
          0.9765625,
          0.9921875,
          1,
          0.9765625,
          0.9921875,
          0.96875,
          0.96875,
          0.96875,
          0.9609375,
          0.984375,
          0.984375,
          1,
          0.9765625,
          0.9921875,
          0.984375,
          0.984375,
          0.9609375,
          0.984375,
          0.9921875,
          0.9765625,
          1,
          0.9765625,
          0.9921875,
          0.984375,
          0.9765625,
          0.9921875,
          0.9921875,
          0.9921875,
          0.9765625,
          0.984375,
          0.9921875,
          0.9765625,
          0.984375,
          0.9921875,
          1,
          0.9921875,
          1,
          0.9609375,
          0.9921875,
          0.9609375,
          0.9921875,
          0.9765625,
          1,
          0.984375,
          0.9765625,
          0.9921875,
          0.96875,
          0.9765625,
          0.984375,
          0.9765625,
          0.9921875,
          0.984375,
          0.96875,
          0.9765625,
          0.984375,
          0.96875,
          0.9765625,
          0.984375,
          0.9921875,
          0.96875,
          0.96875,
          0.9765625,
          0.9765625,
          0.9921875,
          0.953125,
          0.96875,
          0.9921875,
          0.9609375,
          0.984375,
          1,
          0.9765625,
          0.984375,
          0.984375,
          0.9765625,
          0.96875,
          0.984375,
          0.9921875,
          0.9765625,
          1,
          0.984375,
          0.984375,
          0.984375,
          1,
          0.96875,
          1,
          0.9765625,
          0.984375,
          0.9921875,
          1,
          0.984375,
          1,
          0.984375,
          0.984375,
          1,
          0.984375,
          0.9921875,
          0.9921875,
          0.984375,
          1,
          0.96875,
          0.984375,
          0.9921875,
          0.9765625,
          0.984375,
          0.984375,
          0.9765625,
          0.9765625,
          0.96875,
          1,
          1,
          0.9765625,
          0.9921875,
          1,
          1,
          0.984375,
          0.9921875,
          0.9765625,
          0.9921875,
          0.984375,
          0.9921875,
          0.9921875,
          0.9921875,
          0.9921875,
          0.9921875
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "hovermode": "x unified",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "text": "CNN training loss & test accuracy"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.94
         ],
         "range": [
          0,
          938
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x",
         "overlaying": "y",
         "range": [
          0,
          1
         ],
         "side": "right",
         "tickformat": ".0%"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_convnet(trainloader: DataLoader, testloader: DataLoader, epochs: int, loss_fn: Callable) -> list:\n",
    "    '''\n",
    "    Defines a ConvNet using our previous code, and trains it on the data in trainloader.\n",
    "\n",
    "    Returns tuple of (loss_list, accuracy_list), where accuracy_list contains the fraction of accurate classifications on the test set, at the end of each epoch.\n",
    "    '''\n",
    "    model = ConvNet().to(device).train()\n",
    "    optimizer = t.optim.Adam(model.parameters())\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "\n",
    "        \n",
    "        for (x, y) in tqdm_notebook(trainloader, leave=False):\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # get accuracy\n",
    "            totalCorrect = 0.0\n",
    "            count = 128\n",
    "            X_batch, y_batch = next(iter(testloader))\n",
    "            y_hat = t.argmax(model(X_batch),1)\n",
    "            totalCorrect = (y_hat == y_batch).float().sum()\n",
    "            accuracy = totalCorrect / count\n",
    "\n",
    "            print(\"totalCorrect =  \" + str(totalCorrect) + \", count = \" + str(count) )\n",
    "            print(f\"Epoch {epoch}/{epochs}, accuracy is {accuracy:.6f}\")\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs}, train loss is {loss:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Saving model to: {MODEL_FILENAME}\")\n",
    "    t.save(model, MODEL_FILENAME)\n",
    "    return (loss_list, accuracy_list)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "\n",
    "MODEL_FILENAME = \"./w1d2_convnet_mnist.pt\"\n",
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "loss_list, accuracy_list = train_convnet(trainloader, testloader, epochs, loss_fn)\n",
    "\n",
    "\n",
    "\n",
    "fig = px.line(y=loss_list, template=\"simple_white\")\n",
    "fig.update_layout(title=\"Cross entropy loss on MNIST\", yaxis_range=[0, max(loss_list)])\n",
    "fig.show()\n",
    "\n",
    "\n",
    "utils.plot_loss_and_accuracy(loss_list, accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ARENAenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
