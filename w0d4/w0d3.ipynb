{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import w0d3Remnant as fromYesterday\n",
    "import torch as t\n",
    "from torch import nn as nn\n",
    "import utils\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a combination of my day four *and* five work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day Four - Spent two hours, eight minutes on this so far. Going to revisit it later, cannot focus on it\n",
    "Day Five - Spent three hours and thirty three minutes. Finished it. LETS GOOOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization in CNNs\n",
    "\n",
    "Batch Norm is a method to address overfitting and slow training issues in Deep Neural Networks.\n",
    "\n",
    "Normalization itself is just a pre-processing technique used to standardize data. This is often done to make features balanced - if they have differences in ranges, different features could recieve inflated importances. Normalizing your features helps models learn better.\n",
    "\n",
    "You can instead normalize batches of data inside the network itself, done between layers. Each neuron will first apply its weights (not biases!): $$z = g(w,x);$$\n",
    "\n",
    "But before applying the activation function, a batch norm is applied to each neurons output across the layer.\n",
    "\n",
    "$$z^N = (\\frac{z - m_z}{s_z}) \\cdot \\gamma + \\beta$$\n",
    "\n",
    "where $z^N$ is the output of the Batch Norm, $m_z$ is the mean of the neuron's output, $s_z$ is the standard deviation of the output of the neurons, and $\\gamma$ and $\\beta$ are learnable parameters which eventually represent the standard deviation and the mean of the outputs, respectively.\n",
    "\n",
    "The output of batch norm is then fed into the activation function\n",
    "\n",
    "$$a = f(z^N)$$\n",
    "\n",
    "Why does this work? Firstly, think of it intuitively as the same thing you are doing to the features. But secondly, it reduces the \"internal covariate shift of the network\" - it reduces the shift due to a change in data distribution. Here, it the data recieved from the previous layer, which is constantly changing.\n",
    "\n",
    "Batch norm also has a regularization effect, speeds up computation!\n",
    "\n",
    "When applying this to convolutional layers, each feature map ends up having a single mean and standard deviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(weights shape = torch.Size([10, 3, 3, 3]) stride = (1, 1) padding = (0, 0) )\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Sequential(nn.Module):\n",
    "    def __init__(self, *modules: nn.Module):\n",
    "        super().__init__()\n",
    "        for i, mod in enumerate(modules):\n",
    "            self.add_module(str(i), mod)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Chain each module together, with the output from one feeding into the next one.'''\n",
    "        for mod in self._modules.values():\n",
    "            x = mod(x)\n",
    "        return x\n",
    "\n",
    "model = Sequential(fromYesterday.Conv2d(3,10,3,1,0),\n",
    "fromYesterday.ReLU(),\n",
    "nn.Linear(10,5))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_batchnorm2d_module` passed!\n",
      "All tests in `test_batchnorm2d_forward` passed!\n",
      "All tests in `test_batchnorm2d_running_mean` passed!\n"
     ]
    }
   ],
   "source": [
    "from math import gamma\n",
    "from operator import truediv\n",
    "import einops\n",
    "from tkinter.tix import MAIN\n",
    "\n",
    "\n",
    "class BatchNorm2d(nn.Module):\n",
    "    running_mean: t.Tensor         # shape: (num_features,)\n",
    "    running_var: t.Tensor          # shape: (num_features,)\n",
    "    num_batches_tracked: t.Tensor  # shape: ()\n",
    "\n",
    "    def __init__(self, num_features: int, eps=1e-05, momentum=0.1):\n",
    "        '''Like nn.BatchNorm2d with track_running_stats=True and affine=True.\n",
    "\n",
    "        Name the learnable affine parameters `weight` and `bias` in that order.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #print(\"num features is \" + str(num_features))\n",
    "        running_mean = t.zeros(num_features)\n",
    "        running_var = t.ones(num_features)\n",
    "        num_batches_tracked = t.tensor(0)\n",
    "        \n",
    "        self.weight = nn.Parameter(t.ones(num_features)) # tracks gamma\n",
    "        self.bias = nn.Parameter(t.zeros(num_features)) # tracks beta\n",
    "        self.register_buffer('running_mean', running_mean) # holds mean across channels\n",
    "        self.register_buffer('running_var', running_var) # holds variance across channels\n",
    "        self.register_buffer('num_batches_tracked', num_batches_tracked) # stores num batches tracked\n",
    "        self.num_features = num_features\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Normalize each channel.\n",
    "\n",
    "        Compute the variance using `torch.var(x, unbiased=False)`\n",
    "        Hint: you may also find it helpful to use the argument `keepdim`.\n",
    "\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels, height, width)\n",
    "        '''\n",
    "        if self.training:\n",
    "            # update running mean\n",
    "            mean = t.mean(x, (0,2,3), keepdim = False)\n",
    "            self.running_mean = (self.momentum) * mean + (1 - self.momentum) * self.running_mean\n",
    "            # update running variance\n",
    "            var = t.var(x, (0,2,3), keepdim = False, unbiased=False)\n",
    "            self.running_var = (self.momentum) * var + (1 - self.momentum) * self.running_var\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        mean = einops.rearrange(mean, 'c -> 1 c 1 1')\n",
    "        var = einops.rearrange(var, 'c -> 1 c 1 1')\n",
    "        gamma = einops.rearrange(self.weight, 'c -> 1 c 1 1')\n",
    "        beta = einops.rearrange(self.bias, 'c -> 1 c 1 1')\n",
    "\n",
    "        returnX = ((x - mean) / (t.sqrt(var + self.eps))) * gamma + beta\n",
    "        return returnX\n",
    "\n",
    "       \n",
    "    # credit to callum for this\n",
    "    def extra_repr(self) -> str:\n",
    "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in [\"num_features\", \"eps\", \"momentum\"]])\n",
    "\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_batchnorm2d_module(BatchNorm2d)\n",
    "    utils.test_batchnorm2d_forward(BatchNorm2d)\n",
    "    utils.test_batchnorm2d_running_mean(BatchNorm2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watching a paper review of ResNet\n",
    "\n",
    "A key thing to realize is that when they were contemplating why accuracy wasn't increasing with model depth, the issue was not overfitting!\n",
    "\n",
    "Each chunk of the model only has to learn the difference in how they wish for the data to change, rather than also having to learn an identiy function, too. Regluarization pushes against identity since it pushes everything down to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels)\n",
    "        '''\n",
    "\n",
    "        return t.mean(x,(2,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_stride=1):\n",
    "        '''A single residual block with optional downsampling.\n",
    "\n",
    "        For compatibility with the pretrained model, declare the left side branch first using a `Sequential`.\n",
    "\n",
    "        If first_stride is > 1, this means the optional (conv + bn) should be present on the right branch. Declare it second using another `Sequential`.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        left_branch = nn.Sequential(\n",
    "            fromYesterday.Conv2d(in_feats, out_feats, 3, first_stride, 1), # TODO: why is padding 1? not zero?\n",
    "            BatchNorm2d(out_feats),\n",
    "            fromYesterday.ReLU(),\n",
    "            fromYesterday.Conv2d(out_feats, out_feats, 3, 1, 1),\n",
    "            BatchNorm2d(out_feats)\n",
    "        )\n",
    "\n",
    "        if first_stride > 1:\n",
    "            right_branch = nn.Sequential(\n",
    "                fromYesterday.Conv2d(in_feats, out_feats, 1, first_stride, 0),\n",
    "                BatchNorm2d(out_feats)\n",
    "            )\n",
    "        else:\n",
    "            right_branch = nn.Sequential(\n",
    "                nn.Identity()\n",
    "            )\n",
    "\n",
    "        self.left = left_branch\n",
    "        self.right = right_branch\n",
    "        self.relu = fromYesterday.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        '''\n",
    "        leftOutput = self.left.forward(x)\n",
    "        rightOutput = self.right.forward(x)\n",
    "        return self.relu.forward(leftOutput + rightOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockGroup(nn.Module):\n",
    "    def __init__(self, n_blocks: int, in_feats: int, out_feats: int, first_stride=1):\n",
    "        '''An n_blocks-long sequence of ResidualBlock where only the first block uses the provided stride.'''\n",
    "        super().__init__()\n",
    "        self.firstRes = ResidualBlock(in_feats,out_feats,first_stride)\n",
    "        self.otherRes = nn.ModuleList([ResidualBlock(out_feats, out_feats,1) for i in range(n_blocks - 1)])\n",
    "        # TODO: convert back to normal list if this doesn't work!\n",
    "        \n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Compute the forward pass.\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        '''\n",
    "        result = self.firstRes.forward(x)\n",
    "        for l in self.otherRes:\n",
    "            result = l(result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pwd\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks_per_group=[3, 4, 6, 3],\n",
    "        out_features_per_group=[64, 128, 256, 512],\n",
    "        first_strides_per_group=[1, 2, 2, 2],\n",
    "        n_classes=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        BlockGroups = []\n",
    "        for i, j in enumerate(n_blocks_per_group):\n",
    "            n_blocks = j\n",
    "            if i > 1:\n",
    "                in_feats = out_features_per_group[i-1]\n",
    "            else:\n",
    "                in_feats = 64\n",
    "            out_feats = out_features_per_group[i]\n",
    "            first_stride = first_strides_per_group[i]\n",
    "            BlockGroups.append(BlockGroup(n_blocks, in_feats, out_feats, first_stride))\n",
    "\n",
    "\n",
    "        self.preBlockGroups = nn.Sequential(\n",
    "            fromYesterday.Conv2d(3,64,7,2,3),\n",
    "            BatchNorm2d(64),\n",
    "            fromYesterday.ReLU(),\n",
    "            fromYesterday.MaxPool2d(3,2),\n",
    "        )\n",
    "        self.layers = nn.Sequential(\n",
    "            *BlockGroups\n",
    "        )\n",
    "\n",
    "        self.postBlockGroups = nn.Sequential(\n",
    "            AveragePool(),\n",
    "            fromYesterday.Flatten(),\n",
    "            fromYesterday.Linear(out_features_per_group[-1], 1000)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, channels, height, width)\n",
    "\n",
    "        Return: shape (batch, n_classes)\n",
    "        '''\n",
    "        #print(\"HERE\")\n",
    "        y = self.preBlockGroups(x)\n",
    "        #print(\"went through pre block groups\")\n",
    "        y = self.layers(y)\n",
    "        #print(\"went through post block groups\")\n",
    "        y = self.postBlockGroups(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models import resnet34\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>their name</th>\n",
       "      <th>their shape</th>\n",
       "      <th>your name</th>\n",
       "      <th>your shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conv1.weight</td>\n",
       "      <td>(64, 3, 7, 7)</td>\n",
       "      <td>preBlockGroups.0.weight</td>\n",
       "      <td>(64, 3, 7, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bn1.weight</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>preBlockGroups.1.weight</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bn1.bias</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>preBlockGroups.1.bias</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bn1.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>preBlockGroups.1.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bn1.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>preBlockGroups.1.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>preBlockGroups.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>layer1.0.conv1.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "      <td>layers.0.firstRes.left.0.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>layer1.0.bn1.weight</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.firstRes.left.1.weight</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>layer1.0.bn1.bias</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.firstRes.left.1.bias</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>layer1.0.bn1.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.firstRes.left.1.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>layer1.0.bn1.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.firstRes.left.1.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>layer1.0.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.0.firstRes.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>layer1.0.conv2.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "      <td>layers.0.firstRes.left.3.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>layer1.0.bn2.weight</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.firstRes.left.4.weight</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>layer1.0.bn2.bias</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.firstRes.left.4.bias</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>layer1.0.bn2.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.firstRes.left.4.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>layer1.0.bn2.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.firstRes.left.4.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>layer1.0.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.0.firstRes.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>layer1.1.conv1.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "      <td>layers.0.otherRes.0.left.0.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>layer1.1.bn1.weight</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.0.left.1.weight</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>layer1.1.bn1.bias</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.0.left.1.bias</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>layer1.1.bn1.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.0.left.1.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>layer1.1.bn1.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.0.left.1.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>layer1.1.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.0.otherRes.0.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>layer1.1.conv2.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "      <td>layers.0.otherRes.0.left.3.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>layer1.1.bn2.weight</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.0.left.4.weight</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>layer1.1.bn2.bias</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.0.left.4.bias</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>layer1.1.bn2.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.0.left.4.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>layer1.1.bn2.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.0.left.4.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>layer1.1.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.0.otherRes.0.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>layer1.2.conv1.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "      <td>layers.0.otherRes.1.left.0.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>layer1.2.bn1.weight</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.1.left.1.weight</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>layer1.2.bn1.bias</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.1.left.1.bias</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>layer1.2.bn1.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.1.left.1.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>layer1.2.bn1.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.1.left.1.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>layer1.2.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.0.otherRes.1.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>layer1.2.conv2.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "      <td>layers.0.otherRes.1.left.3.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>layer1.2.bn2.weight</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.1.left.4.weight</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>layer1.2.bn2.bias</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.1.left.4.bias</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>layer1.2.bn2.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.1.left.4.running_mean</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>layer1.2.bn2.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>layers.0.otherRes.1.left.4.running_var</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>layer1.2.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.0.otherRes.1.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>layer2.0.conv1.weight</td>\n",
       "      <td>(128, 64, 3, 3)</td>\n",
       "      <td>layers.1.firstRes.left.0.weight</td>\n",
       "      <td>(128, 64, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>layer2.0.bn1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.left.1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>layer2.0.bn1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.left.1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>layer2.0.bn1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.left.1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>layer2.0.bn1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.left.1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>layer2.0.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.1.firstRes.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>layer2.0.conv2.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>layers.1.firstRes.left.3.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>layer2.0.bn2.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.left.4.weight</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>layer2.0.bn2.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.left.4.bias</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>layer2.0.bn2.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.left.4.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>layer2.0.bn2.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.left.4.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>layer2.0.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.1.firstRes.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>layer2.0.downsample.0.weight</td>\n",
       "      <td>(128, 64, 1, 1)</td>\n",
       "      <td>layers.1.firstRes.right.0.weight</td>\n",
       "      <td>(128, 64, 1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>layer2.0.downsample.1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.right.1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>layer2.0.downsample.1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.right.1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>layer2.0.downsample.1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.right.1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>layer2.0.downsample.1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.firstRes.right.1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>layer2.0.downsample.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.1.firstRes.right.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>layer2.1.conv1.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>layers.1.otherRes.0.left.0.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>layer2.1.bn1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.0.left.1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>layer2.1.bn1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.0.left.1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>layer2.1.bn1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.0.left.1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>layer2.1.bn1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.0.left.1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>layer2.1.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.1.otherRes.0.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>layer2.1.conv2.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>layers.1.otherRes.0.left.3.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>layer2.1.bn2.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.0.left.4.weight</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>layer2.1.bn2.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.0.left.4.bias</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>layer2.1.bn2.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.0.left.4.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>layer2.1.bn2.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.0.left.4.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>layer2.1.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.1.otherRes.0.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>layer2.2.conv1.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>layers.1.otherRes.1.left.0.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>layer2.2.bn1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.1.left.1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>layer2.2.bn1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.1.left.1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>layer2.2.bn1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.1.left.1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>layer2.2.bn1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.1.left.1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>layer2.2.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.1.otherRes.1.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>layer2.2.conv2.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>layers.1.otherRes.1.left.3.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>layer2.2.bn2.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.1.left.4.weight</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>layer2.2.bn2.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.1.left.4.bias</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>layer2.2.bn2.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.1.left.4.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>layer2.2.bn2.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.1.left.4.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>layer2.2.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.1.otherRes.1.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>layer2.3.conv1.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>layers.1.otherRes.2.left.0.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>layer2.3.bn1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.2.left.1.weight</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>layer2.3.bn1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.2.left.1.bias</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>layer2.3.bn1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.2.left.1.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>layer2.3.bn1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.2.left.1.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>layer2.3.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.1.otherRes.2.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>layer2.3.conv2.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>layers.1.otherRes.2.left.3.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>layer2.3.bn2.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.2.left.4.weight</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>layer2.3.bn2.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.2.left.4.bias</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>layer2.3.bn2.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.2.left.4.running_mean</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>layer2.3.bn2.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>layers.1.otherRes.2.left.4.running_var</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>layer2.3.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.1.otherRes.2.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>layer3.0.conv1.weight</td>\n",
       "      <td>(256, 128, 3, 3)</td>\n",
       "      <td>layers.2.firstRes.left.0.weight</td>\n",
       "      <td>(256, 128, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>layer3.0.bn1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.left.1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>layer3.0.bn1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.left.1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>layer3.0.bn1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.left.1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>layer3.0.bn1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.left.1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>layer3.0.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.firstRes.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>layer3.0.conv2.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.firstRes.left.3.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>layer3.0.bn2.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.left.4.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>layer3.0.bn2.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.left.4.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>layer3.0.bn2.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.left.4.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>layer3.0.bn2.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.left.4.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>layer3.0.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.firstRes.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>layer3.0.downsample.0.weight</td>\n",
       "      <td>(256, 128, 1, 1)</td>\n",
       "      <td>layers.2.firstRes.right.0.weight</td>\n",
       "      <td>(256, 128, 1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>layer3.0.downsample.1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.right.1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>layer3.0.downsample.1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.right.1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>layer3.0.downsample.1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.right.1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>layer3.0.downsample.1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.firstRes.right.1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>layer3.0.downsample.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.firstRes.right.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>layer3.1.conv1.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.0.left.0.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>layer3.1.bn1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.0.left.1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>layer3.1.bn1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.0.left.1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>layer3.1.bn1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.0.left.1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>layer3.1.bn1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.0.left.1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>layer3.1.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.0.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>layer3.1.conv2.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.0.left.3.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>layer3.1.bn2.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.0.left.4.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>layer3.1.bn2.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.0.left.4.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>layer3.1.bn2.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.0.left.4.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>layer3.1.bn2.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.0.left.4.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>layer3.1.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.0.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>layer3.2.conv1.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.1.left.0.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>layer3.2.bn1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.1.left.1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>layer3.2.bn1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.1.left.1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>layer3.2.bn1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.1.left.1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>layer3.2.bn1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.1.left.1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>layer3.2.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.1.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>layer3.2.conv2.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.1.left.3.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>layer3.2.bn2.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.1.left.4.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>layer3.2.bn2.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.1.left.4.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>layer3.2.bn2.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.1.left.4.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>layer3.2.bn2.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.1.left.4.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>layer3.2.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.1.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>layer3.3.conv1.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.2.left.0.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>layer3.3.bn1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.2.left.1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>layer3.3.bn1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.2.left.1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>layer3.3.bn1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.2.left.1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>layer3.3.bn1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.2.left.1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>layer3.3.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.2.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>layer3.3.conv2.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.2.left.3.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>layer3.3.bn2.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.2.left.4.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>layer3.3.bn2.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.2.left.4.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>layer3.3.bn2.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.2.left.4.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>layer3.3.bn2.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.2.left.4.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>layer3.3.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.2.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>layer3.4.conv1.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.3.left.0.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>layer3.4.bn1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.3.left.1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>layer3.4.bn1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.3.left.1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>layer3.4.bn1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.3.left.1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>layer3.4.bn1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.3.left.1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>layer3.4.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.3.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>layer3.4.conv2.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.3.left.3.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>layer3.4.bn2.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.3.left.4.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>layer3.4.bn2.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.3.left.4.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>layer3.4.bn2.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.3.left.4.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>layer3.4.bn2.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.3.left.4.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>layer3.4.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.3.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>layer3.5.conv1.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.4.left.0.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>layer3.5.bn1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.4.left.1.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>layer3.5.bn1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.4.left.1.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>layer3.5.bn1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.4.left.1.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>layer3.5.bn1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.4.left.1.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>layer3.5.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.4.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>layer3.5.conv2.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>layers.2.otherRes.4.left.3.weight</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>layer3.5.bn2.weight</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.4.left.4.weight</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>layer3.5.bn2.bias</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.4.left.4.bias</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>layer3.5.bn2.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.4.left.4.running_mean</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>layer3.5.bn2.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>layers.2.otherRes.4.left.4.running_var</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>layer3.5.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.2.otherRes.4.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>layer4.0.conv1.weight</td>\n",
       "      <td>(512, 256, 3, 3)</td>\n",
       "      <td>layers.3.firstRes.left.0.weight</td>\n",
       "      <td>(512, 256, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>layer4.0.bn1.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.left.1.weight</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>layer4.0.bn1.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.left.1.bias</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>layer4.0.bn1.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.left.1.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>layer4.0.bn1.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.left.1.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>layer4.0.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.3.firstRes.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>layer4.0.conv2.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>layers.3.firstRes.left.3.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>layer4.0.bn2.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.left.4.weight</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>layer4.0.bn2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.left.4.bias</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>layer4.0.bn2.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.left.4.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>layer4.0.bn2.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.left.4.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>layer4.0.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.3.firstRes.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>layer4.0.downsample.0.weight</td>\n",
       "      <td>(512, 256, 1, 1)</td>\n",
       "      <td>layers.3.firstRes.right.0.weight</td>\n",
       "      <td>(512, 256, 1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>layer4.0.downsample.1.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.right.1.weight</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>layer4.0.downsample.1.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.right.1.bias</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>layer4.0.downsample.1.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.right.1.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>layer4.0.downsample.1.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.firstRes.right.1.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>layer4.0.downsample.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.3.firstRes.right.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>layer4.1.conv1.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>layers.3.otherRes.0.left.0.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>layer4.1.bn1.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.0.left.1.weight</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>layer4.1.bn1.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.0.left.1.bias</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>layer4.1.bn1.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.0.left.1.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>layer4.1.bn1.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.0.left.1.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>layer4.1.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.3.otherRes.0.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>layer4.1.conv2.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>layers.3.otherRes.0.left.3.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>layer4.1.bn2.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.0.left.4.weight</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>layer4.1.bn2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.0.left.4.bias</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>layer4.1.bn2.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.0.left.4.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>layer4.1.bn2.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.0.left.4.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>layer4.1.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.3.otherRes.0.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>layer4.2.conv1.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>layers.3.otherRes.1.left.0.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>layer4.2.bn1.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.1.left.1.weight</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>layer4.2.bn1.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.1.left.1.bias</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>layer4.2.bn1.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.1.left.1.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>layer4.2.bn1.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.1.left.1.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>layer4.2.bn1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.3.otherRes.1.left.1.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>layer4.2.conv2.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>layers.3.otherRes.1.left.3.weight</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>layer4.2.bn2.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.1.left.4.weight</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>layer4.2.bn2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.1.left.4.bias</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>layer4.2.bn2.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.1.left.4.running_mean</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>layer4.2.bn2.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>layers.3.otherRes.1.left.4.running_var</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>layer4.2.bn2.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "      <td>layers.3.otherRes.1.left.4.num_batches_tracked</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>fc.weight</td>\n",
       "      <td>(1000, 512)</td>\n",
       "      <td>postBlockGroups.2.weight</td>\n",
       "      <td>(1000, 512)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>fc.bias</td>\n",
       "      <td>(1000,)</td>\n",
       "      <td>postBlockGroups.2.bias</td>\n",
       "      <td>(1000,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    their name       their shape  \\\n",
       "0                                 conv1.weight     (64, 3, 7, 7)   \n",
       "1                                   bn1.weight             (64,)   \n",
       "2                                     bn1.bias             (64,)   \n",
       "3                             bn1.running_mean             (64,)   \n",
       "4                              bn1.running_var             (64,)   \n",
       "5                      bn1.num_batches_tracked                ()   \n",
       "6                        layer1.0.conv1.weight    (64, 64, 3, 3)   \n",
       "7                          layer1.0.bn1.weight             (64,)   \n",
       "8                            layer1.0.bn1.bias             (64,)   \n",
       "9                    layer1.0.bn1.running_mean             (64,)   \n",
       "10                    layer1.0.bn1.running_var             (64,)   \n",
       "11            layer1.0.bn1.num_batches_tracked                ()   \n",
       "12                       layer1.0.conv2.weight    (64, 64, 3, 3)   \n",
       "13                         layer1.0.bn2.weight             (64,)   \n",
       "14                           layer1.0.bn2.bias             (64,)   \n",
       "15                   layer1.0.bn2.running_mean             (64,)   \n",
       "16                    layer1.0.bn2.running_var             (64,)   \n",
       "17            layer1.0.bn2.num_batches_tracked                ()   \n",
       "18                       layer1.1.conv1.weight    (64, 64, 3, 3)   \n",
       "19                         layer1.1.bn1.weight             (64,)   \n",
       "20                           layer1.1.bn1.bias             (64,)   \n",
       "21                   layer1.1.bn1.running_mean             (64,)   \n",
       "22                    layer1.1.bn1.running_var             (64,)   \n",
       "23            layer1.1.bn1.num_batches_tracked                ()   \n",
       "24                       layer1.1.conv2.weight    (64, 64, 3, 3)   \n",
       "25                         layer1.1.bn2.weight             (64,)   \n",
       "26                           layer1.1.bn2.bias             (64,)   \n",
       "27                   layer1.1.bn2.running_mean             (64,)   \n",
       "28                    layer1.1.bn2.running_var             (64,)   \n",
       "29            layer1.1.bn2.num_batches_tracked                ()   \n",
       "30                       layer1.2.conv1.weight    (64, 64, 3, 3)   \n",
       "31                         layer1.2.bn1.weight             (64,)   \n",
       "32                           layer1.2.bn1.bias             (64,)   \n",
       "33                   layer1.2.bn1.running_mean             (64,)   \n",
       "34                    layer1.2.bn1.running_var             (64,)   \n",
       "35            layer1.2.bn1.num_batches_tracked                ()   \n",
       "36                       layer1.2.conv2.weight    (64, 64, 3, 3)   \n",
       "37                         layer1.2.bn2.weight             (64,)   \n",
       "38                           layer1.2.bn2.bias             (64,)   \n",
       "39                   layer1.2.bn2.running_mean             (64,)   \n",
       "40                    layer1.2.bn2.running_var             (64,)   \n",
       "41            layer1.2.bn2.num_batches_tracked                ()   \n",
       "42                       layer2.0.conv1.weight   (128, 64, 3, 3)   \n",
       "43                         layer2.0.bn1.weight            (128,)   \n",
       "44                           layer2.0.bn1.bias            (128,)   \n",
       "45                   layer2.0.bn1.running_mean            (128,)   \n",
       "46                    layer2.0.bn1.running_var            (128,)   \n",
       "47            layer2.0.bn1.num_batches_tracked                ()   \n",
       "48                       layer2.0.conv2.weight  (128, 128, 3, 3)   \n",
       "49                         layer2.0.bn2.weight            (128,)   \n",
       "50                           layer2.0.bn2.bias            (128,)   \n",
       "51                   layer2.0.bn2.running_mean            (128,)   \n",
       "52                    layer2.0.bn2.running_var            (128,)   \n",
       "53            layer2.0.bn2.num_batches_tracked                ()   \n",
       "54                layer2.0.downsample.0.weight   (128, 64, 1, 1)   \n",
       "55                layer2.0.downsample.1.weight            (128,)   \n",
       "56                  layer2.0.downsample.1.bias            (128,)   \n",
       "57          layer2.0.downsample.1.running_mean            (128,)   \n",
       "58           layer2.0.downsample.1.running_var            (128,)   \n",
       "59   layer2.0.downsample.1.num_batches_tracked                ()   \n",
       "60                       layer2.1.conv1.weight  (128, 128, 3, 3)   \n",
       "61                         layer2.1.bn1.weight            (128,)   \n",
       "62                           layer2.1.bn1.bias            (128,)   \n",
       "63                   layer2.1.bn1.running_mean            (128,)   \n",
       "64                    layer2.1.bn1.running_var            (128,)   \n",
       "65            layer2.1.bn1.num_batches_tracked                ()   \n",
       "66                       layer2.1.conv2.weight  (128, 128, 3, 3)   \n",
       "67                         layer2.1.bn2.weight            (128,)   \n",
       "68                           layer2.1.bn2.bias            (128,)   \n",
       "69                   layer2.1.bn2.running_mean            (128,)   \n",
       "70                    layer2.1.bn2.running_var            (128,)   \n",
       "71            layer2.1.bn2.num_batches_tracked                ()   \n",
       "72                       layer2.2.conv1.weight  (128, 128, 3, 3)   \n",
       "73                         layer2.2.bn1.weight            (128,)   \n",
       "74                           layer2.2.bn1.bias            (128,)   \n",
       "75                   layer2.2.bn1.running_mean            (128,)   \n",
       "76                    layer2.2.bn1.running_var            (128,)   \n",
       "77            layer2.2.bn1.num_batches_tracked                ()   \n",
       "78                       layer2.2.conv2.weight  (128, 128, 3, 3)   \n",
       "79                         layer2.2.bn2.weight            (128,)   \n",
       "80                           layer2.2.bn2.bias            (128,)   \n",
       "81                   layer2.2.bn2.running_mean            (128,)   \n",
       "82                    layer2.2.bn2.running_var            (128,)   \n",
       "83            layer2.2.bn2.num_batches_tracked                ()   \n",
       "84                       layer2.3.conv1.weight  (128, 128, 3, 3)   \n",
       "85                         layer2.3.bn1.weight            (128,)   \n",
       "86                           layer2.3.bn1.bias            (128,)   \n",
       "87                   layer2.3.bn1.running_mean            (128,)   \n",
       "88                    layer2.3.bn1.running_var            (128,)   \n",
       "89            layer2.3.bn1.num_batches_tracked                ()   \n",
       "90                       layer2.3.conv2.weight  (128, 128, 3, 3)   \n",
       "91                         layer2.3.bn2.weight            (128,)   \n",
       "92                           layer2.3.bn2.bias            (128,)   \n",
       "93                   layer2.3.bn2.running_mean            (128,)   \n",
       "94                    layer2.3.bn2.running_var            (128,)   \n",
       "95            layer2.3.bn2.num_batches_tracked                ()   \n",
       "96                       layer3.0.conv1.weight  (256, 128, 3, 3)   \n",
       "97                         layer3.0.bn1.weight            (256,)   \n",
       "98                           layer3.0.bn1.bias            (256,)   \n",
       "99                   layer3.0.bn1.running_mean            (256,)   \n",
       "100                   layer3.0.bn1.running_var            (256,)   \n",
       "101           layer3.0.bn1.num_batches_tracked                ()   \n",
       "102                      layer3.0.conv2.weight  (256, 256, 3, 3)   \n",
       "103                        layer3.0.bn2.weight            (256,)   \n",
       "104                          layer3.0.bn2.bias            (256,)   \n",
       "105                  layer3.0.bn2.running_mean            (256,)   \n",
       "106                   layer3.0.bn2.running_var            (256,)   \n",
       "107           layer3.0.bn2.num_batches_tracked                ()   \n",
       "108               layer3.0.downsample.0.weight  (256, 128, 1, 1)   \n",
       "109               layer3.0.downsample.1.weight            (256,)   \n",
       "110                 layer3.0.downsample.1.bias            (256,)   \n",
       "111         layer3.0.downsample.1.running_mean            (256,)   \n",
       "112          layer3.0.downsample.1.running_var            (256,)   \n",
       "113  layer3.0.downsample.1.num_batches_tracked                ()   \n",
       "114                      layer3.1.conv1.weight  (256, 256, 3, 3)   \n",
       "115                        layer3.1.bn1.weight            (256,)   \n",
       "116                          layer3.1.bn1.bias            (256,)   \n",
       "117                  layer3.1.bn1.running_mean            (256,)   \n",
       "118                   layer3.1.bn1.running_var            (256,)   \n",
       "119           layer3.1.bn1.num_batches_tracked                ()   \n",
       "120                      layer3.1.conv2.weight  (256, 256, 3, 3)   \n",
       "121                        layer3.1.bn2.weight            (256,)   \n",
       "122                          layer3.1.bn2.bias            (256,)   \n",
       "123                  layer3.1.bn2.running_mean            (256,)   \n",
       "124                   layer3.1.bn2.running_var            (256,)   \n",
       "125           layer3.1.bn2.num_batches_tracked                ()   \n",
       "126                      layer3.2.conv1.weight  (256, 256, 3, 3)   \n",
       "127                        layer3.2.bn1.weight            (256,)   \n",
       "128                          layer3.2.bn1.bias            (256,)   \n",
       "129                  layer3.2.bn1.running_mean            (256,)   \n",
       "130                   layer3.2.bn1.running_var            (256,)   \n",
       "131           layer3.2.bn1.num_batches_tracked                ()   \n",
       "132                      layer3.2.conv2.weight  (256, 256, 3, 3)   \n",
       "133                        layer3.2.bn2.weight            (256,)   \n",
       "134                          layer3.2.bn2.bias            (256,)   \n",
       "135                  layer3.2.bn2.running_mean            (256,)   \n",
       "136                   layer3.2.bn2.running_var            (256,)   \n",
       "137           layer3.2.bn2.num_batches_tracked                ()   \n",
       "138                      layer3.3.conv1.weight  (256, 256, 3, 3)   \n",
       "139                        layer3.3.bn1.weight            (256,)   \n",
       "140                          layer3.3.bn1.bias            (256,)   \n",
       "141                  layer3.3.bn1.running_mean            (256,)   \n",
       "142                   layer3.3.bn1.running_var            (256,)   \n",
       "143           layer3.3.bn1.num_batches_tracked                ()   \n",
       "144                      layer3.3.conv2.weight  (256, 256, 3, 3)   \n",
       "145                        layer3.3.bn2.weight            (256,)   \n",
       "146                          layer3.3.bn2.bias            (256,)   \n",
       "147                  layer3.3.bn2.running_mean            (256,)   \n",
       "148                   layer3.3.bn2.running_var            (256,)   \n",
       "149           layer3.3.bn2.num_batches_tracked                ()   \n",
       "150                      layer3.4.conv1.weight  (256, 256, 3, 3)   \n",
       "151                        layer3.4.bn1.weight            (256,)   \n",
       "152                          layer3.4.bn1.bias            (256,)   \n",
       "153                  layer3.4.bn1.running_mean            (256,)   \n",
       "154                   layer3.4.bn1.running_var            (256,)   \n",
       "155           layer3.4.bn1.num_batches_tracked                ()   \n",
       "156                      layer3.4.conv2.weight  (256, 256, 3, 3)   \n",
       "157                        layer3.4.bn2.weight            (256,)   \n",
       "158                          layer3.4.bn2.bias            (256,)   \n",
       "159                  layer3.4.bn2.running_mean            (256,)   \n",
       "160                   layer3.4.bn2.running_var            (256,)   \n",
       "161           layer3.4.bn2.num_batches_tracked                ()   \n",
       "162                      layer3.5.conv1.weight  (256, 256, 3, 3)   \n",
       "163                        layer3.5.bn1.weight            (256,)   \n",
       "164                          layer3.5.bn1.bias            (256,)   \n",
       "165                  layer3.5.bn1.running_mean            (256,)   \n",
       "166                   layer3.5.bn1.running_var            (256,)   \n",
       "167           layer3.5.bn1.num_batches_tracked                ()   \n",
       "168                      layer3.5.conv2.weight  (256, 256, 3, 3)   \n",
       "169                        layer3.5.bn2.weight            (256,)   \n",
       "170                          layer3.5.bn2.bias            (256,)   \n",
       "171                  layer3.5.bn2.running_mean            (256,)   \n",
       "172                   layer3.5.bn2.running_var            (256,)   \n",
       "173           layer3.5.bn2.num_batches_tracked                ()   \n",
       "174                      layer4.0.conv1.weight  (512, 256, 3, 3)   \n",
       "175                        layer4.0.bn1.weight            (512,)   \n",
       "176                          layer4.0.bn1.bias            (512,)   \n",
       "177                  layer4.0.bn1.running_mean            (512,)   \n",
       "178                   layer4.0.bn1.running_var            (512,)   \n",
       "179           layer4.0.bn1.num_batches_tracked                ()   \n",
       "180                      layer4.0.conv2.weight  (512, 512, 3, 3)   \n",
       "181                        layer4.0.bn2.weight            (512,)   \n",
       "182                          layer4.0.bn2.bias            (512,)   \n",
       "183                  layer4.0.bn2.running_mean            (512,)   \n",
       "184                   layer4.0.bn2.running_var            (512,)   \n",
       "185           layer4.0.bn2.num_batches_tracked                ()   \n",
       "186               layer4.0.downsample.0.weight  (512, 256, 1, 1)   \n",
       "187               layer4.0.downsample.1.weight            (512,)   \n",
       "188                 layer4.0.downsample.1.bias            (512,)   \n",
       "189         layer4.0.downsample.1.running_mean            (512,)   \n",
       "190          layer4.0.downsample.1.running_var            (512,)   \n",
       "191  layer4.0.downsample.1.num_batches_tracked                ()   \n",
       "192                      layer4.1.conv1.weight  (512, 512, 3, 3)   \n",
       "193                        layer4.1.bn1.weight            (512,)   \n",
       "194                          layer4.1.bn1.bias            (512,)   \n",
       "195                  layer4.1.bn1.running_mean            (512,)   \n",
       "196                   layer4.1.bn1.running_var            (512,)   \n",
       "197           layer4.1.bn1.num_batches_tracked                ()   \n",
       "198                      layer4.1.conv2.weight  (512, 512, 3, 3)   \n",
       "199                        layer4.1.bn2.weight            (512,)   \n",
       "200                          layer4.1.bn2.bias            (512,)   \n",
       "201                  layer4.1.bn2.running_mean            (512,)   \n",
       "202                   layer4.1.bn2.running_var            (512,)   \n",
       "203           layer4.1.bn2.num_batches_tracked                ()   \n",
       "204                      layer4.2.conv1.weight  (512, 512, 3, 3)   \n",
       "205                        layer4.2.bn1.weight            (512,)   \n",
       "206                          layer4.2.bn1.bias            (512,)   \n",
       "207                  layer4.2.bn1.running_mean            (512,)   \n",
       "208                   layer4.2.bn1.running_var            (512,)   \n",
       "209           layer4.2.bn1.num_batches_tracked                ()   \n",
       "210                      layer4.2.conv2.weight  (512, 512, 3, 3)   \n",
       "211                        layer4.2.bn2.weight            (512,)   \n",
       "212                          layer4.2.bn2.bias            (512,)   \n",
       "213                  layer4.2.bn2.running_mean            (512,)   \n",
       "214                   layer4.2.bn2.running_var            (512,)   \n",
       "215           layer4.2.bn2.num_batches_tracked                ()   \n",
       "216                                  fc.weight       (1000, 512)   \n",
       "217                                    fc.bias           (1000,)   \n",
       "\n",
       "                                          your name        your shape  \n",
       "0                           preBlockGroups.0.weight     (64, 3, 7, 7)  \n",
       "1                           preBlockGroups.1.weight             (64,)  \n",
       "2                             preBlockGroups.1.bias             (64,)  \n",
       "3                     preBlockGroups.1.running_mean             (64,)  \n",
       "4                      preBlockGroups.1.running_var             (64,)  \n",
       "5              preBlockGroups.1.num_batches_tracked                ()  \n",
       "6                   layers.0.firstRes.left.0.weight    (64, 64, 3, 3)  \n",
       "7                   layers.0.firstRes.left.1.weight             (64,)  \n",
       "8                     layers.0.firstRes.left.1.bias             (64,)  \n",
       "9             layers.0.firstRes.left.1.running_mean             (64,)  \n",
       "10             layers.0.firstRes.left.1.running_var             (64,)  \n",
       "11     layers.0.firstRes.left.1.num_batches_tracked                ()  \n",
       "12                  layers.0.firstRes.left.3.weight    (64, 64, 3, 3)  \n",
       "13                  layers.0.firstRes.left.4.weight             (64,)  \n",
       "14                    layers.0.firstRes.left.4.bias             (64,)  \n",
       "15            layers.0.firstRes.left.4.running_mean             (64,)  \n",
       "16             layers.0.firstRes.left.4.running_var             (64,)  \n",
       "17     layers.0.firstRes.left.4.num_batches_tracked                ()  \n",
       "18                layers.0.otherRes.0.left.0.weight    (64, 64, 3, 3)  \n",
       "19                layers.0.otherRes.0.left.1.weight             (64,)  \n",
       "20                  layers.0.otherRes.0.left.1.bias             (64,)  \n",
       "21          layers.0.otherRes.0.left.1.running_mean             (64,)  \n",
       "22           layers.0.otherRes.0.left.1.running_var             (64,)  \n",
       "23   layers.0.otherRes.0.left.1.num_batches_tracked                ()  \n",
       "24                layers.0.otherRes.0.left.3.weight    (64, 64, 3, 3)  \n",
       "25                layers.0.otherRes.0.left.4.weight             (64,)  \n",
       "26                  layers.0.otherRes.0.left.4.bias             (64,)  \n",
       "27          layers.0.otherRes.0.left.4.running_mean             (64,)  \n",
       "28           layers.0.otherRes.0.left.4.running_var             (64,)  \n",
       "29   layers.0.otherRes.0.left.4.num_batches_tracked                ()  \n",
       "30                layers.0.otherRes.1.left.0.weight    (64, 64, 3, 3)  \n",
       "31                layers.0.otherRes.1.left.1.weight             (64,)  \n",
       "32                  layers.0.otherRes.1.left.1.bias             (64,)  \n",
       "33          layers.0.otherRes.1.left.1.running_mean             (64,)  \n",
       "34           layers.0.otherRes.1.left.1.running_var             (64,)  \n",
       "35   layers.0.otherRes.1.left.1.num_batches_tracked                ()  \n",
       "36                layers.0.otherRes.1.left.3.weight    (64, 64, 3, 3)  \n",
       "37                layers.0.otherRes.1.left.4.weight             (64,)  \n",
       "38                  layers.0.otherRes.1.left.4.bias             (64,)  \n",
       "39          layers.0.otherRes.1.left.4.running_mean             (64,)  \n",
       "40           layers.0.otherRes.1.left.4.running_var             (64,)  \n",
       "41   layers.0.otherRes.1.left.4.num_batches_tracked                ()  \n",
       "42                  layers.1.firstRes.left.0.weight   (128, 64, 3, 3)  \n",
       "43                  layers.1.firstRes.left.1.weight            (128,)  \n",
       "44                    layers.1.firstRes.left.1.bias            (128,)  \n",
       "45            layers.1.firstRes.left.1.running_mean            (128,)  \n",
       "46             layers.1.firstRes.left.1.running_var            (128,)  \n",
       "47     layers.1.firstRes.left.1.num_batches_tracked                ()  \n",
       "48                  layers.1.firstRes.left.3.weight  (128, 128, 3, 3)  \n",
       "49                  layers.1.firstRes.left.4.weight            (128,)  \n",
       "50                    layers.1.firstRes.left.4.bias            (128,)  \n",
       "51            layers.1.firstRes.left.4.running_mean            (128,)  \n",
       "52             layers.1.firstRes.left.4.running_var            (128,)  \n",
       "53     layers.1.firstRes.left.4.num_batches_tracked                ()  \n",
       "54                 layers.1.firstRes.right.0.weight   (128, 64, 1, 1)  \n",
       "55                 layers.1.firstRes.right.1.weight            (128,)  \n",
       "56                   layers.1.firstRes.right.1.bias            (128,)  \n",
       "57           layers.1.firstRes.right.1.running_mean            (128,)  \n",
       "58            layers.1.firstRes.right.1.running_var            (128,)  \n",
       "59    layers.1.firstRes.right.1.num_batches_tracked                ()  \n",
       "60                layers.1.otherRes.0.left.0.weight  (128, 128, 3, 3)  \n",
       "61                layers.1.otherRes.0.left.1.weight            (128,)  \n",
       "62                  layers.1.otherRes.0.left.1.bias            (128,)  \n",
       "63          layers.1.otherRes.0.left.1.running_mean            (128,)  \n",
       "64           layers.1.otherRes.0.left.1.running_var            (128,)  \n",
       "65   layers.1.otherRes.0.left.1.num_batches_tracked                ()  \n",
       "66                layers.1.otherRes.0.left.3.weight  (128, 128, 3, 3)  \n",
       "67                layers.1.otherRes.0.left.4.weight            (128,)  \n",
       "68                  layers.1.otherRes.0.left.4.bias            (128,)  \n",
       "69          layers.1.otherRes.0.left.4.running_mean            (128,)  \n",
       "70           layers.1.otherRes.0.left.4.running_var            (128,)  \n",
       "71   layers.1.otherRes.0.left.4.num_batches_tracked                ()  \n",
       "72                layers.1.otherRes.1.left.0.weight  (128, 128, 3, 3)  \n",
       "73                layers.1.otherRes.1.left.1.weight            (128,)  \n",
       "74                  layers.1.otherRes.1.left.1.bias            (128,)  \n",
       "75          layers.1.otherRes.1.left.1.running_mean            (128,)  \n",
       "76           layers.1.otherRes.1.left.1.running_var            (128,)  \n",
       "77   layers.1.otherRes.1.left.1.num_batches_tracked                ()  \n",
       "78                layers.1.otherRes.1.left.3.weight  (128, 128, 3, 3)  \n",
       "79                layers.1.otherRes.1.left.4.weight            (128,)  \n",
       "80                  layers.1.otherRes.1.left.4.bias            (128,)  \n",
       "81          layers.1.otherRes.1.left.4.running_mean            (128,)  \n",
       "82           layers.1.otherRes.1.left.4.running_var            (128,)  \n",
       "83   layers.1.otherRes.1.left.4.num_batches_tracked                ()  \n",
       "84                layers.1.otherRes.2.left.0.weight  (128, 128, 3, 3)  \n",
       "85                layers.1.otherRes.2.left.1.weight            (128,)  \n",
       "86                  layers.1.otherRes.2.left.1.bias            (128,)  \n",
       "87          layers.1.otherRes.2.left.1.running_mean            (128,)  \n",
       "88           layers.1.otherRes.2.left.1.running_var            (128,)  \n",
       "89   layers.1.otherRes.2.left.1.num_batches_tracked                ()  \n",
       "90                layers.1.otherRes.2.left.3.weight  (128, 128, 3, 3)  \n",
       "91                layers.1.otherRes.2.left.4.weight            (128,)  \n",
       "92                  layers.1.otherRes.2.left.4.bias            (128,)  \n",
       "93          layers.1.otherRes.2.left.4.running_mean            (128,)  \n",
       "94           layers.1.otherRes.2.left.4.running_var            (128,)  \n",
       "95   layers.1.otherRes.2.left.4.num_batches_tracked                ()  \n",
       "96                  layers.2.firstRes.left.0.weight  (256, 128, 3, 3)  \n",
       "97                  layers.2.firstRes.left.1.weight            (256,)  \n",
       "98                    layers.2.firstRes.left.1.bias            (256,)  \n",
       "99            layers.2.firstRes.left.1.running_mean            (256,)  \n",
       "100            layers.2.firstRes.left.1.running_var            (256,)  \n",
       "101    layers.2.firstRes.left.1.num_batches_tracked                ()  \n",
       "102                 layers.2.firstRes.left.3.weight  (256, 256, 3, 3)  \n",
       "103                 layers.2.firstRes.left.4.weight            (256,)  \n",
       "104                   layers.2.firstRes.left.4.bias            (256,)  \n",
       "105           layers.2.firstRes.left.4.running_mean            (256,)  \n",
       "106            layers.2.firstRes.left.4.running_var            (256,)  \n",
       "107    layers.2.firstRes.left.4.num_batches_tracked                ()  \n",
       "108                layers.2.firstRes.right.0.weight  (256, 128, 1, 1)  \n",
       "109                layers.2.firstRes.right.1.weight            (256,)  \n",
       "110                  layers.2.firstRes.right.1.bias            (256,)  \n",
       "111          layers.2.firstRes.right.1.running_mean            (256,)  \n",
       "112           layers.2.firstRes.right.1.running_var            (256,)  \n",
       "113   layers.2.firstRes.right.1.num_batches_tracked                ()  \n",
       "114               layers.2.otherRes.0.left.0.weight  (256, 256, 3, 3)  \n",
       "115               layers.2.otherRes.0.left.1.weight            (256,)  \n",
       "116                 layers.2.otherRes.0.left.1.bias            (256,)  \n",
       "117         layers.2.otherRes.0.left.1.running_mean            (256,)  \n",
       "118          layers.2.otherRes.0.left.1.running_var            (256,)  \n",
       "119  layers.2.otherRes.0.left.1.num_batches_tracked                ()  \n",
       "120               layers.2.otherRes.0.left.3.weight  (256, 256, 3, 3)  \n",
       "121               layers.2.otherRes.0.left.4.weight            (256,)  \n",
       "122                 layers.2.otherRes.0.left.4.bias            (256,)  \n",
       "123         layers.2.otherRes.0.left.4.running_mean            (256,)  \n",
       "124          layers.2.otherRes.0.left.4.running_var            (256,)  \n",
       "125  layers.2.otherRes.0.left.4.num_batches_tracked                ()  \n",
       "126               layers.2.otherRes.1.left.0.weight  (256, 256, 3, 3)  \n",
       "127               layers.2.otherRes.1.left.1.weight            (256,)  \n",
       "128                 layers.2.otherRes.1.left.1.bias            (256,)  \n",
       "129         layers.2.otherRes.1.left.1.running_mean            (256,)  \n",
       "130          layers.2.otherRes.1.left.1.running_var            (256,)  \n",
       "131  layers.2.otherRes.1.left.1.num_batches_tracked                ()  \n",
       "132               layers.2.otherRes.1.left.3.weight  (256, 256, 3, 3)  \n",
       "133               layers.2.otherRes.1.left.4.weight            (256,)  \n",
       "134                 layers.2.otherRes.1.left.4.bias            (256,)  \n",
       "135         layers.2.otherRes.1.left.4.running_mean            (256,)  \n",
       "136          layers.2.otherRes.1.left.4.running_var            (256,)  \n",
       "137  layers.2.otherRes.1.left.4.num_batches_tracked                ()  \n",
       "138               layers.2.otherRes.2.left.0.weight  (256, 256, 3, 3)  \n",
       "139               layers.2.otherRes.2.left.1.weight            (256,)  \n",
       "140                 layers.2.otherRes.2.left.1.bias            (256,)  \n",
       "141         layers.2.otherRes.2.left.1.running_mean            (256,)  \n",
       "142          layers.2.otherRes.2.left.1.running_var            (256,)  \n",
       "143  layers.2.otherRes.2.left.1.num_batches_tracked                ()  \n",
       "144               layers.2.otherRes.2.left.3.weight  (256, 256, 3, 3)  \n",
       "145               layers.2.otherRes.2.left.4.weight            (256,)  \n",
       "146                 layers.2.otherRes.2.left.4.bias            (256,)  \n",
       "147         layers.2.otherRes.2.left.4.running_mean            (256,)  \n",
       "148          layers.2.otherRes.2.left.4.running_var            (256,)  \n",
       "149  layers.2.otherRes.2.left.4.num_batches_tracked                ()  \n",
       "150               layers.2.otherRes.3.left.0.weight  (256, 256, 3, 3)  \n",
       "151               layers.2.otherRes.3.left.1.weight            (256,)  \n",
       "152                 layers.2.otherRes.3.left.1.bias            (256,)  \n",
       "153         layers.2.otherRes.3.left.1.running_mean            (256,)  \n",
       "154          layers.2.otherRes.3.left.1.running_var            (256,)  \n",
       "155  layers.2.otherRes.3.left.1.num_batches_tracked                ()  \n",
       "156               layers.2.otherRes.3.left.3.weight  (256, 256, 3, 3)  \n",
       "157               layers.2.otherRes.3.left.4.weight            (256,)  \n",
       "158                 layers.2.otherRes.3.left.4.bias            (256,)  \n",
       "159         layers.2.otherRes.3.left.4.running_mean            (256,)  \n",
       "160          layers.2.otherRes.3.left.4.running_var            (256,)  \n",
       "161  layers.2.otherRes.3.left.4.num_batches_tracked                ()  \n",
       "162               layers.2.otherRes.4.left.0.weight  (256, 256, 3, 3)  \n",
       "163               layers.2.otherRes.4.left.1.weight            (256,)  \n",
       "164                 layers.2.otherRes.4.left.1.bias            (256,)  \n",
       "165         layers.2.otherRes.4.left.1.running_mean            (256,)  \n",
       "166          layers.2.otherRes.4.left.1.running_var            (256,)  \n",
       "167  layers.2.otherRes.4.left.1.num_batches_tracked                ()  \n",
       "168               layers.2.otherRes.4.left.3.weight  (256, 256, 3, 3)  \n",
       "169               layers.2.otherRes.4.left.4.weight            (256,)  \n",
       "170                 layers.2.otherRes.4.left.4.bias            (256,)  \n",
       "171         layers.2.otherRes.4.left.4.running_mean            (256,)  \n",
       "172          layers.2.otherRes.4.left.4.running_var            (256,)  \n",
       "173  layers.2.otherRes.4.left.4.num_batches_tracked                ()  \n",
       "174                 layers.3.firstRes.left.0.weight  (512, 256, 3, 3)  \n",
       "175                 layers.3.firstRes.left.1.weight            (512,)  \n",
       "176                   layers.3.firstRes.left.1.bias            (512,)  \n",
       "177           layers.3.firstRes.left.1.running_mean            (512,)  \n",
       "178            layers.3.firstRes.left.1.running_var            (512,)  \n",
       "179    layers.3.firstRes.left.1.num_batches_tracked                ()  \n",
       "180                 layers.3.firstRes.left.3.weight  (512, 512, 3, 3)  \n",
       "181                 layers.3.firstRes.left.4.weight            (512,)  \n",
       "182                   layers.3.firstRes.left.4.bias            (512,)  \n",
       "183           layers.3.firstRes.left.4.running_mean            (512,)  \n",
       "184            layers.3.firstRes.left.4.running_var            (512,)  \n",
       "185    layers.3.firstRes.left.4.num_batches_tracked                ()  \n",
       "186                layers.3.firstRes.right.0.weight  (512, 256, 1, 1)  \n",
       "187                layers.3.firstRes.right.1.weight            (512,)  \n",
       "188                  layers.3.firstRes.right.1.bias            (512,)  \n",
       "189          layers.3.firstRes.right.1.running_mean            (512,)  \n",
       "190           layers.3.firstRes.right.1.running_var            (512,)  \n",
       "191   layers.3.firstRes.right.1.num_batches_tracked                ()  \n",
       "192               layers.3.otherRes.0.left.0.weight  (512, 512, 3, 3)  \n",
       "193               layers.3.otherRes.0.left.1.weight            (512,)  \n",
       "194                 layers.3.otherRes.0.left.1.bias            (512,)  \n",
       "195         layers.3.otherRes.0.left.1.running_mean            (512,)  \n",
       "196          layers.3.otherRes.0.left.1.running_var            (512,)  \n",
       "197  layers.3.otherRes.0.left.1.num_batches_tracked                ()  \n",
       "198               layers.3.otherRes.0.left.3.weight  (512, 512, 3, 3)  \n",
       "199               layers.3.otherRes.0.left.4.weight            (512,)  \n",
       "200                 layers.3.otherRes.0.left.4.bias            (512,)  \n",
       "201         layers.3.otherRes.0.left.4.running_mean            (512,)  \n",
       "202          layers.3.otherRes.0.left.4.running_var            (512,)  \n",
       "203  layers.3.otherRes.0.left.4.num_batches_tracked                ()  \n",
       "204               layers.3.otherRes.1.left.0.weight  (512, 512, 3, 3)  \n",
       "205               layers.3.otherRes.1.left.1.weight            (512,)  \n",
       "206                 layers.3.otherRes.1.left.1.bias            (512,)  \n",
       "207         layers.3.otherRes.1.left.1.running_mean            (512,)  \n",
       "208          layers.3.otherRes.1.left.1.running_var            (512,)  \n",
       "209  layers.3.otherRes.1.left.1.num_batches_tracked                ()  \n",
       "210               layers.3.otherRes.1.left.3.weight  (512, 512, 3, 3)  \n",
       "211               layers.3.otherRes.1.left.4.weight            (512,)  \n",
       "212                 layers.3.otherRes.1.left.4.bias            (512,)  \n",
       "213         layers.3.otherRes.1.left.4.running_mean            (512,)  \n",
       "214          layers.3.otherRes.1.left.4.running_var            (512,)  \n",
       "215  layers.3.otherRes.1.left.4.num_batches_tracked                ()  \n",
       "216                        postBlockGroups.2.weight       (1000, 512)  \n",
       "217                          postBlockGroups.2.bias           (1000,)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def copy_weights(myresnet: ResNet34, pretrained_resnet: torchvision.models.resnet.ResNet) -> ResNet34:\n",
    "    '''Copy over the weights of `pretrained_resnet` to your resnet.'''\n",
    "\n",
    "    mydict = myresnet.state_dict()\n",
    "    pretraineddict = pretrained_resnet.state_dict()\n",
    "\n",
    "    # Check the number of params/buffers is correct\n",
    "    assert len(mydict) == len(pretraineddict), \"Number of layers is wrong. Have you done the prev step correctly?\"\n",
    "\n",
    "    # Initialise an empty dictionary to store the correct key-value pairs\n",
    "    state_dict_to_load = {}\n",
    "\n",
    "    for (mykey, myvalue), (pretrainedkey, pretrainedvalue) in zip(mydict.items(), pretraineddict.items()):\n",
    "        state_dict_to_load[mykey] = pretrainedvalue\n",
    "\n",
    "    myresnet.load_state_dict(state_dict_to_load)\n",
    "\n",
    "    return myresnet\n",
    "\n",
    "utils.compare_my_resnet_to_pytorch(ResNet34())\n",
    "officialresnet = resnet34()\n",
    "myresnet = copy_weights(ResNet34(), officialresnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image \n",
    "import torch as t\n",
    "\n",
    "IMAGE_FILENAMES = [\n",
    "    \"chimpanzee.jpg\",\n",
    "    \"golden_retriever.jpg\",\n",
    "    \"platypus.jpg\",\n",
    "    \"frogs.jpg\",\n",
    "    \"fireworks.jpg\",\n",
    "    \"astronaut.jpg\",\n",
    "    \"iguana.jpg\",\n",
    "    \"volcano.jpg\",\n",
    "    \"goofy.jpg\",\n",
    "    \"dragonfly.jpg\",\n",
    "]\n",
    "\n",
    "IMAGE_FOLDER = Path(\"./resnet_inputs\")\n",
    "\n",
    "images = [Image.open(IMAGE_FOLDER / filename) for filename in IMAGE_FILENAMES]\n",
    "# ImageNet transforms copied from solution:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def prepare_data(images: list[Image.Image]) -> t.Tensor:\n",
    "    '''\n",
    "    Return: shape (batch=len(images), num_channels=3, height=224, width=224)\n",
    "    '''\n",
    "    return t.stack([transform(i) for i in images], dim = 0)\n",
    "\n",
    "prepared_images = prepare_data(images)\n",
    "\n",
    "def predict(model, images):\n",
    "    logits = model(images)\n",
    "    return logits.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224, 224])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([707, 707, 707, 707, 864,  71, 707, 707, 864, 707])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(myresnet, prepared_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"imagenet_labels.json\") as f:\n",
    "    imagenet_labels = list(json.load(f).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([707, 707, 707, 707, 864,  71, 707, 707, 864, 707])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(officialresnet, prepared_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prepared_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet34(\n",
       "  (preBlockGroups): Sequential(\n",
       "    (0): Conv2d(weights shape = torch.Size([64, 3, 7, 7]) stride = (2, 2) padding = (3, 3) )\n",
       "    (1): BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size = (3, 3) stride = (2, 2) padding = (1, 1))\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): BlockGroup(\n",
       "      (firstRes): ResidualBlock(\n",
       "        (left): Sequential(\n",
       "          (0): Conv2d(weights shape = torch.Size([64, 64, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "          (1): BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(weights shape = torch.Size([64, 64, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "          (4): BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1)\n",
       "        )\n",
       "        (right): Sequential(\n",
       "          (0): Identity()\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (otherRes): ModuleList(\n",
       "        (0): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([64, 64, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([64, 64, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([64, 64, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([64, 64, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): BlockGroup(\n",
       "      (firstRes): ResidualBlock(\n",
       "        (left): Sequential(\n",
       "          (0): Conv2d(weights shape = torch.Size([128, 64, 3, 3]) stride = (2, 2) padding = (1, 1) )\n",
       "          (1): BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(weights shape = torch.Size([128, 128, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "          (4): BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1)\n",
       "        )\n",
       "        (right): Sequential(\n",
       "          (0): Conv2d(weights shape = torch.Size([128, 64, 1, 1]) stride = (2, 2) padding = (0, 0) )\n",
       "          (1): BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (otherRes): ModuleList(\n",
       "        (0): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([128, 128, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([128, 128, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([128, 128, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([128, 128, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([128, 128, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([128, 128, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): BlockGroup(\n",
       "      (firstRes): ResidualBlock(\n",
       "        (left): Sequential(\n",
       "          (0): Conv2d(weights shape = torch.Size([256, 128, 3, 3]) stride = (2, 2) padding = (1, 1) )\n",
       "          (1): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "          (4): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "        )\n",
       "        (right): Sequential(\n",
       "          (0): Conv2d(weights shape = torch.Size([256, 128, 1, 1]) stride = (2, 2) padding = (0, 0) )\n",
       "          (1): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (otherRes): ModuleList(\n",
       "        (0): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (3): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (4): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([256, 256, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): BlockGroup(\n",
       "      (firstRes): ResidualBlock(\n",
       "        (left): Sequential(\n",
       "          (0): Conv2d(weights shape = torch.Size([512, 256, 3, 3]) stride = (2, 2) padding = (1, 1) )\n",
       "          (1): BatchNorm2d(num_features=512, eps=1e-05, momentum=0.1)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(weights shape = torch.Size([512, 512, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "          (4): BatchNorm2d(num_features=512, eps=1e-05, momentum=0.1)\n",
       "        )\n",
       "        (right): Sequential(\n",
       "          (0): Conv2d(weights shape = torch.Size([512, 256, 1, 1]) stride = (2, 2) padding = (0, 0) )\n",
       "          (1): BatchNorm2d(num_features=512, eps=1e-05, momentum=0.1)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (otherRes): ModuleList(\n",
       "        (0): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([512, 512, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=512, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([512, 512, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=512, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (left): Sequential(\n",
       "            (0): Conv2d(weights shape = torch.Size([512, 512, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (1): BatchNorm2d(num_features=512, eps=1e-05, momentum=0.1)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(weights shape = torch.Size([512, 512, 3, 3]) stride = (1, 1) padding = (1, 1) )\n",
       "            (4): BatchNorm2d(num_features=512, eps=1e-05, momentum=0.1)\n",
       "          )\n",
       "          (right): Sequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (postBlockGroups): Sequential(\n",
       "    (0): AveragePool()\n",
       "    (1): Flatten(start_dim = 1 end_dim = -1)\n",
       "    (2): Linear(weight.shape = torch.Size([1000, 512]) bias.shape = torch.Size([1000]))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "day 6 begins here\n",
    "\n",
    "I'll admit. at a first passthrough, I'm basically copying much of the tutorial code after writing out some pseudocode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resenet\"\n",
    "data_dir = Path(\"./hymenoptera_data\")\n",
    "num_classes = 2\n",
    "batch_size = 8\n",
    "num_epochs = 6\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functioin to do train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import sin\n",
    "import time\n",
    "import copy\n",
    "def train_model(model, dataloaders, criterion, optimizer, nun_epochs=25, is_inception = False):\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # for each epoch, go through train and validation\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0 #TODO: may need to be int, and add .double() later\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with t.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs) #do a forward pass\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = t.max(outputs, 1)\n",
    "                    optimizer.step()\n",
    "            \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += t.sum(preds == labels.data)\n",
    "        \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained = True):\n",
    "    my_model = ResNet34()\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        pure_model_ft = models.resnet34(pretrained = use_pretrained)\n",
    "        copy_weights(my_model, pure_model_ft)\n",
    "\n",
    "        set_parameter_requires_grad(my_model, feature_extract)\n",
    "        \n",
    "        num_ftrs = my_model.postBlockGroups[2].weight.shape[1]\n",
    "        my_model.postBlockGroups[2] = fromYesterday.Linear(num_ftrs, num_classes,True)\n",
    "        input_size = 224 # yo? hello? the tutorial just threw it down but idk where it came from\n",
    "    else:\n",
    "        print('yeah i got nothing for you')\n",
    "\n",
    "    return my_model, input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(weight.shape = torch.Size([1000, 512]) bias.shape = torch.Size([1000]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WE NEED TO MAKE SURE THIS MATCHES\n",
    "myresnet.postBlockGroups[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/codyrushing/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/codyrushing/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "my_model, input_size = initialize_model(\"resnet\", 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): AveragePool()\n",
       "  (1): Flatten(start_dim = 1 end_dim = -1)\n",
       "  (2): Linear(weight.shape = torch.Size([2, 512]) bias.shape = torch.Size([2]))\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK WITH ABOVE TO MAKE SURE IT IS THE SAME!\n",
    "my_model.postBlockGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# create train and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "data_transforms[x]) for x in ['train', 'val']}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: t.utils.data.DataLoader(image_datasets[x],\n",
    "batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x1657ed970>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x1657edd60>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = my_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t postBlockGroups.2.weight\n",
      "\t postBlockGroups.2.bias\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "params_to_update = my_model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    print(\"THIS IS NOT FINISHED! I AM NOT FEATURE EXTRACTING\")\n",
    "else:\n",
    "    for name, param in my_model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\", name)\n",
    "\n",
    "optimizer_ft = t.optim.Adam(params_to_update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#model_ft, hist = train_model(my_model, dataloaders_dict, criterion, optimizer_ft, num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random debugging stuff\n",
    "Cheating, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import PIL\n",
    "\n",
    "epochs = 3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "\n",
    "MODEL_FILENAME = \"./w1d2_convnet_mnist.pt\"\n",
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "trainset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "def train_convnet(trainloader: DataLoader, testloader: DataLoader, epochs: int, loss_fn) -> list:\n",
    "    \"\"\"\n",
    "    Defines a ResNet using our previous code, and trains it on the data in trainloader.\n",
    "    \n",
    "    Returns tuple of (loss_list, accuracy_list), where accuracy_list contains the fraction of accurate classifications on the test set, at the end of each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    my_model, input_size = initialize_model(\"resnet\", 10, True)\n",
    "    model = my_model.to(device).train()\n",
    "\n",
    "    model = resnet34().to(device).train()\n",
    "    optimizer = t.optim.Adam(model.parameters())\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        \n",
    "        for (x, y) in tqdm_notebook(trainloader, leave=False):\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "        \n",
    "        with t.inference_mode():\n",
    "            \n",
    "            accuracy = 0\n",
    "            total = 0\n",
    "            \n",
    "            for (x, y) in testloader:\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_hat = model(x)\n",
    "                y_predictions = y_hat.argmax(1)\n",
    "                accuracy += (y_predictions == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "            accuracy_list.append(accuracy/total)\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, train loss is {loss:.6f}, accuracy is {accuracy}/{total}\")\n",
    "    \n",
    "    print(f\"Saving model to: {MODEL_FILENAME}\")\n",
    "    t.save(model, MODEL_FILENAME)\n",
    "    return loss_list, accuracy_list\n",
    "\n",
    "loss_list, accuracy_list = train_convnet(trainloader, testloader, epochs, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ARENAenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
