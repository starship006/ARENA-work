{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import w0d3Remnant as fromYesterday\n",
    "import torch as t\n",
    "from torch import nn as nn\n",
    "import utils\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a combination of my day four *and* five work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day Four - Spent two hours, eight minutes on this so far. Going to revisit it later, cannot focus on it\n",
    "Day Five - Spent three hours and thirty three minutes. Finished it. LETS GOOOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization in CNNs\n",
    "\n",
    "Batch Norm is a method to address overfitting and slow training issues in Deep Neural Networks.\n",
    "\n",
    "Normalization itself is just a pre-processing technique used to standardize data. This is often done to make features balanced - if they have differences in ranges, different features could recieve inflated importances. Normalizing your features helps models learn better.\n",
    "\n",
    "You can instead normalize batches of data inside the network itself, done between layers. Each neuron will first apply its weights (not biases!): $$z = g(w,x);$$\n",
    "\n",
    "But before applying the activation function, a batch norm is applied to each neurons output across the layer.\n",
    "\n",
    "$$z^N = (\\frac{z - m_z}{s_z}) \\cdot \\gamma + \\beta$$\n",
    "\n",
    "where $z^N$ is the output of the Batch Norm, $m_z$ is the mean of the neuron's output, $s_z$ is the standard deviation of the output of the neurons, and $\\gamma$ and $\\beta$ are learnable parameters which eventually represent the standard deviation and the mean of the outputs, respectively.\n",
    "\n",
    "The output of batch norm is then fed into the activation function\n",
    "\n",
    "$$a = f(z^N)$$\n",
    "\n",
    "Why does this work? Firstly, think of it intuitively as the same thing you are doing to the features. But secondly, it reduces the \"internal covariate shift of the network\" - it reduces the shift due to a change in data distribution. Here, it the data recieved from the previous layer, which is constantly changing.\n",
    "\n",
    "Batch norm also has a regularization effect, speeds up computation!\n",
    "\n",
    "When applying this to convolutional layers, each feature map ends up having a single mean and standard deviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(weights shape = torch.Size([10, 3, 3, 3]) stride = (1, 1) padding = (0, 0) )\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Sequential(nn.Module):\n",
    "    def __init__(self, *modules: nn.Module):\n",
    "        super().__init__()\n",
    "        for i, mod in enumerate(modules):\n",
    "            self.add_module(str(i), mod)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Chain each module together, with the output from one feeding into the next one.'''\n",
    "        for mod in self._modules.values():\n",
    "            x = mod(x)\n",
    "        return x\n",
    "\n",
    "model = Sequential(fromYesterday.Conv2d(3,10,3,1,0),\n",
    "fromYesterday.ReLU(),\n",
    "nn.Linear(10,5))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_batchnorm2d_module` passed!\n",
      "All tests in `test_batchnorm2d_forward` passed!\n",
      "All tests in `test_batchnorm2d_running_mean` passed!\n"
     ]
    }
   ],
   "source": [
    "from math import gamma\n",
    "from operator import truediv\n",
    "import einops\n",
    "from tkinter.tix import MAIN\n",
    "\n",
    "\n",
    "class BatchNorm2d(nn.Module):\n",
    "    running_mean: t.Tensor         # shape: (num_features,)\n",
    "    running_var: t.Tensor          # shape: (num_features,)\n",
    "    num_batches_tracked: t.Tensor  # shape: ()\n",
    "\n",
    "    def __init__(self, num_features: int, eps=1e-05, momentum=0.1):\n",
    "        '''Like nn.BatchNorm2d with track_running_stats=True and affine=True.\n",
    "\n",
    "        Name the learnable affine parameters `weight` and `bias` in that order.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #print(\"num features is \" + str(num_features))\n",
    "        running_mean = t.zeros(num_features)\n",
    "        running_var = t.ones(num_features)\n",
    "        num_batches_tracked = t.tensor(0)\n",
    "        \n",
    "        self.weight = nn.Parameter(t.ones(num_features)) # tracks gamma\n",
    "        self.bias = nn.Parameter(t.zeros(num_features)) # tracks beta\n",
    "        self.register_buffer('running_mean', running_mean) # holds mean across channels\n",
    "        self.register_buffer('running_var', running_var) # holds variance across channels\n",
    "        self.register_buffer('num_batches_tracked', num_batches_tracked) # stores num batches tracked\n",
    "        self.num_features = num_features\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Normalize each channel.\n",
    "\n",
    "        Compute the variance using `torch.var(x, unbiased=False)`\n",
    "        Hint: you may also find it helpful to use the argument `keepdim`.\n",
    "\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels, height, width)\n",
    "        '''\n",
    "        if self.training:\n",
    "            # update running mean\n",
    "            mean = t.mean(x, (0,2,3), keepdim = False)\n",
    "            self.running_mean = (1 - self.momentum) * mean + (self.momentum) * self.running_mean\n",
    "            # update running variance\n",
    "            var = t.var(x, (0,2,3), keepdim = False, unbiased=False)\n",
    "            self.running_var = (1 - self.momentum) * var + (self.momentum) * self.running_var\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        mean = einops.rearrange(mean, 'c -> 1 c 1 1')\n",
    "        var = einops.rearrange(var, 'c -> 1 c 1 1')\n",
    "        gamma = einops.rearrange(self.weight, 'c -> 1 c 1 1')\n",
    "        beta = einops.rearrange(self.bias, 'c -> 1 c 1 1')\n",
    "\n",
    "        returnX = ((x - mean) / (t.sqrt(var + self.eps))) * gamma + beta\n",
    "        return returnX\n",
    "\n",
    "       \n",
    "    # credit to callum for this\n",
    "    def extra_repr(self) -> str:\n",
    "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in [\"num_features\", \"eps\", \"momentum\"]])\n",
    "\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_batchnorm2d_module(BatchNorm2d)\n",
    "    utils.test_batchnorm2d_forward(BatchNorm2d)\n",
    "    utils.test_batchnorm2d_running_mean(BatchNorm2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watching a paper review of ResNet\n",
    "\n",
    "A key thing to realize is that when they were contemplating why accuracy wasn't increasing with model depth, the issue was not overfitting!\n",
    "\n",
    "Each chunk of the model only has to learn the difference in how they wish for the data to change, rather than also having to learn an identiy function, too. Regluarization pushes against identity since it pushes everything down to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels)\n",
    "        '''\n",
    "\n",
    "        return t.mean(x,(2,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_stride=1):\n",
    "        '''A single residual block with optional downsampling.\n",
    "\n",
    "        For compatibility with the pretrained model, declare the left side branch first using a `Sequential`.\n",
    "\n",
    "        If first_stride is > 1, this means the optional (conv + bn) should be present on the right branch. Declare it second using another `Sequential`.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        left_branch = nn.Sequential(\n",
    "            fromYesterday.Conv2d(in_feats, out_feats, 3, first_stride, 1), # TODO: why is padding 1? not zero?\n",
    "            BatchNorm2d(out_feats),\n",
    "            fromYesterday.ReLU(),\n",
    "            fromYesterday.Conv2d(out_feats, out_feats, 3, 1, 1),\n",
    "            BatchNorm2d(out_feats)\n",
    "        )\n",
    "\n",
    "        if first_stride > 1:\n",
    "            right_branch = nn.Sequential(\n",
    "                fromYesterday.Conv2d(in_feats, out_feats, 1, first_stride, 0),\n",
    "                BatchNorm2d(out_feats)\n",
    "            )\n",
    "        else:\n",
    "            right_branch = nn.Sequential(\n",
    "                nn.Identity()\n",
    "            )\n",
    "\n",
    "        self.left = left_branch\n",
    "        self.right = right_branch\n",
    "        self.relu = fromYesterday.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        '''\n",
    "        leftOutput = self.left.forward(x)\n",
    "        rightOutput = self.right.forward(x)\n",
    "        return self.relu.forward(leftOutput + rightOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockGroup(nn.Module):\n",
    "    def __init__(self, n_blocks: int, in_feats: int, out_feats: int, first_stride=1):\n",
    "        '''An n_blocks-long sequence of ResidualBlock where only the first block uses the provided stride.'''\n",
    "        super().__init__()\n",
    "        self.firstRes = ResidualBlock(in_feats,out_feats,first_stride)\n",
    "        self.otherRes = nn.ModuleList([ResidualBlock(out_feats, out_feats,1) for i in range(n_blocks - 1)])\n",
    "        # TODO: convert back to normal list if this doesn't work!\n",
    "        \n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Compute the forward pass.\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        '''\n",
    "        result = self.firstRes.forward(x)\n",
    "        for l in self.otherRes:\n",
    "            result = l(result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pwd\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks_per_group=[3, 4, 6, 3],\n",
    "        out_features_per_group=[64, 128, 256, 512],\n",
    "        first_strides_per_group=[1, 2, 2, 2],\n",
    "        n_classes=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        BlockGroups = []\n",
    "        for i, j in enumerate(n_blocks_per_group):\n",
    "            n_blocks = j\n",
    "            if i > 1:\n",
    "                in_feats = out_features_per_group[i-1]\n",
    "            else:\n",
    "                in_feats = 64\n",
    "            out_feats = out_features_per_group[i]\n",
    "            first_stride = first_strides_per_group[i]\n",
    "            BlockGroups.append(BlockGroup(n_blocks, in_feats, out_feats, first_stride))\n",
    "\n",
    "\n",
    "        self.preBlockGroups = nn.Sequential(\n",
    "            fromYesterday.Conv2d(3,64,7,2,3),\n",
    "            BatchNorm2d(64),\n",
    "            fromYesterday.ReLU(),\n",
    "            fromYesterday.MaxPool2d(3,2),\n",
    "        )\n",
    "        self.layers = nn.Sequential(\n",
    "            *BlockGroups\n",
    "        )\n",
    "\n",
    "        self.postBlockGroups = nn.Sequential(\n",
    "            AveragePool(),\n",
    "            fromYesterday.Flatten(),\n",
    "            fromYesterday.Linear(out_features_per_group[-1], 1000)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, channels, height, width)\n",
    "\n",
    "        Return: shape (batch, n_classes)\n",
    "        '''\n",
    "        print(\"HERE\")\n",
    "        y = self.preBlockGroups(x)\n",
    "        print(\"went through pre block groups\")\n",
    "        y = self.layers(y)\n",
    "        print(\"went through post block groups\")\n",
    "        y = self.postBlockGroups(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models import resnet34\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(myresnet: ResNet34, pretrained_resnet: torchvision.models.resnet.ResNet) -> ResNet34:\n",
    "    '''Copy over the weights of `pretrained_resnet` to your resnet.'''\n",
    "\n",
    "    mydict = myresnet.state_dict()\n",
    "    pretraineddict = pretrained_resnet.state_dict()\n",
    "\n",
    "    # Check the number of params/buffers is correct\n",
    "    assert len(mydict) == len(pretraineddict), \"Number of layers is wrong. Have you done the prev step correctly?\"\n",
    "\n",
    "    # Initialise an empty dictionary to store the correct key-value pairs\n",
    "    state_dict_to_load = {}\n",
    "\n",
    "    for (mykey, myvalue), (pretrainedkey, pretrainedvalue) in zip(mydict.items(), pretraineddict.items()):\n",
    "        state_dict_to_load[mykey] = pretrainedvalue\n",
    "\n",
    "    myresnet.load_state_dict(state_dict_to_load)\n",
    "\n",
    "    return myresnet\n",
    "\n",
    "#utils.compare_my_resnet_to_pytorch(ResNet34())\n",
    "officialresnet = resnet34()\n",
    "myresnet = copy_weights(ResNet34(), officialresnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image \n",
    "import torch as t\n",
    "\n",
    "IMAGE_FILENAMES = [\n",
    "    \"chimpanzee.jpg\",\n",
    "    \"golden_retriever.jpg\",\n",
    "    \"platypus.jpg\",\n",
    "    \"frogs.jpg\",\n",
    "    \"fireworks.jpg\",\n",
    "    \"astronaut.jpg\",\n",
    "    \"iguana.jpg\",\n",
    "    \"volcano.jpg\",\n",
    "    \"goofy.jpg\",\n",
    "    \"dragonfly.jpg\",\n",
    "]\n",
    "\n",
    "IMAGE_FOLDER = Path(\"./resnet_inputs\")\n",
    "\n",
    "images = [Image.open(IMAGE_FOLDER / filename) for filename in IMAGE_FILENAMES]\n",
    "# ImageNet transforms copied from solution:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def prepare_data(images: list[Image.Image]) -> t.Tensor:\n",
    "    '''\n",
    "    Return: shape (batch=len(images), num_channels=3, height=224, width=224)\n",
    "    '''\n",
    "    return t.stack([transform(i) for i in images], dim = 0)\n",
    "\n",
    "prepared_images = prepare_data(images)\n",
    "\n",
    "def predict(model, images):\n",
    "    logits = model(images)\n",
    "    return logits.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224, 224])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "<built-in method size of Tensor object at 0x35fefec70>\n",
      "(10, 3, 112, 112, 7, 7)\n",
      "went through pre block groups\n",
      "<built-in method size of Tensor object at 0x2d5eda6d0>\n",
      "(10, 64, 56, 56, 3, 3)\n",
      "<built-in method size of Tensor object at 0x35feb2cc0>\n",
      "(10, 64, 56, 56, 3, 3)\n",
      "<built-in method size of Tensor object at 0x35feb2cc0>\n",
      "(10, 64, 56, 56, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d5eda180>\n",
      "(10, 64, 56, 56, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d612d180>\n",
      "(10, 64, 56, 56, 3, 3)\n",
      "<built-in method size of Tensor object at 0x35fefe8b0>\n",
      "(10, 64, 56, 56, 3, 3)\n",
      "<built-in method size of Tensor object at 0x35fefe1d0>\n",
      "(10, 64, 28, 28, 3, 3)\n",
      "<built-in method size of Tensor object at 0x35fefe680>\n",
      "(10, 128, 28, 28, 3, 3)\n",
      "<built-in method size of Tensor object at 0x35fefe1d0>\n",
      "(10, 64, 28, 28, 1, 1)\n",
      "<built-in method size of Tensor object at 0x35fefeea0>\n",
      "(10, 128, 28, 28, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d612dd10>\n",
      "(10, 128, 28, 28, 3, 3)\n",
      "<built-in method size of Tensor object at 0x35ffa7450>\n",
      "(10, 128, 28, 28, 3, 3)\n",
      "<built-in method size of Tensor object at 0x35ffa7810>\n",
      "(10, 128, 28, 28, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d5fd3630>\n",
      "(10, 128, 28, 28, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d5fd3d60>\n",
      "(10, 128, 28, 28, 3, 3)\n",
      "<built-in method size of Tensor object at 0x307e3df40>\n",
      "(10, 128, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x3073d87c0>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x307e3df40>\n",
      "(10, 128, 14, 14, 1, 1)\n",
      "<built-in method size of Tensor object at 0x35fe9c9a0>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d615a2c0>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d615ae00>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d615ab80>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d615a1d0>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x3073ca860>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d6158f40>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d6158c20>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d6158e00>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d6158180>\n",
      "(10, 256, 14, 14, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d6158f40>\n",
      "(10, 256, 7, 7, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d6158f90>\n",
      "(10, 512, 7, 7, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d6158f40>\n",
      "(10, 256, 7, 7, 1, 1)\n",
      "<built-in method size of Tensor object at 0x2d6158040>\n",
      "(10, 512, 7, 7, 3, 3)\n",
      "<built-in method size of Tensor object at 0x2d5eda220>\n",
      "(10, 512, 7, 7, 3, 3)\n",
      "<built-in method size of Tensor object at 0x347c1df90>\n",
      "(10, 512, 7, 7, 3, 3)\n",
      "<built-in method size of Tensor object at 0x347c1df40>\n",
      "(10, 512, 7, 7, 3, 3)\n",
      "went through post block groups\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([714, 714, 714, 714, 714, 714, 352, 714, 352, 714])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(myresnet, prepared_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"imagenet_labels.json\") as f:\n",
    "    imagenet_labels = list(json.load(f).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([714, 714, 714, 714, 714, 714, 352, 714, 352, 714])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(officialresnet, prepared_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prepared_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ARENAenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
