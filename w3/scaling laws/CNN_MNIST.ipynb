{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starship006/ARENA-work/blob/main/w3/scaling%20laws/CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4e_IJ7QM4XP"
      },
      "source": [
        "# \"Write a script to train a small CNN on MNIST, or find one you have written previously.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cjhFBmcM4XQ"
      },
      "source": [
        "Day 1 turned out to be more of a \"relearn how to actually make a CNN, and then implement it out\" type of day; I got a working CNN predicting on MNIST on 98% accuracy\n",
        "\n",
        "Day 2, I finally implemented sweeping on the CNN!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "X4hGBiL_Snlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "SyxndoS5M4XQ"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import wandb\n",
        "\n",
        "device = \"cuda\" if t.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "fjUP7d6yM4XR"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self, out_one = 6, out_two = 18, kernel_size = 5, lin_out_one = 120, lin_out_two = 60):\n",
        "        super().__init__()\n",
        "        \n",
        "\n",
        "        lin_out_three = 10\n",
        "        self.conv1 = nn.Conv2d(1, out_one, kernel_size)\n",
        "        self.conv2 = nn.Conv2d(out_one, out_two, kernel_size)\n",
        "\n",
        "        final_dim = (((28 - kernel_size + 1) / 2) - kernel_size + 1) / 2 # two comes from max pooling\n",
        "        final_dim = int(final_dim) # convert float to int\n",
        "        self.lin1 = nn.Linear(out_two * final_dim * final_dim, lin_out_one) \n",
        "        self.lin2 = nn.Linear(lin_out_one, lin_out_two)\n",
        "        self.lin3 = nn.Linear(lin_out_two, lin_out_three)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply convolutions\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "        # flatten everything except the batch\n",
        "        x = t.flatten(x, 1) \n",
        "        # apply linear layers\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "fk4hNhdGM4XR"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = datasets.MNIST(root = './data', train=True, download=True, transform=transforms.ToTensor()) # has [28,28] sized images\n",
        "mnist_testset = datasets.MNIST(root = './data', train=False, download=True, transform=transforms.ToTensor()) # has [28,28] sized images\n",
        "\n",
        "train_dl = DataLoader(mnist_trainset, batch_size=64, shuffle=True) \n",
        "test_dl = DataLoader(mnist_testset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "HgEnKe3UM4XR"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dl, test_dl, criterion, optimizer, num_epochs: int, val_history):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # train\n",
        "        for phase in ['train', 'val']:\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "            current_dl = None\n",
        "\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "                current_dl = train_dl\n",
        "            else:\n",
        "                model.eval()\n",
        "                current_dl = test_dl\n",
        "            \n",
        "\n",
        "            for inputs, labels in current_dl:\n",
        "                inputs = inputs.float().to(device)\n",
        "                labels = labels.float().to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with t.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    orig_labels = labels\n",
        "                    labels = labels.to(t.int64)\n",
        "                    labels = F.one_hot(labels, 10).to(t.float64)\n",
        "\n",
        "\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    preds = t.argmax(outputs,dim=-1)\n",
        "\n",
        "                    if (phase == 'train'):\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += t.sum(preds == orig_labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(current_dl.dataset) # TODO: divide by something else, right????\n",
        "            epoch_acc = running_corrects / len(current_dl.dataset)\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if phase == 'val':\n",
        "                val_history.append(epoch_acc)\n",
        "        print() \n",
        "    return model, val_history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_wandb():\n",
        "    wandb.init()\n",
        "    # constant\n",
        "    num_epochs = 3\n",
        "\n",
        "    # wandb parameters\n",
        "    lr = wandb.config.lr\n",
        "\n",
        "    # set-up\n",
        "    model = ConvNet().to(device)\n",
        "    optim = t.optim.SGD(model.parameters(), lr)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    examples_seen = 0\n",
        "    wandb.watch(model, criterion = criterion, log=\"all\", log_freq = 10, log_graph = True)\n",
        "\n",
        "    mid_log_count = 60 # how many training steps to take before looking at test loss\n",
        "    count = 0\n",
        "    # train!\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0.0\n",
        "        model.train()\n",
        "        current_dl = train_dl\n",
        "        for inputs, labels in current_dl:\n",
        "            inputs = inputs.float().to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            optim.zero_grad()\n",
        "            with t.set_grad_enabled(True):\n",
        "                outputs = model(inputs)\n",
        "                orig_labels = labels\n",
        "                labels = labels.to(t.int64)\n",
        "                labels = F.one_hot(labels, 10).to(t.float64)\n",
        "\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                preds = t.argmax(outputs,dim=-1)\n",
        "\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                examples_seen += len(inputs)\n",
        "                # logging\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += t.sum(preds == orig_labels.data)\n",
        "                wandb.log({\"train_loss\" : loss}, step = examples_seen)\n",
        "            \n",
        "            count += 1          \n",
        "            if count == mid_log_count:\n",
        "              test_acc = find_test_acc(model, criterion)\n",
        "              wandb.log({\"test_accuracy\" : test_acc}, step = examples_seen)\n",
        "              model.train()\n",
        "              count = 0\n",
        "\n",
        "\n",
        "\n",
        "    filename = f\"{wandb.run.dir}/model_state_dict.pt\"\n",
        "    print(f\"Saving model to: {filename}\")\n",
        "    t.save(model.state_dict(), filename)\n",
        "    wandb.save(filename)"
      ],
      "metadata": {
        "id": "RnsX8OIMRWnJ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_test_acc(model, criterion):\n",
        "    running_corrects = 0.0\n",
        "    model.eval()\n",
        "    current_dl = test_dl\n",
        "    for inputs, labels in current_dl:\n",
        "        inputs = inputs.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        with t.set_grad_enabled(False): # don't think this is necessary, but just in case\n",
        "            outputs = model(inputs)\n",
        "            orig_labels = labels\n",
        "            labels = labels.to(t.int64)\n",
        "            labels = F.one_hot(labels, 10).to(t.float64)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = t.argmax(outputs,dim=-1)\n",
        "            running_corrects += t.sum(preds == orig_labels.data)\n",
        "    epoch_acc = running_corrects / len(current_dl.dataset)\n",
        "    return epoch_acc     "
      ],
      "metadata": {
        "id": "iIUYqwBxYIS9"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "aT4H7BdcM4XS"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name': 'CNN_lr_sweep_0',\n",
        "    'metric': {'name' : 'test_accuracy', 'goal' : 'maximize'},\n",
        "    'parameters':\n",
        "    {\n",
        "        'lr': {'max': 0.1, 'min': 0.0001, 'distribution': 'log_uniform_values'}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep = sweep_config, project='CNN_LR')\n",
        "wandb.agent(sweep_id = sweep_id, function = train_with_wandb, count = 2)"
      ],
      "metadata": {
        "id": "fDC2d5T3Uzs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('ARENAenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}