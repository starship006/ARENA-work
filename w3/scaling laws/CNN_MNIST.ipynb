{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starship006/ARENA-work/blob/main/w3/scaling%20laws/CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4e_IJ7QM4XP"
      },
      "source": [
        "# \"Write a script to train a small CNN on MNIST, or find one you have written previously.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cjhFBmcM4XQ"
      },
      "source": [
        "Day 1 turned out to be more of a \"relearn how to actually make a CNN, and then implement it out\" type of day; I got a working CNN predicting on MNIST on 98% accuracy\n",
        "\n",
        "Day 2, I finally implemented sweeping on the CNN! I'm not getting clean scaling results, primarily because the learning rate is playing an extrodinariy amount of important compared to the parameter count, and I'm not sure about how I should change the learning rate in relation to parameter count.\n",
        "\n",
        "Day 3, I restarted my hyperparameter searching; day 2 was kind of sloppy, and I didn't get any clear results. I'll start by finetuning for the best learning rate for \"scale 5\" CNNs. From there, I can go between different parameters counts easily - a change in scale by one doubles the parameter count, which roughly cooresponds to a change in the learning rate by $1/\\sqrt{2}$.\n",
        "\n",
        "For a scale of 5, the optimal lr turned out to be around 0.0009.\n",
        "This means that for a scale of 4, I can predict it to be around 0.0012 - let's see if this shows to be true! And for a scale of 3, I can predict for it to be around 0.0017. For 2, 0.0024. For 1, 0.0033."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "X4hGBiL_Snlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "SyxndoS5M4XQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555d683c-cd9e-4827-986d-ab2b9f62c19a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2d0fee2070>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import wandb\n",
        "\n",
        "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
        "t.manual_seed(1) # reproducability!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fjUP7d6yM4XR"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self, scale = 3, kernel_size = 5):\n",
        "        super().__init__()\n",
        "        \n",
        "        scale = (2 ** 0.5) ** scale\n",
        "        out_one = int(2 * scale)\n",
        "        out_two = int(6 * scale)\n",
        "        lin_out_one = int(40 * scale)\n",
        "        lin_out_two = int(20 * scale)\n",
        "\n",
        "\n",
        "\n",
        "        lin_out_three = 10\n",
        "        self.conv1 = nn.Conv2d(1, out_one, kernel_size)\n",
        "        self.conv2 = nn.Conv2d(out_one, out_two, kernel_size)\n",
        "\n",
        "        final_dim = (((28 - kernel_size + 1) / 2) - kernel_size + 1) / 2 # two comes from max pooling\n",
        "        final_dim = int(final_dim) # convert float to int\n",
        "        self.lin1 = nn.Linear(out_two * final_dim * final_dim, lin_out_one) \n",
        "        self.lin2 = nn.Linear(lin_out_one, lin_out_two)\n",
        "        self.lin3 = nn.Linear(lin_out_two, lin_out_three)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply convolutions\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "        # flatten everything except the batch\n",
        "        x = t.flatten(x, 1) \n",
        "        # apply linear layers\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "        return x\n",
        "\n",
        "    def getParamCount(self):\n",
        "        totalParamCount = 0\n",
        "        for parameter in self.parameters():\n",
        "          currentCount = 1\n",
        "          for i in parameter.shape:\n",
        "            currentCount = currentCount * i\n",
        "          totalParamCount += currentCount\n",
        "        return totalParamCount"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing to see how scale manipulates size; increasing scale linearlly increases num parameters quadratically\n",
        "for i in range(9):\n",
        "  print(\"num parameters with the following 'scale' of ConvNet():\" + str(i))\n",
        "  net = ConvNet(i)\n",
        "  print(net.getParamCount())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lIfVyUmJYnq",
        "outputId": "8dfd1d4e-9d5a-4a16-971b-27bb23c96fe9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num parameters with the following 'scale' of ConvNet():0\n",
            "5268\n",
            "num parameters with the following 'scale' of ConvNet():1\n",
            "9570\n",
            "num parameters with the following 'scale' of ConvNet():2\n",
            "20406\n",
            "num parameters with the following 'scale' of ConvNet():3\n",
            "38141\n",
            "num parameters with the following 'scale' of ConvNet():4\n",
            "80322\n",
            "num parameters with the following 'scale' of ConvNet():5\n",
            "155739\n",
            "num parameters with the following 'scale' of ConvNet():6\n",
            "318714\n",
            "num parameters with the following 'scale' of ConvNet():7\n",
            "627133\n",
            "num parameters with the following 'scale' of ConvNet():8\n",
            "1269738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fk4hNhdGM4XR"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = datasets.MNIST(root = './data', train=True, download=True, transform=transforms.ToTensor()) # has [28,28] sized images\n",
        "mnist_testset = datasets.MNIST(root = './data', train=False, download=True, transform=transforms.ToTensor()) # has [28,28] sized images\n",
        "\n",
        "train_dl = DataLoader(mnist_trainset, batch_size=64, shuffle=True) \n",
        "test_dl = DataLoader(mnist_testset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Apologies, the following training code is messy and hard to follow and could probably look better. For now, I'm lazy)"
      ],
      "metadata": {
        "id": "grpG4At7HQ11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_wandb():\n",
        "    wandb.init()\n",
        "    # constant\n",
        "    num_epochs = 1\n",
        "\n",
        "    # wandb parameters\n",
        "    lr = wandb.config.lr\n",
        "\n",
        "    # set-up\n",
        "    model = ConvNet(wandb.config.scale).to(device)\n",
        "    optim = t.optim.Adam(model.parameters(), lr)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    examples_seen = 0\n",
        "    wandb.watch(model, criterion = criterion, log=\"all\", log_freq = 10, log_graph = True)\n",
        "    # log param size\n",
        "    wandb.log({\"Parameter Size\" : model.getParamCount()}, step = examples_seen)\n",
        "\n",
        "    mid_log_count = 60 # how many training steps to take before looking at test loss\n",
        "    count = 0\n",
        "    # train!\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0.0\n",
        "        model.train()\n",
        "        current_dl = train_dl\n",
        "        for inputs, labels in current_dl:\n",
        "            inputs = inputs.float().to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            optim.zero_grad()\n",
        "            with t.set_grad_enabled(True):\n",
        "                outputs = model(inputs)\n",
        "                orig_labels = labels\n",
        "                labels = labels.to(t.int64)\n",
        "                labels = F.one_hot(labels, 10).to(t.float64)\n",
        "\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                preds = t.argmax(outputs,dim=-1)\n",
        "\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                examples_seen += len(inputs)\n",
        "                # logging\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += t.sum(preds == orig_labels.data)\n",
        "                wandb.log({\"Train Loss\" : loss}, step = examples_seen)\n",
        "            \n",
        "            count += 1          \n",
        "            if count == mid_log_count:\n",
        "              test_acc = find_test_acc(model, criterion)\n",
        "              wandb.log({\"Test Accuracy\" : test_acc}, step = examples_seen)\n",
        "              model.train()\n",
        "              count = 0\n",
        "\n",
        "\n",
        "\n",
        "    filename = f\"{wandb.run.dir}/model_state_dict.pt\"\n",
        "    print(f\"Saving model to: {filename}\")\n",
        "    t.save(model.state_dict(), filename)\n",
        "    wandb.save(filename)"
      ],
      "metadata": {
        "id": "RnsX8OIMRWnJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_test_acc(model, criterion):\n",
        "    running_corrects = 0.0\n",
        "    model.eval()\n",
        "    current_dl = test_dl\n",
        "    for inputs, labels in current_dl:\n",
        "        inputs = inputs.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        with t.set_grad_enabled(False): # don't think this is necessary, but just in case\n",
        "            outputs = model(inputs)\n",
        "            orig_labels = labels\n",
        "            labels = labels.to(t.int64)\n",
        "            labels = F.one_hot(labels, 10).to(t.float64)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = t.argmax(outputs,dim=-1)\n",
        "            running_corrects += t.sum(preds == orig_labels.data)\n",
        "    epoch_acc = running_corrects / len(current_dl.dataset)\n",
        "    return epoch_acc     "
      ],
      "metadata": {
        "id": "iIUYqwBxYIS9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "aT4H7BdcM4XS"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name': 'CNN_scale_sweep',\n",
        "    'metric': {'name' : 'Test Accuracy', 'goal' : 'maximize'},\n",
        "    'parameters':\n",
        "    {\n",
        "        'lr': {'max': 0.01, 'min': 0.0001, 'distribution': 'log_uniform_values'},\n",
        "        'scale': {'values': [1]} # +1 scale doubles parameter count\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = \"oufsr67l\" # wandb.sweep(sweep = sweep_config, project='Scaling_Laws_CNN')\n",
        "\n",
        "# sweep size - associated sweep_id\n",
        "# 1 - hcidgdal\n",
        "# 2 - 03tt0k9t\n",
        "# 3 - 9xwcgfwm\n",
        "# 4 - g9wdllp4\n",
        "# 5 - oufsr67l"
      ],
      "metadata": {
        "id": "bCt_47wsyPCp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the bottom isn't exactly what I did; I changed the random seed in the middle of the training for each sweep, too\n",
        "for i in [\"oufsr67l\", \"g9wdllp4\", \"9xwcgfwm\", \"03tt0k9t\", \"hcidgdal\"]: \n",
        "    sweep_id = i\n",
        "    wandb.agent(sweep_id = sweep_id, function = train_with_wandb, count = 12)"
      ],
      "metadata": {
        "id": "ZbMgSxX6_Zy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('ARENAenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}