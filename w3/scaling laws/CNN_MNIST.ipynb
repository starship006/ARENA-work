{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starship006/ARENA-work/blob/main/w3/scaling%20laws/CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4e_IJ7QM4XP"
      },
      "source": [
        "# \"Write a script to train a small CNN on MNIST, or find one you have written previously.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cjhFBmcM4XQ"
      },
      "source": [
        "Day 1 turned out to be more of a \"relearn how to actually make a CNN, and then implement it out\" type of day; I got a working CNN predicting on MNIST on 98% accuracy\n",
        "\n",
        "Day 2, I finally implemented sweeping on the CNN!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "X4hGBiL_Snlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ee02cf-ccf0-407c-d036-86dc12fc2dd4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.5)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.10)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "SyxndoS5M4XQ"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import wandb\n",
        "\n",
        "device = \"cuda\" if t.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "fjUP7d6yM4XR"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self, scale = 3, kernel_size = 5):\n",
        "        super().__init__()\n",
        "        \n",
        "        scale = (2 ** 0.5) ** scale\n",
        "        out_one = int(2 * scale)\n",
        "        out_two = int(6 * scale)\n",
        "        lin_out_one = int(40 * scale)\n",
        "        lin_out_two = int(20 * scale)\n",
        "\n",
        "\n",
        "\n",
        "        lin_out_three = 10\n",
        "        self.conv1 = nn.Conv2d(1, out_one, kernel_size)\n",
        "        self.conv2 = nn.Conv2d(out_one, out_two, kernel_size)\n",
        "\n",
        "        final_dim = (((28 - kernel_size + 1) / 2) - kernel_size + 1) / 2 # two comes from max pooling\n",
        "        final_dim = int(final_dim) # convert float to int\n",
        "        self.lin1 = nn.Linear(out_two * final_dim * final_dim, lin_out_one) \n",
        "        self.lin2 = nn.Linear(lin_out_one, lin_out_two)\n",
        "        self.lin3 = nn.Linear(lin_out_two, lin_out_three)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply convolutions\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "        # flatten everything except the batch\n",
        "        x = t.flatten(x, 1) \n",
        "        # apply linear layers\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "        return x\n",
        "\n",
        "    def getParamCount(self):\n",
        "        totalParamCount = 0\n",
        "        for parameter in self.parameters():\n",
        "          currentCount = 1\n",
        "          for i in parameter.shape:\n",
        "            currentCount = currentCount * i\n",
        "          totalParamCount += currentCount\n",
        "        return totalParamCount"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing to see how scale manipulates size; increasing scale linearlly increases num parameters quadratically\n",
        "for i in range(9):\n",
        "  print(\"num parameters with the following 'scale' of ConvNet():\" + str(i))\n",
        "  net = ConvNet(i)\n",
        "  print(net.getParamCount())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lIfVyUmJYnq",
        "outputId": "e6ea937f-ac1c-4a7a-ac5f-cfe3d707a00e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num parameters with the following 'scale' of ConvNet():0\n",
            "5268\n",
            "num parameters with the following 'scale' of ConvNet():1\n",
            "9570\n",
            "num parameters with the following 'scale' of ConvNet():2\n",
            "20406\n",
            "num parameters with the following 'scale' of ConvNet():3\n",
            "38141\n",
            "num parameters with the following 'scale' of ConvNet():4\n",
            "80322\n",
            "num parameters with the following 'scale' of ConvNet():5\n",
            "155739\n",
            "num parameters with the following 'scale' of ConvNet():6\n",
            "318714\n",
            "num parameters with the following 'scale' of ConvNet():7\n",
            "627133\n",
            "num parameters with the following 'scale' of ConvNet():8\n",
            "1269738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "fk4hNhdGM4XR"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = datasets.MNIST(root = './data', train=True, download=True, transform=transforms.ToTensor()) # has [28,28] sized images\n",
        "mnist_testset = datasets.MNIST(root = './data', train=False, download=True, transform=transforms.ToTensor()) # has [28,28] sized images\n",
        "\n",
        "train_dl = DataLoader(mnist_trainset, batch_size=64, shuffle=True) \n",
        "test_dl = DataLoader(mnist_testset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "HgEnKe3UM4XR"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dl, test_dl, criterion, optimizer, num_epochs: int, val_history):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # train\n",
        "        for phase in ['train', 'val']:\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "            current_dl = None\n",
        "\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "                current_dl = train_dl\n",
        "            else:\n",
        "                model.eval()\n",
        "                current_dl = test_dl\n",
        "            \n",
        "\n",
        "            for inputs, labels in current_dl:\n",
        "                inputs = inputs.float().to(device)\n",
        "                labels = labels.float().to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with t.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    orig_labels = labels\n",
        "                    labels = labels.to(t.int64)\n",
        "                    labels = F.one_hot(labels, 10).to(t.float64)\n",
        "\n",
        "\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    preds = t.argmax(outputs,dim=-1)\n",
        "\n",
        "                    if (phase == 'train'):\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += t.sum(preds == orig_labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(current_dl.dataset) # TODO: divide by something else, right????\n",
        "            epoch_acc = running_corrects / len(current_dl.dataset)\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if phase == 'val':\n",
        "                val_history.append(epoch_acc)\n",
        "        print() \n",
        "    return model, val_history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_wandb():\n",
        "    wandb.init()\n",
        "    # constant\n",
        "    num_epochs = 3\n",
        "\n",
        "    # wandb parameters\n",
        "    lr = wandb.config.lr\n",
        "\n",
        "    # set-up\n",
        "    model = ConvNet(wandb.config.scale).to(device)\n",
        "    optim = t.optim.Adam(model.parameters(), lr)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    examples_seen = 0\n",
        "    wandb.watch(model, criterion = criterion, log=\"all\", log_freq = 10, log_graph = True)\n",
        "    # log param size\n",
        "    wandb.log({\"Parameter Size\" : model.getParamCount()}, step = examples_seen)\n",
        "\n",
        "    mid_log_count = 60 # how many training steps to take before looking at test loss\n",
        "    count = 0\n",
        "    # train!\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0.0\n",
        "        model.train()\n",
        "        current_dl = train_dl\n",
        "        for inputs, labels in current_dl:\n",
        "            inputs = inputs.float().to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            optim.zero_grad()\n",
        "            with t.set_grad_enabled(True):\n",
        "                outputs = model(inputs)\n",
        "                orig_labels = labels\n",
        "                labels = labels.to(t.int64)\n",
        "                labels = F.one_hot(labels, 10).to(t.float64)\n",
        "\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                preds = t.argmax(outputs,dim=-1)\n",
        "\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                examples_seen += len(inputs)\n",
        "                # logging\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += t.sum(preds == orig_labels.data)\n",
        "                wandb.log({\"Train Loss\" : loss}, step = examples_seen)\n",
        "            \n",
        "            count += 1          \n",
        "            if count == mid_log_count:\n",
        "              test_acc = find_test_acc(model, criterion)\n",
        "              wandb.log({\"Test Accuracy\" : test_acc}, step = examples_seen)\n",
        "              model.train()\n",
        "              count = 0\n",
        "\n",
        "\n",
        "\n",
        "    filename = f\"{wandb.run.dir}/model_state_dict.pt\"\n",
        "    print(f\"Saving model to: {filename}\")\n",
        "    t.save(model.state_dict(), filename)\n",
        "    wandb.save(filename)"
      ],
      "metadata": {
        "id": "RnsX8OIMRWnJ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_test_acc(model, criterion):\n",
        "    running_corrects = 0.0\n",
        "    model.eval()\n",
        "    current_dl = test_dl\n",
        "    for inputs, labels in current_dl:\n",
        "        inputs = inputs.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        with t.set_grad_enabled(False): # don't think this is necessary, but just in case\n",
        "            outputs = model(inputs)\n",
        "            orig_labels = labels\n",
        "            labels = labels.to(t.int64)\n",
        "            labels = F.one_hot(labels, 10).to(t.float64)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = t.argmax(outputs,dim=-1)\n",
        "            running_corrects += t.sum(preds == orig_labels.data)\n",
        "    epoch_acc = running_corrects / len(current_dl.dataset)\n",
        "    return epoch_acc     "
      ],
      "metadata": {
        "id": "iIUYqwBxYIS9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "aT4H7BdcM4XS"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name': 'CNN_scale_sweep',\n",
        "    'metric': {'name' : 'test_accuracy', 'goal' : 'maximize'},\n",
        "    'parameters':\n",
        "    {\n",
        "        'lr': {'max': 0.001, 'min': 0.00001, 'distribution': 'log_uniform_values'},\n",
        "        'scale': {'values': [4, 5]} # +1 scale doubles parameter count\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep = sweep_config, project='CNN_SCALING')\n",
        "wandb.agent(sweep_id = sweep_id, function = train_with_wandb, count = 3)"
      ],
      "metadata": {
        "id": "fDC2d5T3Uzs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('ARENAenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}