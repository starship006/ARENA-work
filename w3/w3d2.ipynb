{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import utilsd2 as utils\n",
    "import typing\n",
    "from typing import Callable, Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser Investigations\n",
    "A series of various code snippets related to certain optimisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Groups\n",
    "Pytorch allows you to specify parameter groups, meaning that you can have different hyperparameters operating on different parameters.\n",
    "\n",
    "(Note: this exercise took me a lot longer than anticipated, and I had to look at the solution for some inspiration as well. Really sneaky bug where I forgot that generators exhaust everything. I get the high level idea of this, though; just that the code is not reflecting it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing configuration:  [{'params': 'base'}, {'params': 'classifier', 'lr': 0.001}]\n",
      "\n",
      "Testing configuration:  [{'params': 'base'}, {'params': 'classifier'}]\n",
      "\n",
      "Testing configuration:  [{'params': 'base', 'lr': 0.01, 'momentum': 0.95}, {'params': 'classifier', 'lr': 0.001}]\n",
      "\n",
      "Testing that your function doesn't allow duplicates (this should raise an error): \n",
      "Got an error, as expected.\n",
      "\n",
      "All tests in `test_sgd_param_groups` passed!\n"
     ]
    }
   ],
   "source": [
    "class SGD:\n",
    "\n",
    "    def __init__(self, params, **kwargs):\n",
    "        '''Implements SGD with momentum.\n",
    "\n",
    "        Accepts parameters in groups, or an iterable.\n",
    "\n",
    "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
    "        kwargs can contain lr, momentum or weight_decay\n",
    "        '''\n",
    "        self.param_groups = []\n",
    "        default_param_values = dict(momentum = 0., weight_decay = 0.)\n",
    "\n",
    "        checkParams = set()\n",
    "\n",
    "        for param_group in params:\n",
    "            param_group = {**default_param_values, **kwargs, **param_group}\n",
    "            param_group['params'] = list(param_group['params'])\n",
    "            param_group['bs'] = [t.zeros_like(p) for p in param_group['params']]\n",
    "            assert param_group['lr'] is not None, \"lr must be specified\"\n",
    "\n",
    "            self.param_groups.append(param_group)\n",
    "\n",
    "            for param in param_group['params']:\n",
    "                assert param not in checkParams, \"WHY?!\"\n",
    "                checkParams.add(param)\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "        self.t = 1\n",
    "\n",
    "        \n",
    "        # check if parameters appear in more than one group?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def step(self):\n",
    "\n",
    "        pg: dict\n",
    "        for i, pg in enumerate(self.param_groups):\n",
    "            params = pg.get('params')\n",
    "            assert params is not None, 'must have parameter'\n",
    "            lr = pg['lr']          \n",
    "            momentum = pg['momentum']\n",
    "            weight_decay = pg['weight_decay']\n",
    "                       \n",
    "            for j, param in enumerate(params):\n",
    "                g = param.grad\n",
    "                if weight_decay != 0:\n",
    "                    g = g + weight_decay * param\n",
    "                if momentum != 0:\n",
    "                    if self.t > 1:\n",
    "                        b = momentum * pg.get('bs')[j] + g\n",
    "                    else:\n",
    "                        b = g\n",
    "                    g = b\n",
    "                    pg.get('bs')[j] = b\n",
    "                param -= lr * g \n",
    "        self.t = self.t + 1\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for param_group in self.param_groups:\n",
    "            for param in param_group.get('params'):\n",
    "                param.grad = None\n",
    "\n",
    "utils.test_sgd_param_groups(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "I then proceeded to look at Deep Double Descent and the Lottery Ticket Hypothesis. Really cool stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ARENAenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
