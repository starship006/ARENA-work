{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union, List\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.envs.registration\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import utilsd3 as utils\n",
    "from solutionsd2 import *\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = int\n",
    "ActType = int\n",
    "\n",
    "class DiscreteEnviroGym(gym.Env):\n",
    "    action_space: gym.spaces.Discrete\n",
    "    observation_space: gym.spaces.Discrete\n",
    "\n",
    "    def __init__(self, env: Environment):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.observation_space = gym.spaces.Discrete(env.num_states)\n",
    "        self.action_space = gym.spaces.Discrete(env.num_actions)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        '''\n",
    "        Samples from the underlying dynamics of the environment\n",
    "        '''\n",
    "        (states, rewards, probs) = self.env.dynamics(self.pos, action)\n",
    "        idx = self.np_random.choice(len(states), p=probs)\n",
    "        (new_state, reward) = (states[idx], rewards[idx])\n",
    "        self.pos = new_state\n",
    "        done = self.pos in self.env.terminal\n",
    "        return (new_state, reward, done, {\"env\": self.env})\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, tuple[ObsType, dict]]:\n",
    "        super().reset(seed=seed)\n",
    "        self.pos = self.env.start\n",
    "        return (self.pos, {\"env\": self.env}) if return_info else self.pos\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        assert mode == \"human\", f\"Mode {mode} not supported!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.register(\n",
    "    id=\"NorvigGrid-v0\",\n",
    "    entry_point=DiscreteEnviroGym,\n",
    "    max_episode_steps=100,\n",
    "    nondeterministic=True,\n",
    "    kwargs={\"env\": Norvig(penalty=-0.04)},\n",
    ")\n",
    "\n",
    "gym.envs.registration.register(\n",
    "    id=\"ToyGym-v0\", \n",
    "    entry_point=DiscreteEnviroGym, \n",
    "    max_episode_steps=2, \n",
    "    nondeterministic=False, \n",
    "    kwargs={\"env\": Toy()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experience:\n",
    "    '''A class for storing one piece of experience during an episode run'''\n",
    "    obs: ObsType\n",
    "    act: ActType\n",
    "    reward: float\n",
    "    new_obs: ObsType\n",
    "    new_act: Optional[ActType] = None\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    '''Hyperparameters for agents'''\n",
    "    epsilon: float = 0.1\n",
    "    lr: float = 0.05\n",
    "    optimism: float = 0\n",
    "\n",
    "defaultConfig = AgentConfig()\n",
    "\n",
    "class Agent:\n",
    "    '''Base class for agents interacting with an environment (you do not need to add any implementation here)'''\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma: float = 0.99, seed: int = 0):\n",
    "        self.env = env\n",
    "        self.reset(seed)\n",
    "        self.config = config\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.num_states = env.observation_space.n\n",
    "        self.name = type(self).__name__\n",
    "\n",
    "    def get_action(self, obs: ObsType) -> ActType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def observe(self, exp: Experience) -> None:\n",
    "        '''\n",
    "        Agent observes experience, and updates model as appropriate.\n",
    "        Implementation depends on type of agent.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def run_episode(self, seed) -> List[int]:\n",
    "        '''\n",
    "        Simulates one episode of interaction, agent learns as appropriate\n",
    "        Inputs:\n",
    "            seed : Seed for the random number generator\n",
    "        Outputs:\n",
    "            The rewards obtained during the episode\n",
    "        '''\n",
    "        rewards = []\n",
    "        obs = self.env.reset(seed=seed)\n",
    "        self.reset(seed=seed)\n",
    "        done = False\n",
    "        while not done:\n",
    "            act = self.get_action(obs)\n",
    "            (new_obs, reward, done, info) = self.env.step(act)\n",
    "            exp = Experience(obs, act, reward, new_obs)\n",
    "            self.observe(exp)\n",
    "            rewards.append(reward)\n",
    "            obs = new_obs\n",
    "        return rewards\n",
    "\n",
    "    def train(self, n_runs=500):\n",
    "        '''\n",
    "        Run a batch of episodes, and return the total reward obtained per episode\n",
    "        Inputs:\n",
    "            n_runs : The number of episodes to simulate\n",
    "        Outputs:\n",
    "            The discounted sum of rewards obtained for each episode\n",
    "        '''\n",
    "        all_rewards = []\n",
    "        for seed in trange(n_runs):\n",
    "            rewards = self.run_episode(seed)\n",
    "            all_rewards.append(utils.sum_rewards(rewards, self.gamma))\n",
    "        return all_rewards\n",
    "\n",
    "class Random(Agent):\n",
    "    def get_action(self, obs: ObsType) -> ActType:\n",
    "        return self.rng.integers(0, self.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ARENAenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
