{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular RL and Policy Improvement\n",
    "\n",
    "A signfiicant amount of time was spent just trying to understand state-value functions, and how policy improvement works. I think I have the basic intuitions down as a result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gettext import find\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.envs.registration\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "import utilsd2 as utils\n",
    "import fancy_einsum as einsum\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "Arr = np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, num_states: int, num_actions: int, start=0, terminal=None):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.start = start\n",
    "        self.terminal = np.array([], dtype=int) if terminal is None else terminal\n",
    "        (self.T, self.R) = self.build()\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Constructs the T and R tensors from the dynamics of the environment.\n",
    "        Outputs:\n",
    "            T : (num_states, num_actions, num_states) State transition probabilities\n",
    "            R : (num_states, num_actions, num_states) Reward function\n",
    "        '''\n",
    "        num_states = self.num_states\n",
    "        num_actions = self.num_actions\n",
    "        T = np.zeros((num_states, num_actions, num_states))\n",
    "        R = np.zeros((num_states, num_actions, num_states))\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                (states, rewards, probs) = self.dynamics(s, a)\n",
    "                (all_s, all_r, all_p) = self.out_pad(states, rewards, probs)\n",
    "                T[s, a, all_s] = all_p\n",
    "                R[s, a, all_s] = all_r\n",
    "        return (T, R)\n",
    "\n",
    "    def dynamics(self, state: int, action: int) -> tuple[Arr, Arr, Arr]:\n",
    "        '''\n",
    "        Computes the distribution over possible outcomes for a given state\n",
    "        and action.\n",
    "        Inputs:\n",
    "            state : int (index of state)\n",
    "            action : int (index of action)\n",
    "        Outputs:\n",
    "            states  : (m,) all the possible next states\n",
    "            rewards : (m,) rewards for each next state transition\n",
    "            probs   : (m,) likelihood of each state-reward pair\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def render(pi: Arr):\n",
    "        '''\n",
    "        Takes a policy pi, and draws an image of the behavior of that policy, if applicable.\n",
    "        Inputs:\n",
    "            pi : (num_actions,) a policy\n",
    "        Outputs:\n",
    "            None\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def out_pad(self, states: Arr, rewards: Arr, probs: Arr):\n",
    "        '''\n",
    "        Inputs:\n",
    "            states  : (m,) all the possible next states\n",
    "            rewards : (m,) rewards for each next state transition\n",
    "            probs   : (m,) likelihood of each state-reward pair\n",
    "        Outputs:\n",
    "            states  : (num_states,) all the next states\n",
    "            rewards : (num_states,) rewards for each next state transition\n",
    "            probs   : (num_states,) likelihood of each state-reward pair (including zero-prob outcomes.)\n",
    "        '''\n",
    "        out_s = np.arange(self.num_states)\n",
    "        out_r = np.zeros(self.num_states)\n",
    "        out_p = np.zeros(self.num_states)\n",
    "        for i in range(len(states)):\n",
    "            idx = states[i]\n",
    "            out_r[idx] += rewards[i]\n",
    "            out_p[idx] += probs[i]\n",
    "        return (out_s, out_r, out_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course introduced a minature grid environment to simulate a basic reinforment learning problem. Here would be the toy environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toy(Environment):\n",
    "    def dynamics(self, state: int, action: int):\n",
    "        (S0, SL, SR) = (0, 1, 2)\n",
    "        LEFT = 0\n",
    "        num_states = 3 # TODO: does this need to be self.num_states... or is this even important?\n",
    "        num_actions = 2\n",
    "        assert 0 <= state < self.num_states and 0 <= action < self.num_actions\n",
    "        if state == S0:\n",
    "            if action == LEFT:\n",
    "                (next_state, reward) = (SL, 1)\n",
    "            else:\n",
    "                (next_state, reward) = (SR, 0)\n",
    "        elif state == SL:\n",
    "            (next_state, reward) = (S0, 0)\n",
    "        elif state == SR:\n",
    "            (next_state, reward) = (S0, 2)\n",
    "        return (np.array([next_state]), np.array([reward]), np.array([1]))\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]]]\n",
      "[[[0. 1. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[2. 0. 0.]\n",
      "  [2. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "if MAIN:\n",
    "    toy = Toy()\n",
    "    print(toy.T)\n",
    "    print(toy.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norvig(Environment):\n",
    "    def dynamics(self, state: int, action: int) -> tuple[Arr, Arr, Arr]:\n",
    "        def state_index(state):\n",
    "            assert 0 <= state[0] < self.width and 0 <= state[1] < self.height, print(state)\n",
    "            pos = state[0] + state[1] * self.width\n",
    "            assert 0 <= pos < self.num_states, print(state, pos)\n",
    "            return pos\n",
    "\n",
    "        pos = self.states[state]\n",
    "        move = self.actions[action]\n",
    "        if state in self.terminal or state in self.walls:\n",
    "            return (np.array([state]), np.array([0]), np.array([1]))\n",
    "        out_probs = np.zeros(self.num_actions) + 0.1\n",
    "        out_probs[action] = 0.7\n",
    "        out_states = np.zeros(self.num_actions, dtype=int) + self.num_actions\n",
    "        out_rewards = np.zeros(self.num_actions) + self.penalty\n",
    "        new_states = [pos + x for x in self.actions]\n",
    "        for (i, s_new) in enumerate(new_states):\n",
    "            if not (0 <= s_new[0] < self.width and 0 <= s_new[1] < self.height):\n",
    "                out_states[i] = state\n",
    "                continue\n",
    "            new_state = state_index(s_new)\n",
    "            if new_state in self.walls:\n",
    "                out_states[i] = state\n",
    "            else:\n",
    "                out_states[i] = new_state\n",
    "            for idx in range(len(self.terminal)):\n",
    "                if new_state == self.terminal[idx]:\n",
    "                    out_rewards[i] = self.goal_rewards[idx]\n",
    "        return (out_states, out_rewards, out_probs)\n",
    "\n",
    "    def render(self, pi: Arr):\n",
    "        assert len(pi) == self.num_states\n",
    "        emoji = [\"‚¨ÜÔ∏è\", \"‚û°Ô∏è\", \"‚¨áÔ∏è\", \"‚¨ÖÔ∏è\"]\n",
    "        grid = [emoji[act] for act in pi]\n",
    "        grid[3] = \"üü©\"\n",
    "        grid[7] = \"üü•\"\n",
    "        grid[5] = \"‚¨õ\"\n",
    "        print(str(grid[0:4]) + \"\\n\" + str(grid[4:8]) + \"\\n\" + str(grid[8:]))\n",
    "\n",
    "    def __init__(self, penalty=-0.04):\n",
    "        self.height = 3\n",
    "        self.width = 4\n",
    "        self.penalty = penalty\n",
    "        num_states = self.height * self.width\n",
    "        num_actions = 4\n",
    "        self.states = np.array([[x, y] for y in range(self.height) for x in range(self.width)])\n",
    "        self.actions = np.array([[0, -1], [1, 0], [0, 1], [-1, 0]])\n",
    "        self.dim = (self.height, self.width)\n",
    "        terminal = np.array([3, 7], dtype=int)\n",
    "        self.walls = np.array([5], dtype=int)\n",
    "        self.goal_rewards = np.array([1.0, -1])\n",
    "        super().__init__(num_states, num_actions, start=8, terminal=terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, my turn to actually code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 135 steps.\n",
      "Converged in 128 steps.\n",
      "Converged in 140 steps.\n",
      "Converged in 126 steps.\n",
      "Converged in 122 steps.\n"
     ]
    }
   ],
   "source": [
    "def policy_eval_numerical(env: Environment, pi: Arr, gamma=0.99, eps=1e-08):\n",
    "    '''\n",
    "    Numerically evaluates the value of a given policy by iterating the Bellman equation\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        pi : shape (num_states,) - The policy to evaluate\n",
    "        gamma: float - Discount factor\n",
    "        eps  : float - Tolerance\n",
    "    Outputs:\n",
    "        value : float (num_states,) - The value function for policy pi\n",
    "    '''\n",
    "    value = np.zeros((env.num_states,), dtype=float)\n",
    "\n",
    "    while True:\n",
    "        oldvalue = value.copy()\n",
    "        state = 0\n",
    "        for action in pi:\n",
    "            newValue = 0\n",
    "            # for each possible state given the current state and action\n",
    "            new_state_index = 0\n",
    "            for prob in env.T[state, action]:\n",
    "                newValue += prob * (env.R[state, action, new_state_index] + gamma * oldvalue[new_state_index])\n",
    "                new_state_index += 1\n",
    "\n",
    "            value[state] = newValue\n",
    "            state += 1\n",
    "\n",
    "        if np.linalg.norm(value-oldvalue) < eps:\n",
    "            break # break when there isn't much more change!\n",
    "\n",
    "    return value\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_eval(policy_eval_numerical, exact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_exact(env: Environment, pi: Arr, gamma=0.99) -> Arr:\n",
    "    p_pi = np.zeros((env.num_states, env.num_states))\n",
    "    R_pi = np.zeros((env.num_states, env.num_states))\n",
    "    for x in range(env.num_states):\n",
    "        for y in range(env.num_states):\n",
    "            p_pi[x,y] = env.T[x,pi[x],y]   \n",
    "            R_pi[x,y] = env.R[x, pi[x], y] \n",
    "    \n",
    "\n",
    "\n",
    "    r_pi = np.zeros(env.num_states)\n",
    "    for state in range(env.num_states):\n",
    "        value = 0\n",
    "        for jstate in range(env.num_states):\n",
    "            value += p_pi[state, jstate] * R_pi[state, jstate]\n",
    "        r_pi[state] = value\n",
    "\n",
    "\n",
    "    I = np.identity(env.num_states)\n",
    "\n",
    "\n",
    "    return np.linalg.inv(I - gamma * p_pi).dot(r_pi)\n",
    "    \n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_eval(policy_eval_exact, exact=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 11, 17],\n",
       "       [11, 25, 39]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]]) #shape 2 by 4\n",
    "b = np.array([[1,2],[3,4],[5,6]]) # shape 3 by 4\n",
    "\n",
    "einsum.einsum('a c, b c-> a b', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "def policy_improvement(env: Environment, V: Arr, gamma=0.99) -> Arr:\n",
    "    '''\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        V  : (num_states,) value of each state following some policy pi\n",
    "    Outputs:\n",
    "        pi_better : vector (num_states,) of actions representing a new policy obtained via policy iteration\n",
    "    '''\n",
    "    pi_better = np.zeros(env.num_states, dtype=int)\n",
    "    \n",
    "    for eval_state in range(env.num_states):\n",
    "        action_highest = 0\n",
    "        action_highest_sum = None\n",
    "\n",
    "        for action in range(env.num_actions):\n",
    "            sum = 0\n",
    "            for s_prime in range(env.num_states):\n",
    "                sum += env.T[eval_state, action, s_prime] * (env.R[eval_state, action, s_prime] + gamma * V[s_prime])\n",
    "            \n",
    "            if action_highest_sum == None or sum > action_highest_sum:\n",
    "                action_highest = action\n",
    "                action_highest_sum = sum\n",
    "\n",
    "        pi_better[eval_state] = action_highest\n",
    "\n",
    "\n",
    "    return pi_better\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_improvement(policy_improvement)\n",
    "    print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚û°Ô∏è', '‚û°Ô∏è', '‚û°Ô∏è', 'üü©']\n",
      "['‚¨ÜÔ∏è', '‚¨õ', '‚¨ÜÔ∏è', 'üü•']\n",
      "['‚¨ÜÔ∏è', '‚¨ÖÔ∏è', '‚¨ÜÔ∏è', '‚¨ÖÔ∏è']\n"
     ]
    }
   ],
   "source": [
    "def find_optimal_policy(env: Environment, gamma=0.99):\n",
    "    '''\n",
    "    Inputs:\n",
    "        env: environment\n",
    "    Outputs:\n",
    "        pi : (num_states,) int, of actions represeting an optimal policy\n",
    "    '''\n",
    "    pi = np.zeros(env.num_states, dtype=int)\n",
    "    while True:\n",
    "        valueFunc = policy_eval_exact(env, pi, gamma)\n",
    "        newpi = policy_improvement(env, valueFunc, gamma)\n",
    "\n",
    "        if (np.array_equal(pi, newpi)):\n",
    "            break\n",
    "        else:\n",
    "            pi = newpi.copy()\n",
    "            \n",
    "\n",
    "    return pi\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_find_optimal_policy(find_optimal_policy)\n",
    "    penalty = -0.04\n",
    "    norvig = Norvig(penalty)\n",
    "    pi_opt = find_optimal_policy(norvig, gamma=0.99)\n",
    "    norvig.render(pi_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ARENAenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
