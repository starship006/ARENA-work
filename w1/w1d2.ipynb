{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "import typing\n",
    "import einops\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dim = embedding_dim\n",
    "        self.length = max_seq_len\n",
    "\n",
    "        # mostly copied. i understand this, just need to work on \n",
    "        # making more tensors and getting more exposure to methods of making tensors\n",
    "        def P (delta):\n",
    "            n = 10000 # hardcoded\n",
    "            d = embedding_dim\n",
    "            l = max_seq_len\n",
    "            sin_array = np.sin(delta / n ** (2 * np.arange(d//2) / d))\n",
    "            cos_array = np.cos(delta / n ** (2 * np.arange(d//2) / d))\n",
    "\n",
    "            array = np.zeros(d)\n",
    "            array[::2] = sin_array\n",
    "            array[1::2] = cos_array\n",
    "\n",
    "            return array\n",
    "\n",
    "        tokenArray = []\n",
    "        for i in range(max_seq_len):\n",
    "            tokenArray[i] =P(i)\n",
    "        \n",
    "        self.multMax = tokenArray\n",
    "        \n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq_len, embedding_dim)\n",
    "        '''\n",
    "        return x + self.addMat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_head_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (seq_len, head_size)\n",
    "    K: shape (seq_len, head_size)\n",
    "    V: shape (seq_len, value_size)\n",
    "\n",
    "    Return: shape (seq_len, value_size)\n",
    "    '''\n",
    "\n",
    "    # second step - calculate a \"score\"\n",
    "    score = Q@(K.T) # shape (seq_len,seq_len)\n",
    "    # third step - divide score by dimensionality\n",
    "    score = score / np.sqrt(Q.shape[-1])\n",
    "    # fourth step - softmax\n",
    "    print(score[0])\n",
    "    score = nn.functional.softmax(score, dim=1)\n",
    "    print(score[0])\n",
    "\n",
    "    # fifth step - multiply each value vector by the softmax score\n",
    "    z = score@V\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 4.], dtype=torch.float64)\n",
      "tensor([0.0351, 0.2595, 0.7054], dtype=torch.float64)\n",
      "tensor([[3.6351],\n",
      "        [3.9522],\n",
      "        [3.9820]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# personal mini-test to see if its working correctly\n",
    "Q = t.tensor([[1],[3],[4]],dtype=float)\n",
    "K = t.tensor([[1],[3],[4]],dtype=float)\n",
    "V = t.tensor([[1],[3],[4]],dtype=float)\n",
    "\n",
    "print(single_head_attention(Q,K,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ARENAenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
