{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starship006/ARENA-work/blob/main/w1/w1d4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlRgy2kjSzdd",
        "outputId": "a811e651-aa95-471d-81b0-7af4a1b75c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fancy_einsum\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.5.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: fancy-einsum, einops\n",
            "Successfully installed einops-0.5.0 fancy-einsum-0.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install fancy_einsum einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aOsyx41fOvuN"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import fancy_einsum as einsum\n",
        "import einops\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oei89mWsOvuP"
      },
      "source": [
        "# Training Shakespeare Himself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HguafRmOvuQ"
      },
      "source": [
        "## Copy transformer code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9pLknZ-OvuQ"
      },
      "source": [
        "Well, not copy entirely - I'm gonna put down optimizations so it can use the GPU.\n",
        "\n",
        "And I did just that. The speed improvements are MASSIVE, wow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SMSyuB3OvuQ",
        "outputId": "36c9d02f-be9e-40b6-a6be-9101d27b6f33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
        "t.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dARzlkbDOvuQ",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
        "    '''\n",
        "    Implements multihead masked attention on the matrices Q, K and V.\n",
        "\n",
        "    Q: shape (batch, seq_len, nheads*headsize)\n",
        "    K: shape (batch, seq_len, nheads*headsize)\n",
        "    V: shape (batch, seq_len, nheads*headsize)\n",
        "    '''\n",
        "    \n",
        "    Q = einops.rearrange(Q, 'b s (n h) -> b n s h', n = num_heads)\n",
        "    K = einops.rearrange(K, 'b s (n h) -> b n s h', n = num_heads)\n",
        "    V = einops.rearrange(V, 'b s (n h) -> b n s h', n = num_heads)\n",
        "\n",
        "\n",
        "    scores = einsum.einsum('b n k h, b n s h -> b n s k', K, Q)\n",
        "    assert scores.shape == t.Size([Q.shape[0], num_heads,Q.shape[2], K.shape[2]])\n",
        "\n",
        "    scores = scores / np.sqrt(Q.shape[-1])\n",
        "    attention = scores + t.triu(t.ones_like(scores,device = device) * float(\"-inf\"), diagonal=1) # THIS IS STOLEN FROM JAY - testing it out\n",
        "    softed = t.softmax(attention,dim=-1)\n",
        "    result =  einsum.einsum('batch numheads seqQ seqK, batch numheads seqK headsize -> batch numheads seqQ headsize',softed, V)\n",
        "    return einops.rearrange(result, 'batch numheads seqQ headsize -> batch seqQ (numheads headsize)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pxFIcpudOvuR",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class MultiheadMaskedAttention(nn.Module):\n",
        "    W_QKV: nn.Linear\n",
        "    W_O: nn.Linear\n",
        "\n",
        "    def __init__(self, hidden_size: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = hidden_size // num_heads\n",
        "\n",
        "        self.WQKV = t.nn.Linear(self.hidden_size, 3 * hidden_size) # TODO: why do we use a linear layer here? aren't they matricies?\n",
        "        self.W0 = t.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        '''\n",
        "        x: shape (batch, seq, hidden_size)\n",
        "\n",
        "        Return: shape (batch, seq, hidden_size)\n",
        "        '''\n",
        "        #print(\"YO?\")\n",
        "        x = x.float() # seems like it needs to be a float!\n",
        "        QKV = self.WQKV(x)\n",
        "        Q = QKV[:,:,:self.hidden_size]\n",
        "        K = QKV[:,:,self.hidden_size:self.hidden_size * 2]\n",
        "        V = QKV[:,:,self.hidden_size * 2:]\n",
        "        assert Q.shape == K.shape == V.shape == x.shape\n",
        "        return self.W0(multihead_masked_attention(Q,K,V,self.num_heads))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nG9-RZDwOvuR",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TransformerConfig:\n",
        "    '''Constants used throughout your decoder-only transformer model.'''\n",
        "\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    vocab_size: int\n",
        "    hidden_size: int\n",
        "    max_seq_len: int\n",
        "    dropout: float = 0.1\n",
        "    layer_norm_epsilon: float = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dxznpdXeOvuS",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "# from yesterday\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, max_seq_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dim = embedding_dim\n",
        "        self.length = max_seq_len\n",
        "\n",
        "        # mostly copied. i understand this, just need to work on \n",
        "        # making more tensors and getting more exposure to methods of making tensors\n",
        "        def P (delta):\n",
        "            n = 10000 # hardcoded\n",
        "            d = embedding_dim\n",
        "            l = max_seq_len\n",
        "            sin_array = np.sin(delta / n ** (2 * np.arange(d//2) / d))\n",
        "            cos_array = np.cos(delta / n ** (2 * np.arange(d//2) / d))\n",
        "\n",
        "            array = np.zeros(d)\n",
        "            array[::2] = sin_array\n",
        "            array[1::2] = cos_array\n",
        "\n",
        "            return array\n",
        "\n",
        "        tokenArray = []\n",
        "        for i in range(max_seq_len):\n",
        "            tokenArray.append(P(i)) # changed from previous design\n",
        "        \n",
        "        self.multMax = t.tensor(np.array(tokenArray), dtype=t.float, device = device)\n",
        "        \n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        '''\n",
        "        x: shape (batch, seq_len, embedding_dim)\n",
        "        '''\n",
        "        return x + self.multMax[:x.shape[1]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4evwbrSDOvuT",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "    def forward(self, x: t.Tensor):\n",
        "        x = x.float() # seems like it needs to be a float!\n",
        "        return self.layers(x).float() # ima do the same thing again!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O_i-QmphOvuT",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.attentionBlock = nn.Sequential(\n",
        "            MultiheadMaskedAttention(config.hidden_size,  config.num_heads),\n",
        "            nn.LayerNorm(config.hidden_size)\n",
        "        )\n",
        "        self.MLP = nn.Sequential(\n",
        "            MLP(config),\n",
        "            nn.LayerNorm(config.hidden_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        partOne = x + self.attentionBlock(x)\n",
        "        return (partOne + self.MLP(partOne)).float() # seems like it needs to be a float!\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YoM0le7qOvuU",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.tokenize = nn.Embedding(config.vocab_size, config.hidden_size).to(device)\n",
        "        self.positionize = PositionalEncoding(config.hidden_size,config.max_seq_len)\n",
        "        self.restModel = nn.Sequential(\n",
        "            nn.Dropout(config.dropout),\n",
        "            *[DecoderBlock(config) for i in range(config.num_layers)],\n",
        "            nn.LayerNorm(config.hidden_size),\n",
        "        )\n",
        "        self.unembed = self.tokenize.weight.T.to(device)\n",
        "        \n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        x = self.tokenize(x)\n",
        "        x = self.positionize(x)\n",
        "        toUnembed = self.restModel(x).to(device)\n",
        "        return toUnembed@self.unembed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E108n5oQOvuU"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvqBu-CxOvuU"
      },
      "source": [
        "Make the dataset to parse through all of the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x17J93OHOvuU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, words, seq_len, fractionOfWords):\n",
        "        self.fractionOfWords = fractionOfWords\n",
        "        self.words = words\n",
        "        self.setOfWords = set(words)\n",
        "        self.seq_len = seq_len\n",
        "        self.max_len = len(self.words) - (self.seq_len + 1)\n",
        "        self.vocab_size = len(self.setOfWords)\n",
        "        self.word_to_token = {word: idx for (idx, word) in enumerate(sorted(self.setOfWords))}\n",
        "        self.token_to_word = {idx: word for (idx, word) in enumerate(sorted(self.setOfWords))}\n",
        "        self.allTokens = t.tensor([self.word_to_token[word] for word in self.words],device = device)\n",
        "        \n",
        "        if (self.fractionOfWords > 0.9):\n",
        "            print(\"Probably don't do this. Errors may about\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.max_len * self.fractionOfWords)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.allTokens[idx:idx + self.seq_len + 1]\n",
        "        input = tokens[:-1]\n",
        "        target = tokens[1:]\n",
        "        return input, target \n",
        "\n",
        "    def getDataSize(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def convertToTokens(self, phrase: list) -> t.tensor:\n",
        "        return t.tensor([self.word_to_token[word] for word in phrase],device = device)\n",
        "\n",
        "    def convertStringToTokenList(self, phrase: str) -> list:\n",
        "        words = re.split(r\"\\b\", phrase)\n",
        "        return [self.word_to_token[word] for word in words]\n",
        "\n",
        "    def convertToText(self, tokens: t.tensor):\n",
        "        temp = []\n",
        "        for i, value in enumerate(tokens):\n",
        "            #print(value.item())\n",
        "            temp.append(self.token_to_word[value.item()])\n",
        "        return temp\n",
        "\n",
        "    def decodeList(self, words: list):\n",
        "        temp = []\n",
        "        for value in words:\n",
        "            temp.append(self.token_to_word[value])\n",
        "        return temp\n",
        "        \n",
        "    def listToString(self, words: list) -> str:\n",
        "        temp = \"\"\n",
        "        for word in words:\n",
        "            temp = temp + word\n",
        "        return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nv945w7VOvuU"
      },
      "outputs": [],
      "source": [
        "file = open(\"shakespeare.txt\")\n",
        "text = file.read()\n",
        "words = re.split(r\"\\b\", text)\n",
        "\n",
        "fractionOfWords = 0.1 # what percent of the corpus to train on \n",
        "\n",
        "\n",
        "lengthOfSeq = 100\n",
        "\n",
        "shak = CustomTextDataset(words, lengthOfSeq, fractionOfWords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeWDlv7FOvuU"
      },
      "source": [
        "## Running this data through a transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdqNIkuIOvuV"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(shak, batch_size=32,shuffle=True)\n",
        "\n",
        "# this specific one trained for 24 minutes and 9 seconds on colab GPU\n",
        "\n",
        "thisConfig = TransformerConfig(\n",
        "    num_layers = 4, # 6 layers in the Attention paper\n",
        "    num_heads = 4, # 8 heads in Attention paper\n",
        "    vocab_size = trainloader.dataset.getDataSize(), # 37000 tokens in Attention paper (?)\n",
        "    hidden_size = 512, # recall that this = num_heads * headsize | 512 is the embedding dim used in Attention paper\n",
        "    max_seq_len = lengthOfSeq, \n",
        "    dropout = 0.1, # same as Attention paper\n",
        "    layer_norm_epsilon=0.00001\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Tl8YEwDhOvuV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Pre-trained Model!\n"
          ]
        }
      ],
      "source": [
        "use_pretrained = True\n",
        "if use_pretrained:\n",
        "    print(\"Using Pre-trained Model!\")\n",
        "    myTransformer = DecoderOnlyTransformer(thisConfig).to(device)\n",
        "    optimizer = t.optim.Adam(myTransformer.parameters(), lr = 1e-3)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    myTransformer.load_state_dict(t.load(\"toInfer.pt\", map_location=device))\n",
        "    myTransformer.eval()\n",
        "else:\n",
        "    print(\"Training Model... better hope you got enough GPU!\")\n",
        "    myTransformer = DecoderOnlyTransformer(thisConfig).to(device)\n",
        "    optimizer = t.optim.Adam(myTransformer.parameters(), lr = 1e-3)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    NUM_EPOCHS = 1\n",
        "\n",
        "    losses = []\n",
        "    myTransformer.train()\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        for inputs, targets in trainloader:\n",
        "            outputs = myTransformer(inputs).to(device)\n",
        "            targets = t.nn.functional.one_hot(targets, num_classes=trainloader.dataset.getDataSize()).float().to(device)\n",
        "            \n",
        "            outputs = einops.rearrange(outputs, 'batch seq vocab -> (batch seq) vocab')\n",
        "            targets = einops.rearrange(targets, 'batch seq vocab -> (batch seq) vocab')\n",
        "\n",
        "            outputs = outputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            loss = criterion(outputs,targets).to(device)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "NUjqRiN4OvuV",
        "outputId": "82e19786-1fac-421e-e75c-4ea4081eb059"
      },
      "outputs": [],
      "source": [
        "if not use_pretrained:\n",
        "    df = pd.DataFrame(losses)\n",
        "    df.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vyx4rgkOvuV",
        "outputId": "879c83db-4f97-45a7-cd4c-132df03c8182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  111, 22082,   111, 24741,   111, 31376,   111, 20247,   111, 24082,\n",
            "          405,  9859,   111])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[' ',\n",
              " 'it',\n",
              " ' ',\n",
              " 'offended',\n",
              " ' ',\n",
              " 'to',\n",
              " ' ',\n",
              " 'govern',\n",
              " ' ',\n",
              " 'mother',\n",
              " ',\\n',\n",
              " 'The',\n",
              " ' ']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# quick test - use the sample method if you wish to actually use the transformer: \n",
        "\n",
        "myTransformer.eval()\n",
        "\n",
        "testPhrase = [\"Be\", \" \", \"not\", \" \", \"afraid\", \" \", \"to\", \" \", \"the\", \" \", \"Florentine\", \"\\n\",\n",
        "              \"And\"]\n",
        "input = shak.convertToTokens(testPhrase)\n",
        "input = input[None, :]\n",
        "tokens = myTransformer(input).argmax(dim=-1)[0]\n",
        "print(tokens)\n",
        "shak.convertToText(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSwuSOXhVi1B"
      },
      "source": [
        "# Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkE5o5i-VlVK",
        "outputId": "1e5efe8e-56cb-485a-8d6e-dd4a7ae27159"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def apply_sampling_methods(input_ids: t.Tensor, logits: t.Tensor, temperature=1.0, freq_penalty=0.0, top_k=0, top_p=0.0) -> int:\n",
        "  # returns a next token based on provided sampling method\n",
        "  # thanks callum for the this method\n",
        "  assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
        "  assert temperature >= 0, \"Temperature should be non-negative\"\n",
        "  assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
        "  assert 0 <= top_k, \"Top-k must be non-negative\"\n",
        "  assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
        "\n",
        "  if temperature == 0:\n",
        "    return greedy_search(logits)\n",
        "  if temperature != 1.0:\n",
        "    logits = apply_temperature(logits, temperature)\n",
        "  if freq_penalty != 0.0:\n",
        "    logits = apply_freq_penalty(input_ids, logits, freq_penalty)\n",
        "  if top_k > 0:\n",
        "    return sample_top_k(logits, top_k)\n",
        "  if top_p > 0:\n",
        "    return sample_top_p(logits, top_p)\n",
        "  return sample_basic(logits)\n",
        "\n",
        "\n",
        "def sample_tokens(\n",
        "    model,\n",
        "    encodeMethod,\n",
        "    decodeMethod,\n",
        "    initial_text: str,\n",
        "    max_tokens_generated = 40,\n",
        "    **kwargs) -> list:\n",
        "    # samples tokens until model outputs eos_token_id or token limit reached\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    model.eval()\n",
        "    input_ids: list = encodeMethod(initial_text)\n",
        "    generated_ids = []\n",
        "    device = next(model.parameters()).device #what is next doing here?\n",
        "\n",
        "    tokens_to_generate = max_tokens_generated - len(input_ids)\n",
        "    for _ in range(tokens_to_generate):\n",
        "        #print(input_ids + generated_ids)\n",
        "        new_input_ids = t.tensor(input_ids + generated_ids, dtype=t.int64, device=device)\n",
        "        #print(new_input_ids.unsqueeze(0).shape)\n",
        "        logits = model(new_input_ids.unsqueeze(0))[0, -1]\n",
        "        #print(logits.shape)\n",
        "        new_token = apply_sampling_methods(new_input_ids, logits, **kwargs)\n",
        "        generated_ids.append(new_token)\n",
        "\n",
        "      \n",
        "    return decodeMethod(input_ids + generated_ids)\n",
        "\n",
        "\n",
        "# quick test:\n",
        "\n",
        "myTransformer.eval()\n",
        "\n",
        "testPhrase = [\"Be\", \" \", \"not\", \" \", \"afraid\", \" \", \"to\", \" \", \"the\", \" \", \"Florentine\", \"\\n\",\n",
        "              \"And\"]\n",
        "input = shak.convertToTokens(testPhrase)\n",
        "type(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "McngECeVWCl-"
      },
      "outputs": [],
      "source": [
        "def greedy_search(logits):\n",
        "    '''\n",
        "    returns the most likely next token, BUT THE TIEBREAKER IS INCORRECT!\n",
        "    i got lazy - it *is* deterministic, but it just doesn't necessarily\n",
        "    choose the smallest word out of the tie. perhaps treat it as a symbol\n",
        "    of my ingenuity?\n",
        "    '''\n",
        "    return logits.argmax(dim=-1).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCZKbbAkltBf",
        "outputId": "2ce05748-87c4-4b11-9592-0bf8009be3d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.1018, 0.2026, 0.3009, 0.3947])\n",
            "Tests passed!\n"
          ]
        }
      ],
      "source": [
        "def sample_basic(logits) -> int:\n",
        "    '''\n",
        "    samples from the distributions, possibly with temp and freq changes applied\n",
        "\n",
        "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
        "\n",
        "    return: a sampled token\n",
        "    '''\n",
        "    probs = t.distributions.categorical.Categorical(logits=logits)\n",
        "    return probs.sample().item()\n",
        "\n",
        "N = 20000\n",
        "probs = t.linspace(0, 0.4, 5)\n",
        "unnormalized_logits = probs.log() + 1.2345\n",
        "samples = t.tensor([sample_basic(unnormalized_logits) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(probs)) / N\n",
        "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
        "t.testing.assert_close(counts, probs, atol=0.01, rtol=0)\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoCPqZUilokP",
        "outputId": "e7663ad5-ed0d-40e5-d08c-dc5fe4445fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0, 2037, 0]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def apply_freq_penalty(input_ids: t.Tensor, logits: t.Tensor, freq_penalty: float) -> t.Tensor:\n",
        "    '''\n",
        "    input_ids: shape (seq, )\n",
        "    logits: shape (vocab_size, )\n",
        "\n",
        "    Return: shape (vocab_size, )\n",
        "    '''\n",
        "    (vocab_size,) = logits.shape\n",
        "    id_freqs = t.bincount(input_ids, minlength=vocab_size)\n",
        "    return logits - freq_penalty * id_freqs\n",
        "\n",
        "bieber_prompt = \"And I was like baby, baby, baby, oh Like, baby, baby, baby, no Like, baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
        "input_ids = shak.convertStringToTokenList(bieber_prompt)\n",
        "logits = t.ones(shak.getDataSize()).to(device)\n",
        "penalized_logits = apply_freq_penalty(t.tensor(input_ids).to(device), logits, 2.0)\n",
        "#i believe mine is different!\n",
        "#assert penalized_logits[5156].item() == -11, \"Expected 6 occurrences of ' baby' with leading space\"\n",
        "#assert penalized_logits[14801].item() == -5, \"Expected 3 occurrences of ' Baby' with leading space\"\n",
        "#print(\"Tests passed!\")\n",
        "\n",
        "print(penalized_logits[2037].item()) # should be low since it was found!\n",
        "shak.convertStringToTokenList(\"And\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A low temperature \"sharpens\" or \"peaks\" the distribution:  tensor([  0.0000, 693.1472])\n",
            "A high temperature flattens the distribution:  tensor([0.0000, 0.0007])\n",
            "Tests passed!\n"
          ]
        }
      ],
      "source": [
        "def apply_temperature(logits: t.Tensor, temperature: float) -> t.Tensor:\n",
        "    assert temperature > 0, \"temp cannot be less than or equal to 0\"\n",
        "\n",
        "    return logits / temperature\n",
        "\n",
        "logits = t.tensor([1, 2]).log()\n",
        "cold_logits = apply_temperature(logits, 0.001)\n",
        "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
        "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
        "hot_logits = apply_temperature(logits, 1000.0)\n",
        "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
        "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 0 with: High freq penalty ({'freq_penalty': 100.0}):\n",
            "Your model said: We are the champions, my friends,\n",
            "    Let, 'twill\n",
            "    Here's; of '\n",
            "\n",
            "Sample 0 with: Negative freq penalty ({'freq_penalty': -1.0}):\n",
            "Your model said: We are the champions, my friends in thy kingdom be missed:\n",
            "\n",
            "\n",
            "Sample 0 with: Too hot! ({'temperature': 2.0}):\n",
            "Your model said: We are the champions, my friends.\n",
            "\n",
            "DEMETRIUS.\n",
            "Dramatis to love. pleasest \n",
            "\n",
            "Sample 0 with: Pleasantly cool ({'temperature': 0.7}):\n",
            "Your model said: We are the champions, my friends, and next unto high heaven \n",
            "\n",
            "Sample 0 with: Pleasantly warm ({'temperature': 0.9}):\n",
            "Your model said: We are the champions, my friends, and turn’d daily in \n",
            "\n",
            "Sample 0 with: Too cold! ({'temperature': 0.01}):\n",
            "Your model said: We are the champions, my friends, and true,\n",
            "Like a deceived \n",
            "\n"
          ]
        }
      ],
      "source": [
        "N_RUNS = 1\n",
        "your_prompt = \"We are the champions, my friends\"\n",
        "cases = [\n",
        "    (\"High freq penalty\", dict(freq_penalty=100.0)),\n",
        "    (\"Negative freq penalty\", dict(freq_penalty=-1.0)),\n",
        "    (\"Too hot!\", dict(temperature=2.0)),\n",
        "    (\"Pleasantly cool\", dict(temperature=0.7)),\n",
        "    (\"Pleasantly warm\", dict(temperature=0.9)),\n",
        "    (\"Too cold!\", dict(temperature=0.01)),\n",
        "]\n",
        "for (name, kwargs) in cases:\n",
        "    for i in range(N_RUNS):\n",
        "        output = sample_tokens(myTransformer, shak.convertStringToTokenList,shak.decodeList, your_prompt, max_tokens_generated=24, **kwargs)\n",
        "        print(f\"Sample {i} with: {name} ({kwargs}):\")\n",
        "        print(f\"Your model said: {shak.listToString(output)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.0000, 0.2215, 0.3307, 0.4478])\n",
            "Tests passed!\n"
          ]
        }
      ],
      "source": [
        "def sample_top_k(logits: t.Tensor, top_k: int) -> int:\n",
        "    '''\n",
        "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
        "    top_k: only consider this many of the most likely tokens for sampling\n",
        "\n",
        "    Return: a sampled token\n",
        "    '''\n",
        "    topk = t.topk(logits,top_k).indices\n",
        "    almost_zeroes = t.ones(logits.shape) * t.inf * -1\n",
        "    for _, token in enumerate(topk):\n",
        "        almost_zeroes[token] = 0\n",
        "    logits = logits + almost_zeroes\n",
        "    return sample_basic(logits)\n",
        "\n",
        "k = 3\n",
        "probs = t.linspace(0, 0.4, 5)\n",
        "unnormalized_logits = probs.log() + 1.2345\n",
        "samples = t.tensor([sample_top_k(unnormalized_logits, k) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(probs)) / N\n",
        "expected = probs.clone()\n",
        "expected[:-k] = 0\n",
        "expected /= expected.sum()\n",
        "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
        "t.testing.assert_close(counts, expected, atol=0.01, rtol=0)\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "top_p of 0.5 or lower should only return token 2:  tensor([0., 0., 1.])\n",
            "top_p in (0.5, 0.8] should return tokens 1 and 2:  tensor([0.0000, 0.3485, 0.6515])\n",
            "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.0000, 0.2173, 0.3405, 0.4423])\n",
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "def sample_top_p(logits: t.Tensor, top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
        "    '''\n",
        "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
        "\n",
        "    Return: a sampled token\n",
        "    '''\n",
        "    # find the indices of importang logits\n",
        "    sorted, indices = t.sort(logits,descending=True)\n",
        "    probs = t.nn.functional.softmax(sorted, dim=-1)\n",
        "    num_words_kept = 0\n",
        "    sum = 0\n",
        "    while sum < top_p:\n",
        "        sum = sum + probs[num_words_kept]\n",
        "        num_words_kept = num_words_kept + 1\n",
        "        \n",
        "\n",
        "    if num_words_kept < min_tokens_to_keep:\n",
        "        num_words_kept = min_tokens_to_keep\n",
        "    \n",
        "    important_indices = indices[:num_words_kept]\n",
        "\n",
        "    # prepare tensor to zero out small logits\n",
        "    almost_zeroes = t.ones(logits.shape) * t.inf * -1\n",
        "    for _, token in enumerate(important_indices):\n",
        "        almost_zeroes[token] = 0\n",
        "    logits = logits + almost_zeroes\n",
        "    return sample_basic(logits)\n",
        "\n",
        "N = 2000\n",
        "unnormalized_logits = t.tensor([0.2, 0.3, 0.5]).log() + 2.3456\n",
        "samples = t.tensor([sample_top_p(unnormalized_logits, 0.5) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(unnormalized_logits)) / N\n",
        "print(\"top_p of 0.5 or lower should only return token 2: \", counts)\n",
        "assert counts[0] == 0 and counts[1] == 0\n",
        "\n",
        "N = 2000\n",
        "unnormalized_logits = t.tensor([0.2, 0.3, 0.5]).log() + 2.3456\n",
        "samples = t.tensor([sample_top_p(unnormalized_logits, 0.50001) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(unnormalized_logits)) / N\n",
        "print(\"top_p in (0.5, 0.8] should return tokens 1 and 2: \", counts)\n",
        "assert counts[0] == 0\n",
        "\n",
        "N = 4000\n",
        "top_p = 0.71\n",
        "probs = t.linspace(0, 0.4, 5)\n",
        "unnormalized_logits = probs.log() + 1.2345\n",
        "samples = t.tensor([sample_top_p(unnormalized_logits, top_p) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(probs)) / N\n",
        "expected = probs.clone()\n",
        "expected[0:2] = 0\n",
        "expected /= expected.sum()\n",
        "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
        "t.testing.assert_close(counts, expected, atol=0.01, rtol=0.0)\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speak, Shakespeare!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Death waits at the door, and rotten and take\n",
            "The fellow outworn twice ordering.\n",
            "\n",
            " [_Exit._]\n",
            "\n",
            "CHARMIAN.\n",
            "Hold, keep your army.\n",
            "\n",
            "CLEOPATRA.\n",
            "Noblest of Rome, or good water-suit?\n",
            "\n",
            "MARDIAN.\n",
            "As well-whit.\n",
            "\n",
            "CLEOPATRA.\n",
            "Is’t not?\n",
            "\n",
            "ANTONY.\n",
            "As \n"
          ]
        }
      ],
      "source": [
        "input = \"Death waits at the door\"\n",
        "\n",
        "print(shak.listToString(sample_tokens(myTransformer,shak.convertStringToTokenList,shak.decodeList,\n",
        "                                     input, 80,freq_penalty=0.1, top_k = 10)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSfPwNmmmY32"
      },
      "source": [
        "## Save the model for future use\n",
        "(This was over 20 minutes of GPU computation. Not too shabby!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "CBK0fyujcfIr"
      },
      "outputs": [],
      "source": [
        "t.save(myTransformer.state_dict(), \"toInfer.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
