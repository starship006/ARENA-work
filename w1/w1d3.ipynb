{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import createdFuncs as past\n",
    "import fancy_einsum as einsum\n",
    "import einops\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to change the masked attention block to work over multiple batches. As I'm working through this, I'm reminded that I really need to understand einops and linear algebra. The matrix multiplication really feels like a black box operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_head_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    Should return the results of masked self-attention.\n",
    "\n",
    "    See \"The Decoder Side\" section of the Illustrated Transformer for an explanation of masking.\n",
    "\n",
    "    Q: shape (batch, seq_len, head_size)\n",
    "    K: shape (batch, seq_len, head_size)\n",
    "    V: shape (batch, seq_len, value_size)\n",
    "\n",
    "    Return: shape (batch, seq_len, value_size)\n",
    "    '''\n",
    "    # second step - calculate a \"score\"\n",
    "    newK = einsum.einsum('b s h -> b h s', K)\n",
    "    score = Q@newK\n",
    "    # third step - divide score by dimensionality\n",
    "    score = score / np.sqrt(Q.shape[-1])\n",
    "    print(score.shape)\n",
    "    # MASKING IN BETWEEN!\n",
    "    mask = t.ones(score.shape)\n",
    "    for batch, submask in enumerate(mask):\n",
    "        for i, x in enumerate(submask):\n",
    "            x[i+1:] = -t.inf\n",
    "\n",
    "    score = score * mask\n",
    "    # fourth step - softmax\n",
    "    score = nn.functional.softmax(score, dim=-1)\n",
    "\n",
    "    # fifth step - multiply each value vector by the softmax score\n",
    "    z = score@V\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "tensor([[[1.0000],\n",
      "         [2.9951],\n",
      "         [3.9820]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [2.9951],\n",
      "         [3.9820]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# testing the new block\n",
    "# personal mini-test to see if its working correctly\n",
    "Q = t.tensor([[[1],[3],[4]],[[1],[3],[4]]],dtype=float)\n",
    "K = t.tensor([[[1],[3],[4]],[[1],[3],[4]]],dtype=float)\n",
    "V = t.tensor([[[1],[3],[4]],[[1],[3],[4]]],dtype=float)\n",
    "\n",
    "print(single_head_masked_attention(Q,K,V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the multiheaaded attention block!\n",
    "\n",
    "*(Cody on Saturday here: this one does not work fully as intended! I have reimplemented it below, look at it below!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is bad! do not use it!\n",
    "# this is bad! do not use it!\n",
    "# this is bad! do not use it!\n",
    "\n",
    "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "    '''\n",
    "    Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "    Q: shape (batch, seq, nheads*head_size)\n",
    "    K: shape (batch, seq, nheads*head_size)\n",
    "    V: shape (batch, seq, nheads*head_size)\n",
    "    '''\n",
    "    # reshape Q, K, and V into 4D tensors to isolate the different heads\n",
    "    batch, seq, product = Q.shape\n",
    "    head_size = product // num_heads\n",
    "    Q = Q.reshape((batch, seq, num_heads, head_size))\n",
    "    K = K.reshape((batch, seq, num_heads, head_size))\n",
    "    V = V.reshape((batch, seq, num_heads, head_size))\n",
    "\n",
    "    # calculate attention scores\n",
    "    newK = einsum.einsum('b s n h -> b h n s', K)\n",
    "    score = einsum.einsum('b s n h, b h n a -> b s n a', Q, newK)\n",
    "    # third step - divide score by dimensionality\n",
    "    score = score / np.sqrt(Q.shape[-1])\n",
    "    # MASKING IN BETWEEN!\n",
    "    mask = t.ones(score.shape)\n",
    "    for _, batchMask in enumerate(mask):\n",
    "        for i, seqMask in enumerate(batchMask):\n",
    "            for j, x in enumerate(seqMask):\n",
    "                x[i+1:] = -t.inf \n",
    "\n",
    "    score = score * mask\n",
    "    # fourth step - softmax\n",
    "    score = nn.functional.softmax(score, dim=-1)\n",
    "    print(score.shape)\n",
    "    print(V.shape)\n",
    "    # fifth step - multiply each value vector by the softmax score\n",
    "    z = einsum.einsum('b s n s, b s n h -> b s n h ', score, V)\n",
    "    z = z.reshape((batch, seq, product))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 2, 5])\n",
      "torch.Size([2, 5, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[15.0000, 14.6667, 14.3333, 14.0000],\n",
       "         [12.6399, 12.3316, 12.6629, 12.3383],\n",
       "         [12.2258, 11.8953, 11.6333, 11.3009],\n",
       "         [10.9897, 10.6567, 10.3302,  9.9969],\n",
       "         [ 9.6657,  9.3324,  8.9997,  8.6664]],\n",
       "\n",
       "        [[ 8.3333,  8.0000,  7.6667,  7.3333],\n",
       "         [ 7.0000,  6.6667,  6.3333,  6.0000],\n",
       "         [ 5.6667,  5.3333,  5.0000,  4.6667],\n",
       "         [ 4.3333,  4.0000,  3.6667,  3.3333],\n",
       "         [ 3.0000,  2.6667,  2.3333,  2.0000]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test from James Dao; my stuff continues to work\n",
    "Q = t.linspace(0, 10, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "K = t.linspace(5, 20, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "V = t.linspace(15, 2, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "multihead_masked_attention(Q, K, V, num_heads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making an attention block out of the above function is tricky. May need to come back to it after re-reading some more transformer stuff. For now, I'll go and review einsum :)\n",
    "\n",
    "And... after reviewing, and feeling better, about einsum, I'll need to call it a day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saturday, October 30 - it's been a bit since I've worked on this, and I really need to understand multi-headed attention. But, I don't think I really do, yet, so I'm going to try to write out the previous function again :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "    '''\n",
    "    Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "    Q: shape (batch, seq_len, nheads*headsize)\n",
    "    K: shape (batch, seq_len, nheads*headsize)\n",
    "    V: shape (batch, seq_len, nheads*headsize)\n",
    "    '''\n",
    "    \n",
    "    Q = einops.rearrange(Q, 'b s (n h) -> b n s h', n = num_heads)\n",
    "    K = einops.rearrange(K, 'b s (n h) -> b n s h', n = num_heads)\n",
    "    V = einops.rearrange(V, 'b s (n h) -> b n s h', n = num_heads)\n",
    "\n",
    "\n",
    "    scores = einsum.einsum('b n k h, b n s h -> b n s k', K, Q)\n",
    "    assert scores.shape == t.Size([Q.shape[0], num_heads,Q.shape[2], K.shape[2]])\n",
    "\n",
    "    scores = scores / np.sqrt(Q.shape[-1])\n",
    "    attention = scores + t.triu(t.ones_like(scores) * float(\"-inf\"), diagonal=1) # THIS IS STOLEN FROM JAY - testing it out\n",
    "    softed = t.softmax(attention,dim=-1)\n",
    "    result =  einsum.einsum('batch numheads seqQ seqK, batch numheads seqK headsize -> batch numheads seqQ headsize',softed, V)\n",
    "    return einops.rearrange(result, 'batch numheads seqQ headsize -> batch seqQ (numheads headsize)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15.0000, 14.6667, 14.3333, 14.0000],\n",
       "         [13.7668, 13.4335, 13.0346, 12.7012],\n",
       "         [12.3451, 12.0117, 11.6705, 11.3372],\n",
       "         [11.0013, 10.6679, 10.3337, 10.0004],\n",
       "         [ 9.6668,  9.3335,  9.0000,  8.6667]],\n",
       "\n",
       "        [[ 8.3333,  8.0000,  7.6667,  7.3333],\n",
       "         [ 7.0000,  6.6667,  6.3333,  6.0000],\n",
       "         [ 5.6667,  5.3333,  5.0000,  4.6667],\n",
       "         [ 4.3333,  4.0000,  3.6667,  3.3333],\n",
       "         [ 3.0000,  2.6667,  2.3333,  2.0000]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test from James Dao; my stuff continues to work\n",
    "Q = t.linspace(0, 10, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "K = t.linspace(5, 20, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "V = t.linspace(15, 2, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "multihead_masked_attention(Q, K, V, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadMaskedAttention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "\n",
    "        self.WQKV = t.nn.Linear(self.hidden_size, 3 * hidden_size) # TODO: why do we use a linear layer here? aren't they matricies?\n",
    "        self.W0 = t.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        #print(\"YO?\")\n",
    "        x = x.float() # seems like it needs to be a float!\n",
    "        QKV = self.WQKV(x)\n",
    "        Q = QKV[:,:,:self.hidden_size]\n",
    "        K = QKV[:,:,self.hidden_size:self.hidden_size * 2]\n",
    "        V = QKV[:,:,self.hidden_size * 2:]\n",
    "        assert Q.shape == K.shape == V.shape == x.shape\n",
    "        return self.W0(multihead_masked_attention(Q,K,V,self.num_heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.7193,   0.4614,   0.4117,  -0.5813,   0.2754,  -0.5745],\n",
       "         [ -0.7746,   0.6206,   0.5520,  -0.7370,   0.1787,  -0.7289],\n",
       "         [ -1.1632,   1.7392,   1.5775,  -1.7907,  -0.5079,  -1.8103]],\n",
       "\n",
       "        [[  0.0549,  -1.9665, -10.8756,  -7.1792,   3.4559,   0.9521],\n",
       "         [ -0.3971,  -0.6652,  -9.6883,  -8.4108,   2.6582,  -0.3063],\n",
       "         [ -0.8686,   0.6920,  -8.4500,  -9.6953,   1.8262,  -1.6189]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(420)\n",
    "m = MultiheadMaskedAttention(6, 2)\n",
    "x = t.linspace(0, 42, 2 * 3 * 6).reshape(2, 3, 6)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    max_seq_len: int\n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partly copy-pasting - but FIXING -  the positional encoding from previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from yesterday\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dim = embedding_dim\n",
    "        self.length = max_seq_len\n",
    "\n",
    "        # mostly copied. i understand this, just need to work on \n",
    "        # making more tensors and getting more exposure to methods of making tensors\n",
    "        def P (delta):\n",
    "            n = 10000 # hardcoded\n",
    "            d = embedding_dim\n",
    "            l = max_seq_len\n",
    "            sin_array = np.sin(delta / n ** (2 * np.arange(d//2) / d))\n",
    "            cos_array = np.cos(delta / n ** (2 * np.arange(d//2) / d))\n",
    "\n",
    "            array = np.zeros(d)\n",
    "            array[::2] = sin_array\n",
    "            array[1::2] = cos_array\n",
    "\n",
    "            return array\n",
    "\n",
    "        tokenArray = []\n",
    "        for i in range(max_seq_len):\n",
    "            tokenArray.append(P(i)) # changed from previous design\n",
    "        \n",
    "        self.multMax = t.tensor(np.array(tokenArray), dtype=float)\n",
    "        \n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq_len, embedding_dim)\n",
    "        '''\n",
    "        return x + self.multMax[:x.shape[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    def forward(self, x: t.Tensor):\n",
    "        x = x.float() # seems like it needs to be a float!\n",
    "        return self.layers(x).float() # ima do the same thing again!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attentionBlock = nn.Sequential(\n",
    "            MultiheadMaskedAttention(config.hidden_size,  config.num_heads),\n",
    "            nn.LayerNorm(config.hidden_size)\n",
    "        )\n",
    "        self.MLP = nn.Sequential(\n",
    "            MLP(config),\n",
    "            nn.LayerNorm(config.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        partOne = x + self.attentionBlock(x)\n",
    "        return (partOne + self.MLP(partOne)).float() # seems like it needs to be a float!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.tokenize = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positionize = PositionalEncoding(config.hidden_size,config.max_seq_len)\n",
    "        self.restModel = nn.Sequential(\n",
    "            nn.Dropout(config.dropout),\n",
    "            *[DecoderBlock(config) for i in range(config.num_layers)],\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "        )\n",
    "        self.unembed = self.tokenize.weight.T\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x = self.tokenize(x)\n",
    "        x = self.positionize(x)\n",
    "        toUnembed = self.restModel(x)\n",
    "        return toUnembed@self.unembed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self,seq_len, total_size):\n",
    "        self.seq_len = seq_len\n",
    "        self.total_size = total_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = t.randint(0,10,(self.seq_len,))\n",
    "        target = t.flip(input,dims=(0,))\n",
    "        return (input, target) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisConfig = TransformerConfig(\n",
    "    num_layers = 2,\n",
    "    num_heads = 8,\n",
    "    vocab_size = 10,\n",
    "    hidden_size = 40, # recall that this = num_heads * headsize\n",
    "    max_seq_len = 10,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon=0.00001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomTextDataset(8,1000)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5, 0, 4, 6, 0, 7, 1, 9]), tensor([9, 1, 7, 0, 6, 4, 0, 5]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransformer = DecoderOnlyTransformer(thisConfig)\n",
    "optimizer = t.optim.Adam(myTransformer.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 5, 3, 5, 1, 2, 5]) tensor([[4, 4, 5, 3, 5, 1, 2, 5]]) tensor([5, 2, 1, 5, 3, 5, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the first few datapoints\n",
    "myTransformer.eval()\n",
    "for i in range(1):\n",
    "    with t.no_grad():\n",
    "        input, target = trainset[i]\n",
    "        newInput = input[None, :] # TODO: this is so bad please i need to fix this bruh\n",
    "        output = myTransformer(newInput).argmax(dim=-1)\n",
    "        print(input, output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 9, 1, 0, 5, 4, 4, 5])\n",
      "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# playing around with how to modify the targets to be able to use it in the criterion\n",
    "for inputs, targets in trainloader:\n",
    "    print(targets[0])\n",
    "    print(t.nn.functional.one_hot(targets[0], num_classes=10)) # yeah, so the loss has gotta be one-hot-encoded!\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 120\n",
    "\n",
    "losses = []\n",
    "myTransformer.train()\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    for inputs, targets in trainloader:\n",
    "        outputs = myTransformer(inputs)\n",
    "        targets = t.nn.functional.one_hot(targets, num_classes=10).float()\n",
    "        outputs = einops.rearrange(outputs, 'batch seq vocab -> (batch seq) vocab')\n",
    "        targets = einops.rearrange(targets, 'batch seq vocab -> (batch seq) vocab')\n",
    "\n",
    "        loss = criterion(outputs,targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(losses)\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 9, 4, 0, 6, 5, 3, 1]])\n",
      "tensor([[1, 0, 2, 2, 0, 4, 9, 2]])\n"
     ]
    }
   ],
   "source": [
    "myTransformer.eval()\n",
    "# testing it out\n",
    "for inputs, targets in trainloader:\n",
    "    input = inputs[0]\n",
    "    input = input[None, :]\n",
    "    print(input)\n",
    "    \n",
    "    print(myTransformer(input).argmax(dim=-1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrospective:\n",
    "\n",
    "Seeing this - albeit basic - transformer actually reverse the numbers is VERY MUCH satisfying. My implementation of the transformer is very iffy, I threw down a bunch of bandaid fixes at different parts, but it *works*, and that feels unbelievably amazing. \n",
    "\n",
    "The way the transformer still feels a bit abstract to me. Perhaps this is normal, maybe it's not. I can audibly describe the high-level ideas of how keys, queries, and values come together with attentioin, as well as other MLPs and Decoder Blocks, but it hasn't really... clicked, yet. I don't know if it clicks for most people, this architecture just seems too massive to really internalize. Also, I blame my poor understanding of Linear Algebra, that probably isn't helping, either.\n",
    "\n",
    "Simply making this has taught me a lot about the difficulties of AI technical work in general, and is making me update upwards on the skill ceiling of alignment work. This exercise has been the hardest by a large margin, and I've tackled it for multiple days straight. Theres times where everything just feels overwhelming and my brain crashes, and those suck. But I think with practice, I'll get better at understanding these architectures, and not feeling too overwhelmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
