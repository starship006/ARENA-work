{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import createdFuncs as past\n",
    "import fancy_einsum as einsum\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to change the masked attention block to work over multiple batches. Once again, this exercise reminds me that I really need to understand einops and linear algebra. The matrix multiplication is truly a black box!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_head_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    Should return the results of masked self-attention.\n",
    "\n",
    "    See \"The Decoder Side\" section of the Illustrated Transformer for an explanation of masking.\n",
    "\n",
    "    Q: shape (batch, seq_len, head_size)\n",
    "    K: shape (batch, seq_len, head_size)\n",
    "    V: shape (batch, seq_len, value_size)\n",
    "\n",
    "    Return: shape (batch, seq_len, value_size)\n",
    "    '''\n",
    "    # second step - calculate a \"score\"\n",
    "    newK = einsum.einsum('b s h -> b h s', K)\n",
    "    score = Q@newK\n",
    "    # third step - divide score by dimensionality\n",
    "    score = score / np.sqrt(Q.shape[-1])\n",
    "    print(score.shape)\n",
    "    # MASKING IN BETWEEN!\n",
    "    mask = t.ones(score.shape)\n",
    "    for batch, submask in enumerate(mask):\n",
    "        for i, x in enumerate(submask):\n",
    "            x[i+1:] = -t.inf\n",
    "\n",
    "    score = score * mask\n",
    "    # fourth step - softmax\n",
    "    score = nn.functional.softmax(score, dim=-1)\n",
    "\n",
    "    # fifth step - multiply each value vector by the softmax score\n",
    "    z = score@V\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "tensor([[[1.0000],\n",
      "         [2.9951],\n",
      "         [3.9820]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [2.9951],\n",
      "         [3.9820]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# testing the new block\n",
    "# personal mini-test to see if its working correctly\n",
    "Q = t.tensor([[[1],[3],[4]],[[1],[3],[4]]],dtype=float)\n",
    "K = t.tensor([[[1],[3],[4]],[[1],[3],[4]]],dtype=float)\n",
    "V = t.tensor([[[1],[3],[4]],[[1],[3],[4]]],dtype=float)\n",
    "\n",
    "print(single_head_masked_attention(Q,K,V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the multiheaaded attention block!\n",
    "\n",
    "*(Cody on Saturday here: this one does not work fully as intended! look at the one below!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is bad! do not use it!\n",
    "# this is bad! do not use it!\n",
    "# this is bad! do not use it!\n",
    "\n",
    "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "    '''\n",
    "    Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "    Q: shape (batch, seq, nheads*head_size)\n",
    "    K: shape (batch, seq, nheads*head_size)\n",
    "    V: shape (batch, seq, nheads*head_size)\n",
    "    '''\n",
    "    # reshape Q, K, and V into 4D tensors to isolate the different heads\n",
    "    batch, seq, product = Q.shape\n",
    "    head_size = product // num_heads\n",
    "    Q = Q.reshape((batch, seq, num_heads, head_size))\n",
    "    K = K.reshape((batch, seq, num_heads, head_size))\n",
    "    V = V.reshape((batch, seq, num_heads, head_size))\n",
    "\n",
    "    # calculate attention scores\n",
    "    newK = einsum.einsum('b s n h -> b h n s', K)\n",
    "    score = einsum.einsum('b s n h, b h n a -> b s n a', Q, newK)\n",
    "    # third step - divide score by dimensionality\n",
    "    score = score / np.sqrt(Q.shape[-1])\n",
    "    # MASKING IN BETWEEN!\n",
    "    mask = t.ones(score.shape)\n",
    "    for _, batchMask in enumerate(mask):\n",
    "        for i, seqMask in enumerate(batchMask):\n",
    "            for j, x in enumerate(seqMask):\n",
    "                x[i+1:] = -t.inf \n",
    "\n",
    "    score = score * mask\n",
    "    # fourth step - softmax\n",
    "    score = nn.functional.softmax(score, dim=-1)\n",
    "    print(score.shape)\n",
    "    print(V.shape)\n",
    "    # fifth step - multiply each value vector by the softmax score\n",
    "    z = einsum.einsum('b s n s, b s n h -> b s n h ', score, V)\n",
    "    z = z.reshape((batch, seq, product))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 2, 5])\n",
      "torch.Size([2, 5, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[15.0000, 14.6667, 14.3333, 14.0000],\n",
       "         [12.6399, 12.3316, 12.6629, 12.3383],\n",
       "         [12.2258, 11.8953, 11.6333, 11.3009],\n",
       "         [10.9897, 10.6567, 10.3302,  9.9969],\n",
       "         [ 9.6657,  9.3324,  8.9997,  8.6664]],\n",
       "\n",
       "        [[ 8.3333,  8.0000,  7.6667,  7.3333],\n",
       "         [ 7.0000,  6.6667,  6.3333,  6.0000],\n",
       "         [ 5.6667,  5.3333,  5.0000,  4.6667],\n",
       "         [ 4.3333,  4.0000,  3.6667,  3.3333],\n",
       "         [ 3.0000,  2.6667,  2.3333,  2.0000]]])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test from James Dao; my stuff continues to work\n",
    "Q = t.linspace(0, 10, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "K = t.linspace(5, 20, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "V = t.linspace(15, 2, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "multihead_masked_attention(Q, K, V, num_heads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making an attention block out of the above function. This is tricky. May need to come back to it after re-reading some more transformer stuff. For now, I'll go and review einsum :)\n",
    "\n",
    "Okay. I'm back! Feel better about einsum. I'll have to finish this tomorrow, I think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friday, October 29 - it's been a bit since I've worked on this, and I really need to understand multi-headed attention. I don't think I really do, yet, so I'm going to try to write out the previous function again :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "    '''\n",
    "    Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "    Q: shape (batch, seq_len, nheads*headsize)\n",
    "    K: shape (batch, seq_len, nheads*headsize)\n",
    "    V: shape (batch, seq_len, nheads*headsize)\n",
    "    '''\n",
    "    \n",
    "    Q = einops.rearrange(Q, 'b s (n h) -> b n s h', n = num_heads)\n",
    "    K = einops.rearrange(K, 'b s (n h) -> b n s h', n = num_heads)\n",
    "    V = einops.rearrange(V, 'b s (n h) -> b n s h', n = num_heads)\n",
    "\n",
    "\n",
    "    scores = einsum.einsum('b n k h, b n s h -> b n s k', K, Q)\n",
    "    assert scores.shape == t.Size([Q.shape[0], num_heads,Q.shape[2], K.shape[2]])\n",
    "\n",
    "    scores = scores / np.sqrt(Q.shape[-1])\n",
    "    attention = scores + t.triu(t.ones_like(scores) * float(\"-inf\"), diagonal=1) # THIS IS STOLEN FROM JAY - testing it out\n",
    "    softed = t.softmax(attention,dim=-1)\n",
    "    result =  einsum.einsum('batch numheads seqQ seqK, batch numheads seqK headsize -> batch numheads seqQ headsize',softed, V)\n",
    "    return einops.rearrange(result, 'batch numheads seqQ headsize -> batch seqQ (numheads headsize)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15.0000, 14.6667, 14.3333, 14.0000],\n",
       "         [13.7668, 13.4335, 13.0346, 12.7012],\n",
       "         [12.3451, 12.0117, 11.6705, 11.3372],\n",
       "         [11.0013, 10.6679, 10.3337, 10.0004],\n",
       "         [ 9.6668,  9.3335,  9.0000,  8.6667]],\n",
       "\n",
       "        [[ 8.3333,  8.0000,  7.6667,  7.3333],\n",
       "         [ 7.0000,  6.6667,  6.3333,  6.0000],\n",
       "         [ 5.6667,  5.3333,  5.0000,  4.6667],\n",
       "         [ 4.3333,  4.0000,  3.6667,  3.3333],\n",
       "         [ 3.0000,  2.6667,  2.3333,  2.0000]]])"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test from James Dao; my stuff continues to work\n",
    "Q = t.linspace(0, 10, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "K = t.linspace(5, 20, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "V = t.linspace(15, 2, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "multihead_masked_attention(Q, K, V, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadMaskedAttention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "\n",
    "        self.WQKV = t.nn.Linear(self.hidden_size, 3 * hidden_size) # TODO: why do we use a linear layer here? aren't they matricies?\n",
    "        self.W0 = t.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        print(\"YO?\")\n",
    "        QKV = self.WQKV(x)\n",
    "        Q = QKV[:,:,:self.hidden_size]\n",
    "        K = QKV[:,:,self.hidden_size:self.hidden_size * 2]\n",
    "        V = QKV[:,:,self.hidden_size * 2:]\n",
    "        assert Q.shape == K.shape == V.shape == x.shape\n",
    "        return self.W0(multihead_masked_attention(Q,K,V,self.num_heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YO?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.7193,   0.4614,   0.4117,  -0.5813,   0.2754,  -0.5745],\n",
       "         [ -0.7746,   0.6206,   0.5520,  -0.7370,   0.1787,  -0.7289],\n",
       "         [ -1.1632,   1.7392,   1.5775,  -1.7907,  -0.5079,  -1.8103]],\n",
       "\n",
       "        [[  0.0549,  -1.9665, -10.8756,  -7.1792,   3.4559,   0.9521],\n",
       "         [ -0.3971,  -0.6652,  -9.6883,  -8.4108,   2.6582,  -0.3063],\n",
       "         [ -0.8686,   0.6920,  -8.4500,  -9.6953,   1.8262,  -1.6189]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(420)\n",
    "m = MultiheadMaskedAttention(6, 2)\n",
    "x = t.linspace(0, 42, 2 * 3 * 6).reshape(2, 3, 6)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    max_seq_len: int\n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy-paste the positional encoding from yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from yesterday\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dim = embedding_dim\n",
    "        self.length = max_seq_len\n",
    "\n",
    "        # mostly copied. i understand this, just need to work on \n",
    "        # making more tensors and getting more exposure to methods of making tensors\n",
    "        def P (delta):\n",
    "            n = 10000 # hardcoded\n",
    "            d = embedding_dim\n",
    "            l = max_seq_len\n",
    "            sin_array = np.sin(delta / n ** (2 * np.arange(d//2) / d))\n",
    "            cos_array = np.cos(delta / n ** (2 * np.arange(d//2) / d))\n",
    "\n",
    "            array = np.zeros(d)\n",
    "            array[::2] = sin_array\n",
    "            array[1::2] = cos_array\n",
    "\n",
    "            return array\n",
    "\n",
    "        tokenArray = []\n",
    "        for i in range(max_seq_len):\n",
    "            tokenArray.append(P(i)) # changed from previous design\n",
    "        \n",
    "        self.multMax = t.tensor(np.array(tokenArray))\n",
    "        \n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq_len, embedding_dim)\n",
    "        '''\n",
    "        return x + self.multMax[:x.shape[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    def forward(self, x: t.Tensor):\n",
    "        # print(\"YO\")\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attentionBlock = nn.Sequential(\n",
    "            MultiheadMaskedAttention(config.hidden_size,  config.num_heads),\n",
    "            nn.LayerNorm(config.hidden_size)\n",
    "        )\n",
    "        self.MLP = nn.Sequential(\n",
    "            MLP(config),\n",
    "            nn.LayerNorm(config.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        partOne = x + self.attentionBlock(x)\n",
    "        return partOne + MLP(partOne)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.tokenize = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positionize = PositionalEncoding(config.hidden_size,config.max_seq_len)\n",
    "        self.restModel = nn.Sequential(\n",
    "            nn.Dropout(config.dropout),\n",
    "            *[DecoderBlock(config) for i in range(config.num_layers)],\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "        )\n",
    "        self.unembed = self.tokenize.weight.T\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x = self.tokenize(x)\n",
    "        x = self.positionize(x)\n",
    "        toUnembed = self.restModel(x)\n",
    "        return toUnembed@self.unembed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self,seq_len, total_size):\n",
    "        self.seq_len = seq_len\n",
    "        self.total_size = total_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = t.randint(0,10,(self.seq_len,))\n",
    "        target = t.flip(input,dims=(0,))\n",
    "        return (input, target) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisConfig = TransformerConfig(\n",
    "    num_layers = 2,\n",
    "    num_heads = 4,\n",
    "    vocab_size = 10,\n",
    "    hidden_size = 24,\n",
    "    max_seq_len = 5,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon=0.00001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomTextDataset(4,10)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5, 0, 4, 6]), tensor([6, 4, 0, 5]))"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransformer = DecoderOnlyTransformer(thisConfig)\n",
    "optimizer = t.optim.Adam(myTransformer.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YO?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [497], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39minput\u001b[39m, target \u001b[39m=\u001b[39m trainset[i]\n\u001b[1;32m      6\u001b[0m newInput \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[\u001b[39mNone\u001b[39;00m, :]\n\u001b[0;32m----> 7\u001b[0m output \u001b[39m=\u001b[39m myTransformer(newInput)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39minput\u001b[39m, output, target)\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [489], line 17\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(x)\n\u001b[1;32m     16\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositionize(x)\n\u001b[0;32m---> 17\u001b[0m toUnembed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrestModel(x)\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m toUnembed\u001b[39m@self\u001b[39m\u001b[39m.\u001b[39munembed\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [488], line 15\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: t\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 15\u001b[0m     partOne \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattentionBlock(x)\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m partOne \u001b[39m+\u001b[39m MLP(partOne)\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [483], line 21\u001b[0m, in \u001b[0;36mMultiheadMaskedAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mx: shape (batch, seq, hidden_size)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[39mReturn: shape (batch, seq, hidden_size)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mYO?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m QKV \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWQKV(x)\n\u001b[1;32m     22\u001b[0m Q \u001b[39m=\u001b[39m QKV[:,:,:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size]\n\u001b[1;32m     23\u001b[0m K \u001b[39m=\u001b[39m QKV[:,:,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/ARENAenv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "# Test the model on the first few datapoints\n",
    "myTransformer.eval()\n",
    "for i in range(10):\n",
    "    with t.no_grad():\n",
    "        input, target = trainset[i]\n",
    "        newInput = input[None, :]\n",
    "        output = myTransformer(newInput).argmax(dim=-1)\n",
    "        print(input, output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[4, 1, 7, 8],\n",
       "         [9, 3, 0, 6],\n",
       "         [3, 7, 0, 9],\n",
       "         [1, 8, 5, 8],\n",
       "         [8, 6, 7, 1],\n",
       "         [8, 5, 2, 8],\n",
       "         [1, 1, 3, 5],\n",
       "         [8, 7, 3, 2],\n",
       "         [7, 5, 1, 9],\n",
       "         [3, 2, 6, 8]]),\n",
       " tensor([[8, 7, 1, 4],\n",
       "         [6, 0, 3, 9],\n",
       "         [9, 0, 7, 3],\n",
       "         [8, 5, 8, 1],\n",
       "         [1, 7, 6, 8],\n",
       "         [8, 2, 5, 8],\n",
       "         [5, 3, 1, 1],\n",
       "         [2, 3, 7, 8],\n",
       "         [9, 1, 5, 7],\n",
       "         [8, 6, 2, 3]])]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ARENAenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
