{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fancy_einsum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcreatedFuncs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpast\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfancy_einsum\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01meinsum\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fancy_einsum'"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import createdFuncs as past\n",
    "import fancy_einsum as einsum\n",
    "import einops\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy transformer code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "    '''\n",
    "    Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "    Q: shape (batch, seq_len, nheads*headsize)\n",
    "    K: shape (batch, seq_len, nheads*headsize)\n",
    "    V: shape (batch, seq_len, nheads*headsize)\n",
    "    '''\n",
    "    \n",
    "    Q = einops.rearrange(Q, 'b s (n h) -> b n s h', n = num_heads)\n",
    "    K = einops.rearrange(K, 'b s (n h) -> b n s h', n = num_heads)\n",
    "    V = einops.rearrange(V, 'b s (n h) -> b n s h', n = num_heads)\n",
    "\n",
    "\n",
    "    scores = einsum.einsum('b n k h, b n s h -> b n s k', K, Q)\n",
    "    assert scores.shape == t.Size([Q.shape[0], num_heads,Q.shape[2], K.shape[2]])\n",
    "\n",
    "    scores = scores / np.sqrt(Q.shape[-1])\n",
    "    attention = scores + t.triu(t.ones_like(scores) * float(\"-inf\"), diagonal=1) # THIS IS STOLEN FROM JAY - testing it out\n",
    "    softed = t.softmax(attention,dim=-1)\n",
    "    result =  einsum.einsum('batch numheads seqQ seqK, batch numheads seqK headsize -> batch numheads seqQ headsize',softed, V)\n",
    "    return einops.rearrange(result, 'batch numheads seqQ headsize -> batch seqQ (numheads headsize)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class MultiheadMaskedAttention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "\n",
    "        self.WQKV = t.nn.Linear(self.hidden_size, 3 * hidden_size) # TODO: why do we use a linear layer here? aren't they matricies?\n",
    "        self.W0 = t.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        #print(\"YO?\")\n",
    "        x = x.float() # seems like it needs to be a float!\n",
    "        QKV = self.WQKV(x)\n",
    "        Q = QKV[:,:,:self.hidden_size]\n",
    "        K = QKV[:,:,self.hidden_size:self.hidden_size * 2]\n",
    "        V = QKV[:,:,self.hidden_size * 2:]\n",
    "        assert Q.shape == K.shape == V.shape == x.shape\n",
    "        return self.W0(multihead_masked_attention(Q,K,V,self.num_heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    max_seq_len: int\n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# from yesterday\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dim = embedding_dim\n",
    "        self.length = max_seq_len\n",
    "\n",
    "        # mostly copied. i understand this, just need to work on \n",
    "        # making more tensors and getting more exposure to methods of making tensors\n",
    "        def P (delta):\n",
    "            n = 10000 # hardcoded\n",
    "            d = embedding_dim\n",
    "            l = max_seq_len\n",
    "            sin_array = np.sin(delta / n ** (2 * np.arange(d//2) / d))\n",
    "            cos_array = np.cos(delta / n ** (2 * np.arange(d//2) / d))\n",
    "\n",
    "            array = np.zeros(d)\n",
    "            array[::2] = sin_array\n",
    "            array[1::2] = cos_array\n",
    "\n",
    "            return array\n",
    "\n",
    "        tokenArray = []\n",
    "        for i in range(max_seq_len):\n",
    "            tokenArray.append(P(i)) # changed from previous design\n",
    "        \n",
    "        self.multMax = t.tensor(np.array(tokenArray), dtype=float)\n",
    "        \n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq_len, embedding_dim)\n",
    "        '''\n",
    "        return x + self.multMax[:x.shape[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    def forward(self, x: t.Tensor):\n",
    "        x = x.float() # seems like it needs to be a float!\n",
    "        return self.layers(x).float() # ima do the same thing again!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attentionBlock = nn.Sequential(\n",
    "            MultiheadMaskedAttention(config.hidden_size,  config.num_heads),\n",
    "            nn.LayerNorm(config.hidden_size)\n",
    "        )\n",
    "        self.MLP = nn.Sequential(\n",
    "            MLP(config),\n",
    "            nn.LayerNorm(config.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        partOne = x + self.attentionBlock(x)\n",
    "        return (partOne + self.MLP(partOne)).float() # seems like it needs to be a float!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.tokenize = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positionize = PositionalEncoding(config.hidden_size,config.max_seq_len)\n",
    "        self.restModel = nn.Sequential(\n",
    "            nn.Dropout(config.dropout),\n",
    "            *[DecoderBlock(config) for i in range(config.num_layers)],\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "        )\n",
    "        self.unembed = self.tokenize.weight.T\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x = self.tokenize(x)\n",
    "        x = self.positionize(x)\n",
    "        toUnembed = self.restModel(x)\n",
    "        return toUnembed@self.unembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisConfig = TransformerConfig(\n",
    "    num_layers = 2,\n",
    "    num_heads = 8,\n",
    "    vocab_size = 10,\n",
    "    hidden_size = 40, # recall that this = num_heads * headsize\n",
    "    max_seq_len = 10,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon=0.00001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1829bf021947e771a2c0399247f13cc64d76e227c4c4356073fc0c03f05b7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
